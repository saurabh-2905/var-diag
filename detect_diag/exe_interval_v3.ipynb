{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Interval Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)    theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "\n",
    "base_dir = '../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "### remove.Ds_store from all lists\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label\n",
    "\n",
    "print(train_data_path)\n",
    "print(test_data_path)\n",
    "print(test_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3 or VER == 4:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]]+ varlist_path) ### compare with train varlist\n",
    "\n",
    "    if check_con != False:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        ### load normal varlist\n",
    "        print('loading normal varlist')\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_number = read_json(train_varlist_path[0])\n",
    "from_number = mapint2var(to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### key finder ####\n",
    "# from_number[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confidence Interval:__\n",
    "\n",
    "A confidence interval is a range around the mean that is likely to contain the true population mean. The formula for a confidence interval is mean ± margin of error mean±margin of error, where the margin of error depends on the desired confidence level and the standard error.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "1. Choose a confidence level (e.g., 95%).\n",
    "2. Calculate the standard error: standard deviation/ sqr_root(number of observations)\n",
    "3. Calculate the margin of error: critical value × standard error\n",
    "4. Determine the confidence interval: mean ± margin of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize exeinz\n",
    "ei = exeInt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get execution intervals for all variables\n",
    "\n",
    "exe_list, filewise_exe_list = ei.get_exeint(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(exe_list.keys()):\n",
    "    print(k, len(exe_list[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(exe_list[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## methods to detect outliers based on execution intervals ####################\n",
    "\n",
    "############ calculate dynamic thresholds ############\n",
    "thresholds = ei.get_dynamicthresh(exe_list)\n",
    "\n",
    "############ train lof model ################\n",
    "lof_models = ei.train_lof(exe_list)\n",
    "\n",
    "######### save thresholds and lof models ############\n",
    "### visualize the thresholds for varlist\n",
    "thresholds_var = {}\n",
    "for key in thresholds.keys():\n",
    "    print('key:', key)\n",
    "    thresholds_var[from_number[key]] = thresholds[key]\n",
    "\n",
    "assert len(thresholds_var) == len(thresholds)\n",
    "thresholds_var\n",
    "save_json(thresholds_var, os.path.join(faultybase_path, 'thresholds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window size for Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### number of events between two consecutive occurrence of each event\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "event_list, filewise_event_list = ei.get_eventint(train_data_path)\n",
    "event_range = ei.get_eventrange(event_list)\n",
    "\n",
    "print('event_range:', event_range)\n",
    "max_range = 0\n",
    "all_intervals = []\n",
    "for ranges in event_range.values():\n",
    "    # print('range:', max(ranges))\n",
    "    all_intervals.extend(ranges)\n",
    "\n",
    "print('intervals:', all_intervals)\n",
    "max_window = max(all_intervals)\n",
    "mean_window = np.mean(all_intervals)\n",
    "median_window = np.median(all_intervals)\n",
    "mode_window = stats.mode(all_intervals)\n",
    "\n",
    "std_deviation = np.std(all_intervals)\n",
    "# variance = np.var(all_intervals)\n",
    "\n",
    "print('max_window:', max_window)\n",
    "print('mean_window:', mean_window)\n",
    "print('median_window:', median_window)\n",
    "print('mode_window:', mode_window)\n",
    "print('std_deviation:', std_deviation)\n",
    "# print('variance:', variance)\n",
    "### get max window size for diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot exe_list to vsiualize the distribution of execution intervals\n",
    "# ei.viz_thresholds(exe_list, thresholds=thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect anomalies in faulty traces\n",
    "DIFF_VAL = 5\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_fn = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "all_group_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]\n",
    "all_merged_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "all_gt = []\n",
    "for test_data, test_label in zip(test_data_path, test_label_path):\n",
    "    print(test_data, test_label)\n",
    "\n",
    "    detection = ei.test_single(test_data, thresholds=thresholds)   ### detection in format: [var, (ts1,ts2), file_name]     ### threshold based detection\n",
    "    print('detection:', detection)\n",
    "    # detection = ei.test_single(test_data, lof_models=lof_models)   ### detection in format: [var, (ts1,ts2), file_name]    ### lof based detection\n",
    "    before_merge = len(detection)\n",
    "\n",
    "    merged_detection, grouped_det, events_exe = ei.merge_detections(detection, DIFF_VAL, get_exetime=True)  ### merge detections for multiple variables\n",
    "    # merged_detection, grouped_det = ei.merge_detections(detection, DIFF_VAL)  ### merge detections for multiple variables\n",
    "    detection = merged_detection\n",
    "    print('merged detections:', detection)\n",
    "    print('events_exe:', events_exe)\n",
    "\n",
    "    # dedup_detection, grouped_det = ei.remove_duplicates(detection, DIFF_VAL)  ### remove multiple detections for single ground truth\n",
    "    # detection = dedup_detection\n",
    "    after_merge = len(detection)\n",
    "    # print('before merge:', before_merge, 'after merge:', after_merge)\n",
    "\n",
    "    # all_group_detections += [(test_data, grouped_det, test_label)]  ### used to plot grouped detections\n",
    "    # all_merged_detections += [(test_data, merged_detection, test_label)]  ### used to plot merged detections\n",
    "\n",
    "    ### load ground truths\n",
    "    ground_truth_raw = read_traces(test_label)\n",
    "    ground_truth = ground_truth_raw['labels']\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "    print('ground truths:', ground_truth)\n",
    "    print(len(ground_truth))\n",
    "\n",
    "    # correct_pred, rest_pred, y_pred, y_true = get_ypred_ytrue(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "    # correct_pred, rest_pred, y_pred, y_true = ei.get_correct_detections(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "    correct_pred, rest_pred, y_pred, y_true, false_neg = ei.get_correct_detections(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "\n",
    "    assert( len(detection) == len(correct_pred) + len(rest_pred) )\n",
    "\n",
    "    all_detections += [(test_data, detection, test_label)]  ### used to plot detections\n",
    "    all_tp += [(test_data, correct_pred, test_label)]\n",
    "    all_fp += [(test_data, rest_pred, test_label)]\n",
    "    all_fn += [(test_data, false_neg, test_label)]\n",
    "    all_gt += [(test_data, ground_truth, test_label)]\n",
    "\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Quality after diff_val (percent of non-anomalous trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tp_quality = []\n",
    "all_quality_scores = []\n",
    "all_gt_count = []\n",
    "all_tp_count = []\n",
    "for file_tp in all_tp[0:]:\n",
    "    print(file_tp)\n",
    "    file_path = file_tp[0]\n",
    "    print('File Path:', file_path)\n",
    "    print('File Name:', os.path.basename(file_path))\n",
    "\n",
    "    trace = read_traces(file_path)\n",
    "    # print('Trace Data:', trace)\n",
    "    time_stamps = [e[1] for e in trace]\n",
    "    events = [e[0] for e in trace]\n",
    "    # print('Time Stamps:', time_stamps)\n",
    "    # print('Events:', events)\n",
    "\n",
    "    time_stamps = np.array(time_stamps)\n",
    "    events = np.array(events)\n",
    "\n",
    "    ### load GT\n",
    "    label_path = file_tp[2]\n",
    "    # print('Label Path:', label_path)\n",
    "    ground_truth_raw = read_traces(label_path)\n",
    "    # print('Ground Truth Raw:', ground_truth_raw)\n",
    "    ground_truth = ground_truth_raw['labels']\n",
    "    # print('Ground Truth:', ground_truth)\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "    # print('Label Trace Name:', label_trace_name)\n",
    "    # print('Ground Truth Length:', len(ground_truth))\n",
    "    print('Ground Truth:', ground_truth)\n",
    "\n",
    "    quality_score = []\n",
    "    gt_count = 0\n",
    "    tp_count = 0\n",
    "    for tp in file_tp[1]:\n",
    "        print('TP:', tp)\n",
    "        pd_ts1 = tp[1][0]\n",
    "        pd_ts2 = tp[1][1]\n",
    "        # print('Detection Start:', pd_ts1)\n",
    "        # print('Detection End:', pd_ts2)\n",
    "\n",
    "        ### get the closest timestamp to pd_ts1 and pd_ts2\n",
    "        start_tp = np.argmin(np.abs(time_stamps - pd_ts1))\n",
    "        end_tp = np.argmin(np.abs(time_stamps - pd_ts2))\n",
    "        print('Start Index:', start_tp)\n",
    "        print('End Index:', end_tp)\n",
    "        \n",
    "        ### get exact timestamp match for pd_ts1 and pd_ts2\n",
    "        # start_tp = np.where(time_stamps == pd_ts1)[0][0]\n",
    "        # end_tp = np.where(time_stamps == pd_ts2)[0][0]\n",
    "        # print('Start Index:', start_index)\n",
    "        # print('End Index:', end_index)\n",
    "\n",
    "        ### get the sub trace corresponding to the detection\n",
    "        det_trace = trace[start_tp:end_tp+1]\n",
    "        len_det_trace = len(det_trace)\n",
    "        # print('Detection Trace:', det_trace)\n",
    "        # print('Detection Trace Length:', len(det_trace))\n",
    "\n",
    "        ### collect groundtruths that intersect with the detection\n",
    "        gt_overlap_trace = []\n",
    "        sel_gt = []\n",
    "        for gt in ground_truth:\n",
    "            gt_ts1 = gt[2]\n",
    "            gt_ts2 = gt[3]\n",
    "            # print('Ground Truth Start:', gt_ts1)\n",
    "            # print('Ground Truth End:', gt_ts2)\n",
    "            cond_1 = pd_ts1 >= gt_ts1 and pd_ts2 <= gt_ts2  ### check if the detection timestamp is within the ground truth timestamp (case 1)\n",
    "            cond_2 = pd_ts1 <= gt_ts1 and pd_ts2 >= gt_ts2  ### check if the gorund truth timestamp is within the detection timestamp (case 2)\n",
    "            cond_3 = pd_ts1 >= gt_ts1 and pd_ts1 <= gt_ts2 and pd_ts2 >= gt_ts2    ### partial detection on right of the ground truths, check 5 second difference after this (case 3)\n",
    "            cond_4 = pd_ts2 <= gt_ts2 and pd_ts2 >= gt_ts1 and pd_ts1 <= gt_ts1   ### partial detection on left of the ground truths, check 5 second difference after this (case 4)\n",
    "\n",
    "            if cond_1 or cond_2 or cond_3 or cond_4:\n",
    "                print('GT', gt)\n",
    "                if gt not in sel_gt:\n",
    "                    sel_gt.append(gt)\n",
    "\n",
    "                start_gt = np.where(time_stamps == gt_ts1)[0][0]\n",
    "                end_gt = np.where(time_stamps == gt_ts2)[0][0]\n",
    "                # print('Start Index GT:', start_gt)\n",
    "                # print('End Index GT:', end_gt)\n",
    "                gt_trace = trace[start_gt:end_gt+1]\n",
    "                # print('Ground Truth Trace:', gt_trace)\n",
    "                # print('len of gt:', len(gt_trace))\n",
    "\n",
    "                for (gt_e, gt_t) in gt_trace:\n",
    "                    if gt_t >= pd_ts1 and gt_t <= pd_ts2:\n",
    "                        gt_overlap_trace.append((gt_e, gt_t))\n",
    "\n",
    "\n",
    "        print('Selected Ground Truths:', sel_gt)\n",
    "        # print('gt overlap_trace:', gt_overlap_trace)\n",
    "\n",
    "        ### calculate quality of detection (percentge of detection trace that do not overlap with ground truth)\n",
    "        if len(gt_overlap_trace) == 0:\n",
    "            # raise ValueError('No ground truth overlap trace found for detection', tp, ', file path:', file_path)\n",
    "            overlap_percentage = 0.0\n",
    "        else:\n",
    "            overlap_percentage = len(gt_overlap_trace) / len_det_trace    ### precision of detection (point-wise)\n",
    "            overlap_percentage = np.round(overlap_percentage, 2)\n",
    "\n",
    "\n",
    "            # nonanomaly_percentage = 1 - overlap_percentage\n",
    "            # nonanomaly_percentage = np.round(nonanomaly_percentage, 2)\n",
    "            # print('Detection with normal trace (Percentage):', nonanomaly_percentage)\n",
    "            # quality_score.append(nonanomaly_percentage)\n",
    "            \n",
    "        print('Point-wise precision of detection:', overlap_percentage)\n",
    "        quality_score.append(overlap_percentage)\n",
    "\n",
    "        \n",
    "        ### calculate quality metric (detection to ground truth ratio)\n",
    "        gt_count = len(sel_gt)\n",
    "        tp_count += 1\n",
    "\n",
    "        print('')\n",
    "        # break    \n",
    "\n",
    "    all_gt_count += [gt_count]\n",
    "    all_tp_count += [tp_count]\n",
    "    \n",
    "    all_tp_quality.append((file_path, quality_score, label_path))\n",
    "    all_quality_scores.extend(quality_score)\n",
    "    print('GT count:', gt_count)\n",
    "    print('TP count:', tp_count)\n",
    "    print('')   \n",
    "    # break\n",
    "\n",
    "print('Total GT count:', all_gt_count)\n",
    "print('Total TP count:', all_tp_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality_scores = np.array(all_quality_scores)\n",
    "print('All quality scores:', all_quality_scores)\n",
    "print('Average quality score:', np.mean(all_quality_scores))\n",
    "print('Standard deviation of quality scores:', np.std(all_quality_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quality_scores = np.array(all_quality_scores)\n",
    "print('All quality scores:', all_quality_scores)\n",
    "print('Average quality score:', np.mean(all_quality_scores))\n",
    "print('Standard deviation of quality scores:', np.std(all_quality_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Caculate quality score for each file as well as overall quality score\n",
    "overall_quality_score = []\n",
    "gtc = 0\n",
    "tpc = 0\n",
    "for file_quality, gt_count, tp_count in zip(all_tp_quality, all_gt_count, all_tp_count):\n",
    "    file_path = file_quality[0]\n",
    "    quality_score = file_quality[1]\n",
    "    label_path = file_quality[2]\n",
    "\n",
    "    print('File Path:', file_path)\n",
    "    # print('Quality Score:', quality_score)\n",
    "    # print('Label Path:', label_path)\n",
    "\n",
    "    if len(quality_score) == 0:\n",
    "        print('No quality score found for file:', file_path)\n",
    "        continue\n",
    "\n",
    "    avg_quality = np.mean(quality_score)\n",
    "    avg_quality = np.round(avg_quality, 2)\n",
    "    std_quality = np.std(quality_score)\n",
    "    std_quality = np.round(std_quality, 2)\n",
    "    overall_quality_score.extend(quality_score)\n",
    "    print('Average Quality Score for File:', avg_quality, '±', std_quality)\n",
    "\n",
    "    ### calculate quality metric (detection to ground truth ratio)\n",
    "    per_file = tp_count / gt_count\n",
    "    print('Detection to Ground Truth Ratio for File:', per_file)\n",
    "\n",
    "    gtc += gt_count\n",
    "    tpc += tp_count\n",
    "\n",
    "    print('')\n",
    "\n",
    "overall_avg = np.mean(overall_quality_score)\n",
    "overall_std = np.std(overall_quality_score)\n",
    "overall_avg = np.round(overall_avg, 2)\n",
    "overall_std = np.round(overall_std, 2)\n",
    "print('Overall Average Quality Score:', overall_avg, '±', overall_std)\n",
    "print('')\n",
    "\n",
    "overall_tgr = tpc / gtc\n",
    "overall_tgr = np.round(overall_tgr, 2)\n",
    "print('Overall Detection to Ground Truth Ratio:', overall_tgr)\n",
    "\n",
    "\n",
    "\n",
    "# print('Total Ground Truth Count:', all_gt_count)\n",
    "# print('Total True Positive Count:', all_tp_count)\n",
    "# det2gt = all_tp_count / all_gt_count\n",
    "# det2gt = np.round(det2gt, 2)\n",
    "# print('Detection to Ground Truth Ratio:', det2gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_fp)\n",
    "fp_count = 0\n",
    "for fp in all_fp:\n",
    "    print( len(fp[1]))\n",
    "    fp_count += len(fp[1])\n",
    "\n",
    "print('fp_count:', fp_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "if len(conf_matrix) == 1:\n",
    "    conf_matrix = np.array([[0, 0], [0, conf_matrix[0][0]]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classwise Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_fn = defaultdict(list)\n",
    "classwise_tp = defaultdict(list)\n",
    "gt_len = 0\n",
    "for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "    fn = file_fn[1]\n",
    "    gt = file_gt[1]\n",
    "    for label in gt:\n",
    "        if label in fn:\n",
    "            classwise_fn[label[4]].append(label)\n",
    "        else:\n",
    "            classwise_tp[label[4]].append(label)\n",
    "            # print('tp:', label)\n",
    "\n",
    "    gt_len += len(gt)\n",
    "    # print('file gt:', len(gt))\n",
    "    # print('file fn:', len(fn))\n",
    "    # print('\\n')\n",
    "    # break\n",
    "\n",
    "total_fn = 0\n",
    "total_tp = 0\n",
    "keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# print('keys:', keys)\n",
    "class_recall = []\n",
    "for key in keys:\n",
    "    print('class:', key)\n",
    "    total_fn += len(classwise_fn[key])\n",
    "    total_tp += len(classwise_tp[key])\n",
    "\n",
    "    crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "\n",
    "    # print('not detected:', len(classwise_fn[key]))\n",
    "    print('detected:', len(classwise_tp[key]))\n",
    "    print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    print('Recall (classwise):', crecall)\n",
    "    print('\\n')\n",
    "\n",
    "    class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# print('total fn+tp:', total_fn+total_tp)\n",
    "# print('total gt:', gt_len)\n",
    "assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## save detections for the dashboard to plot #############\n",
    "import traceback\n",
    "\n",
    "for test_data, detections, test_label in all_detections:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', f'ei_detections')\n",
    "    detection_path = detection_path.replace('ei_detections.json', f'ei_detections_{DIFF_VAL}.json')\n",
    "    # tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "    # fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "    # print(detections)\n",
    "    # print(detection_path)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "        with open(detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {detection_path}')\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue\n",
    "\n",
    "for test_data, detections, test_label in all_tp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'ei_detections')\n",
    "    tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "    # fp_detection_path = detection_path.replace('ei_detections.json', 'fp_ei_detections.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(tp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {tp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue\n",
    "\n",
    "for test_data, detections, test_label in all_fp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'ei_detections')\n",
    "    # tp_detection_path = detection_path.replace('ei_detections.json', 'tp_ei_detections.json')\n",
    "    fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(fp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {fp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Total Detections:', len(all_detections[1][1]))\n",
    "# print('Total Groups:', len(all_group_detections[1][1]))\n",
    "# i=0\n",
    "# for item in all_group_detections[1][1]:\n",
    "#     for ind_item in item:\n",
    "#         i+=1\n",
    "# print('Detections in Groups:', i)\n",
    "# print('Total Merged:', len(all_merged_detections[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### plot gt and detections\n",
    "# for test_data, detections, test_label in all_detections:\n",
    "# # for test_data, detections, test_label in all_fp:\n",
    "#     # print('test_data:', test_data)\n",
    "#     # print('detections:', detections)\n",
    "#     # print(test_label)\n",
    "\n",
    "#     ### prepare trace to plot\n",
    "#     col_data = preprocess_traces([test_data])\n",
    "#     all_df = get_dataframe(col_data) \n",
    "#     # print(all_df[0])\n",
    "\n",
    "#     ### prepare detections to plot\n",
    "#     timestamps = col_data[0][1]\n",
    "#     print('timestamps:', timestamps)\n",
    "#     plot_val = []\n",
    "#     plot_x_ticks = []\n",
    "#     plot_class = []\n",
    "#     for det in detections:\n",
    "#         # print(det)\n",
    "#         det_ts1, det_ts2 = det[1]\n",
    "#         # print(det_ts1, det_ts2)\n",
    "\n",
    "#         det_ind1_pre = [ abs(t-det_ts1) for t in timestamps]\n",
    "#         det_ind1 = det_ind1_pre.index(min(det_ind1_pre))\n",
    "\n",
    "#         det_ind2_pre = [ abs(t-det_ts2) for t in timestamps]\n",
    "#         det_ind2 = det_ind2_pre.index(min(det_ind2_pre))\n",
    "#         # print(det_ind1, det_ind2)\n",
    "#         # print(timestamps[det_ind1], timestamps[det_ind2])\n",
    "\n",
    "#         plot_val += [(det_ind1, det_ind2)]\n",
    "#         plot_x_ticks += [(timestamps[det_ind1], timestamps[det_ind2])]\n",
    "#         plot_class += [0]\n",
    "\n",
    "#     plot_detections = [plot_val, plot_x_ticks, plot_class]\n",
    "\n",
    "#     ### get ground truths\n",
    "#     gt_plot = prepare_gt(test_label)\n",
    "\n",
    "#     ### plot\n",
    "#     for df in all_df:\n",
    "#         # print(df.columns)\n",
    "#         plot_fig = plot_single_trace(df, \n",
    "#                           var_list, \n",
    "#                           with_time=False, \n",
    "#                           is_xticks=True, \n",
    "#                           detections=plot_detections, \n",
    "#                           dt_classlist=['detection'],\n",
    "#                           ground_truths=gt_plot,\n",
    "#                           gt_classlist=['gt_communication', 'gt_sensor', 'gt_bitflip'],\n",
    "#                           )\n",
    "#         plot_fig.show()\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### plot merged detections\n",
    "# ### plot gt and detections\n",
    "# # for test_data, detections, test_label in all_detections:\n",
    "# # for test_data, detections, test_label in all_merged_detections: #### all merged detections\n",
    "# for test_data, detections, test_label in all_fp:\n",
    "#     # print('test_data:', test_data)\n",
    "#     # print('detections:', detections)\n",
    "#     # print(test_label)\n",
    "\n",
    "#     ### prepare trace to plot\n",
    "#     col_data = preprocess_traces([test_data])\n",
    "#     all_df = get_dataframe(col_data) \n",
    "#     # print(all_df[0])\n",
    "\n",
    "#     ### prepare detections to plot\n",
    "#     timestamps = col_data[0][1]\n",
    "#     print('timestamps:', timestamps)\n",
    "#     plot_val = []\n",
    "#     plot_x_ticks = []\n",
    "#     plot_class = []\n",
    "#     for det in detections:\n",
    "#         # print(det)\n",
    "#         det_ts1, det_ts2 = det[1]\n",
    "#         # print(det_ts1, det_ts2)\n",
    "\n",
    "#         det_ind1_pre = [ abs(t-det_ts1) for t in timestamps]\n",
    "#         det_ind1 = det_ind1_pre.index(min(det_ind1_pre))\n",
    "\n",
    "#         det_ind2_pre = [ abs(t-det_ts2) for t in timestamps]\n",
    "#         det_ind2 = det_ind2_pre.index(min(det_ind2_pre))\n",
    "#         # print(det_ind1, det_ind2)\n",
    "#         # print(timestamps[det_ind1], timestamps[det_ind2])\n",
    "\n",
    "#         plot_val += [(det_ind1, det_ind2)]\n",
    "#         plot_x_ticks += [(timestamps[det_ind1], timestamps[det_ind2])]\n",
    "#         plot_class += [0]\n",
    "\n",
    "#     plot_detections = [plot_val, plot_x_ticks, plot_class]\n",
    "\n",
    "#     ### get ground truths\n",
    "#     gt_plot = prepare_gt(test_label)\n",
    "\n",
    "#     ### plot\n",
    "#     for df in all_df:\n",
    "#         # print(df.columns)\n",
    "#         plot_fig = plot_single_trace(df, \n",
    "#                           var_list, \n",
    "#                           with_time=False, \n",
    "#                           is_xticks=True, \n",
    "#                           detections=plot_detections, \n",
    "#                           dt_classlist=['detection'],\n",
    "#                           ground_truths=gt_plot,\n",
    "#                           gt_classlist=['gt_communication', 'gt_sensor', 'gt_bitflip'],\n",
    "#                           )\n",
    "#         plot_fig.show()\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff Val for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### diff_val for paper\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "\n",
    "diff_val_res = []  ### used to store results for different diff_val\n",
    "for DIFF_VAL in range(0, 180):\n",
    "    if (DIFF_VAL % 5 == 0 and DIFF_VAL <= 50) or DIFF_VAL <= 5:\n",
    "        print('DIFF_VAL:', DIFF_VAL)\n",
    "\n",
    "        # DIFF_VAL = 2\n",
    "        #### Validate model\n",
    "        all_detections = []  ### format [file1_detection, file2_detection] -> file1_detection: [(state1, state2), (ts1, ts2), filename]\n",
    "        y_pred_all = []\n",
    "        y_true_all = []\n",
    "        all_tp = []\n",
    "        all_fp = []\n",
    "        all_fn = []\n",
    "        all_gt = []\n",
    "\n",
    "        # tp = 0\n",
    "        # fp = 0\n",
    "        # fn = 0\n",
    "            \n",
    "        for test_data, test_label in zip(test_data_path, test_label_path):\n",
    "            print('Testing data:', test_data)\n",
    "            detection = ei.test_single(test_data, thresholds=thresholds)   ### detection in format: [var, (ts1,ts2), file_name]     ### threshold based detection\n",
    "            # print('Detections:', detection)\n",
    "            print('len of detections:', len(detection))\n",
    "\n",
    "            merged_detection, grouped_det, events_exe = ei.merge_detections(detection, DIFF_VAL, get_exetime=True)  ### merge detections for multiple variables\n",
    "            detection = merged_detection\n",
    "\n",
    "\n",
    "            ground_truth_raw = read_traces(test_label)\n",
    "            ground_truth = ground_truth_raw['labels']\n",
    "            label_trace_name = list(ground_truth.keys())[0]\n",
    "            ground_truth = ground_truth[label_trace_name]\n",
    "            # print('ground truths:', ground_truth)\n",
    "            # print(len(ground_truth))\n",
    "            print('merged detections', len(merged_detection))\n",
    "\n",
    "            correct_pred, rest_pred, y_pred, y_true, false_neg = ei.get_correct_detections(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "            \n",
    "            y_pred_all.extend(y_pred)\n",
    "            y_true_all.extend(y_true)\n",
    "\n",
    "            all_detections += [(test_data, detection, test_label)]  ### used to plot detections\n",
    "            all_tp += [(test_data, correct_pred, test_label)]\n",
    "            all_fp += [(test_data, rest_pred, test_label)]\n",
    "            all_fn += [(test_data, false_neg, test_label)]\n",
    "            all_gt += [(test_data, ground_truth, test_label)]\n",
    "\n",
    "\n",
    "        # # Calculate precision\n",
    "        # precision = precision_score(y_true_all, y_pred_all)\n",
    "        # print(f'Precision: {precision:.4f}')\n",
    "\n",
    "        # # Calculate recall\n",
    "        # recall = recall_score(y_true_all, y_pred_all)\n",
    "        # print(f'Recall: {recall:.4f}')\n",
    "\n",
    "        # # Calculate average precision\n",
    "        # average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "        # print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_true_all, y_pred_all)\n",
    "        # print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "        # print(\"Confusion Matrix:\")\n",
    "\n",
    "        ### calculate detection quality\n",
    "        quality_score, total_tp_count, total_gt_count = detection_quality(all_tp, output_score=True)\n",
    "\n",
    "        ### metric 1 (percent of non-anomalous trace)\n",
    "        avg_quality_score = np.mean(quality_score)\n",
    "        std_quality_score = np.std(quality_score)\n",
    "\n",
    "        print('Detection Quality Results:', quality_score, '±', std_quality_score)\n",
    "\n",
    "        ### metric 2 (ratio of TP to GT)\n",
    "        det2gt = total_tp_count / total_gt_count\n",
    "\n",
    "        f1 = np.round(f1, 2)\n",
    "        avg_quality_score = np.round(avg_quality_score, 2)\n",
    "        std_quality_score = np.round(std_quality_score, 2)\n",
    "        det2gt = np.round(det2gt, 2)\n",
    "\n",
    "        # print(conf_matrix.shape)\n",
    "        if conf_matrix.shape[0] == 1:\n",
    "            print(conf_matrix)\n",
    "            diff_val_res.append([DIFF_VAL, f1, 0, 0, avg_quality_score, std_quality_score, det2gt])\n",
    "        else:\n",
    "            diff_val_res.append([DIFF_VAL, f1, conf_matrix[0][1], conf_matrix[1][0], avg_quality_score, std_quality_score, det2gt])\n",
    "\n",
    "\n",
    "        ################## Saving detections ######################\n",
    "        for test_data, detections, test_label in all_detections:\n",
    "            # print(test_data, test_label)\n",
    "            # print(test_label.replace('labels', 'detections'))\n",
    "            detection_path = test_label.replace('labels', f'ei_detections')\n",
    "            detection_path = detection_path.replace('ei_detections.json', f'ei_detections_{DIFF_VAL}.json')\n",
    "            # tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "            # fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "            # print(detections)\n",
    "            # print(detection_path)\n",
    "\n",
    "            detection_dir = os.path.dirname(detection_path)\n",
    "            # print(detection_dir)\n",
    "            if not os.path.exists(detection_dir):\n",
    "                os.makedirs(detection_dir)\n",
    "                print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "            try:\n",
    "                with open(detection_path, 'w') as f:\n",
    "                    json.dump(detections, f)\n",
    "                    print(f'Saved detections in {detection_path}')\n",
    "\n",
    "                    \n",
    "            except Exception as e:\n",
    "                traceback.print_exception(e)\n",
    "                print('Error in saving detections')\n",
    "                continue\n",
    "\n",
    "        for test_data, detections, test_label in all_tp:\n",
    "            # print(test_data, test_label)\n",
    "            # print(test_label.replace('labels', 'detections'))\n",
    "            detection_path = test_label.replace('labels', 'ei_detections')\n",
    "            tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "            # fp_detection_path = detection_path.replace('ei_detections.json', 'fp_ei_detections.json')\n",
    "            # print(detections)\n",
    "\n",
    "            detection_dir = os.path.dirname(detection_path)\n",
    "            # print(detection_dir)\n",
    "            if not os.path.exists(detection_dir):\n",
    "                os.makedirs(detection_dir)\n",
    "                print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "            try:\n",
    "\n",
    "                with open(tp_detection_path, 'w') as f:\n",
    "                    json.dump(detections, f)\n",
    "                    print(f'Saved detections in {tp_detection_path}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                traceback.print_exception(e)\n",
    "                print('Error in saving detections')\n",
    "                continue\n",
    "\n",
    "        for test_data, detections, test_label in all_fp:\n",
    "            # print(test_data, test_label)\n",
    "            # print(test_label.replace('labels', 'detections'))\n",
    "            detection_path = test_label.replace('labels', 'ei_detections')\n",
    "            # tp_detection_path = detection_path.replace('ei_detections.json', 'tp_ei_detections.json')\n",
    "            fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "            # print(detections)\n",
    "\n",
    "            detection_dir = os.path.dirname(detection_path)\n",
    "            # print(detection_dir)\n",
    "            if not os.path.exists(detection_dir):\n",
    "                os.makedirs(detection_dir)\n",
    "                print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "            try:\n",
    "\n",
    "                with open(fp_detection_path, 'w') as f:\n",
    "                    json.dump(detections, f)\n",
    "                    print(f'Saved detections in {fp_detection_path}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                traceback.print_exception(e)\n",
    "                print('Error in saving detections')\n",
    "                continue\n",
    "        ################## Saving detections ######################\n",
    "        \n",
    "        \n",
    "        # break\n",
    "\n",
    "df = pd.DataFrame(diff_val_res, columns=['diff_val', 'f1_score', 'FP', 'FN', 'pdPrecision', 'pdPrecStd', 'tp_gt_ratio'], index=None)\n",
    "###### save df to excel\n",
    "df.to_excel(f'../results/diff_val/{CODE}_ei_results.xlsx', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "---\n",
    "- since multiple variables are affected due to single anomaly, multiple detections are generated for each anomaly.\n",
    "- This leads to multiple FP.\n",
    "- To avoid this, we implement deduplication which groups the detections that are close to each other bsed on timestamp\n",
    "- However, in this process along with decrease in FP, we have more False Negatives i.e. some anomalies are not detected. \n",
    "\n",
    "TODO:\n",
    "- change deduplication stratergy, if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
