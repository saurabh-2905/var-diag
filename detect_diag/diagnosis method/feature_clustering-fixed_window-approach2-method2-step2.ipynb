{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "- label manually for instances of class 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 - method 2 - step2 (Extract Features)\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "WINDOW = 500                 ### window size for subsequence\n",
    "SUBSEQ =  'diag_subseq'        # 'diag_subseq' , diag_subseq_multi       ### subsequence type, diag_subseq, subseq\n",
    "INST_SEP = 'M2'   ###  M2, M3, M0\n",
    "\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, f'diag_refsamples{WINDOW}')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/subseq')\n",
    "diag_el_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/el')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "test_labels_basepath = os.path.join(faultybase_path, 'labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "# ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_el_path = [os.path.join(diag_el_basepath, x) for x in os.listdir(diag_el_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "eval_labels_path = [os.path.join(test_labels_basepath, x) for x in os.listdir(test_labels_basepath)]\n",
    "\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "# ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_feature_path = [x for x in test_el_path if '.DS_Store' not in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "eval_labels_path = [x for x in eval_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    # train_data_path = ref_var_samples_path\n",
    "    raise ValueError('Ref samples for variable window missing')\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "# print('train_data:', train_data_path)\n",
    "print(len(train_data_path))\n",
    "print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "print('eval_labels:\\n', eval_labels_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INST_SEP == 'M2':\n",
    "    #### Load diag_sepAP2_m2\n",
    "\n",
    "    path_diag_sepAP2_m2 = os.path.join(faultybase_path, 'diag_sepAP2_m2/subseq')\n",
    "    path_diag_sepAP2_m2_feature = os.path.join(faultybase_path, 'diag_sepAP2_m2/feature')\n",
    "    path_labels_diag_sepAP2_m2 = os.path.join(path_diag_sepAP2_m2, 'subseq_labels')\n",
    "    print('path_diag_sepAP2_m2:', path_diag_sepAP2_m2)\n",
    "    print('path_labels_diag_sepAP2_m2:', path_labels_diag_sepAP2_m2)\n",
    "\n",
    "elif INST_SEP == 'M3':\n",
    "    #### Load diag_sepAP2_m3\n",
    "\n",
    "    path_diag_sepAP2_m2 = os.path.join(faultybase_path, 'diag_sepAP2_m3/subseq')\n",
    "    path_diag_sepAP2_m2_feature = os.path.join(faultybase_path, 'diag_sepAP2_m3/feature')\n",
    "    path_labels_diag_sepAP2_m2 = os.path.join(path_diag_sepAP2_m2, 'subseq_labels')\n",
    "    print('path_diag_sepAP2_m3:', path_diag_sepAP2_m2)\n",
    "    print('path_labels_diag_sepAP2_m3:', path_labels_diag_sepAP2_m2)\n",
    "\n",
    "elif INST_SEP == 'M0':\n",
    "    #### Load diag_sepAP2_m0\n",
    "\n",
    "    path_diag_sepAP2_m2 = diag_subseq_basepath\n",
    "    path_diag_sepAP2_m2_feature = diag_subseq_basepath\n",
    "    path_labels_diag_sepAP2_m2 = subseq_label_basepath\n",
    "    print('path_diag_sepAP2_m0:', path_diag_sepAP2_m2)\n",
    "    print('path_labels_diag_sepAP2_m0:', path_labels_diag_sepAP2_m2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "files_sepap2 = os.listdir(path_diag_sepAP2_m2)\n",
    "files_sepap2 = [os.path.join(path_diag_sepAP2_m2, x) for x in files_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "files_sepap2 = [x for x in files_sepap2 if os.path.isfile(x)]\n",
    "feature_sepap2 = os.listdir(path_diag_sepAP2_m2_feature)\n",
    "feature_sepap2 = [os.path.join(path_diag_sepAP2_m2_feature, x) for x in feature_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "\n",
    "\n",
    "labels_sepap2 = os.listdir(path_labels_diag_sepAP2_m2)\n",
    "labels_sepap2 = [os.path.join(path_labels_diag_sepAP2_m2, x) for x in labels_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "labels_sepap2 = [x for x in labels_sepap2 if os.path.isfile(x)]\n",
    "print('labels_sepap2:\\n', labels_sepap2)\n",
    "\n",
    "if CODE=='mamba2' and INST_SEP=='M2':\n",
    "    _check_label = labels_sepap2[0].split('/')[-1].split('_')[-1]\n",
    "    print('_check_label:', _check_label)\n",
    "    if _check_label == 'm.json':\n",
    "        print('Updated Labels')\n",
    "    else:\n",
    "        raise ValueError('Labels not updated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Get file names for desired class\n",
    "### class = 0, 1, 2; -1 for all classes\n",
    "CLASS = -1\n",
    "\n",
    "#### Load labels for diag_sepAP2_m2\n",
    "labels_dict = read_json(labels_sepap2[0])\n",
    "print('labels_dict:', labels_dict)\n",
    "\n",
    "\n",
    "key_names = list(labels_dict.keys())\n",
    "# print('key_names:', key_names)\n",
    "\n",
    "sel_ap2_files = []\n",
    "sel_ap2_classes = []\n",
    "sel_ap2_features = []\n",
    "for km in key_names:\n",
    "    print('km:', km)\n",
    "    if INST_SEP == 'M0':\n",
    "        _class = labels_dict[km]\n",
    "    else:\n",
    "        _class = labels_dict[km][0]\n",
    "    print('class:', _class)\n",
    "\n",
    "    ### exclude samples with class -1\n",
    "    if _class != -1:\n",
    "        if _class == CLASS or CLASS == -1:\n",
    "            _file_name = os.path.join(path_diag_sepAP2_m2, km+'.json')\n",
    "            _feature_name = os.path.join(path_diag_sepAP2_m2_feature, km+'.json')\n",
    "            print('file:', _file_name)\n",
    "            ############# check if file exists or not\n",
    "            if os.path.isfile(_file_name):\n",
    "                print('file exists')\n",
    "                sel_ap2_files.append(_file_name)\n",
    "                sel_ap2_classes.append(_class)\n",
    "                sel_ap2_features.append(_feature_name)\n",
    "            else:\n",
    "                raise ValueError('File not found:', _file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_ap2_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load files\n",
    "sel_subseq = []\n",
    "sel_features = []\n",
    "og_subseq = []\n",
    "og_features = []\n",
    "for file, feature, clas in zip(sel_ap2_files, sel_ap2_features, sel_ap2_classes):\n",
    "    print('file:', file)\n",
    "    det_subseq = read_json(file)\n",
    "    # print('subseq:', det_subseq)\n",
    "\n",
    "    ###### store event ids from detected subseq to use as features\n",
    "    _sel_subseq = []\n",
    "    for event in det_subseq:\n",
    "        e_id = event[0]\n",
    "        _sel_subseq.append(e_id)\n",
    "    _sel_subseq = np.array(_sel_subseq)\n",
    "    print('subseq:', _sel_subseq)\n",
    "         \n",
    "    #### load features\n",
    "    print('classes:', clas)\n",
    "    det_feat = read_json(feature)\n",
    "    det_feat = np.array(det_feat)\n",
    "    print('feature:', det_feat)\n",
    "\n",
    "    ### pad features\n",
    "    if INST_SEP == 'M2':\n",
    "        _FEAT_LEN = 5\n",
    "    elif INST_SEP == 'M3':\n",
    "        _FEAT_LEN = 5\n",
    "    elif INST_SEP == 'M0':\n",
    "        _FEAT_LEN = 500    ### max length of detection can be 500\n",
    "\n",
    "    len_feat = det_feat.shape[0]\n",
    "    len_subseq = _sel_subseq.shape[0]\n",
    "    print('len_feat:', len_feat)\n",
    "    pad_num1 = _FEAT_LEN - len_feat\n",
    "    pad_num2 = _FEAT_LEN - len_subseq\n",
    "    if pad_num1 != pad_num2:\n",
    "        print('Padding numbers for features and subseq do not match:', pad_num1, pad_num2)\n",
    "\n",
    "\n",
    "    # if pad_num1 > 0:\n",
    "    if pad_num2 > 0:\n",
    "        pad_feat = np.pad(det_feat, (0, pad_num1), 'constant', constant_values=(0))\n",
    "        pad_subseq = np.pad(_sel_subseq, (0, pad_num2), 'constant', constant_values=(0))\n",
    "    else:\n",
    "        pad_feat = det_feat[:_FEAT_LEN]\n",
    "        pad_subseq = _sel_subseq[:_FEAT_LEN]\n",
    "\n",
    "    if INST_SEP == 'M0':\n",
    "        sel_features.append(pad_subseq)\n",
    "    else:\n",
    "        sel_features.append(pad_feat)\n",
    "        sel_subseq.append(pad_subseq)\n",
    "        og_features.append(det_feat)\n",
    "        og_subseq.append(_sel_subseq)\n",
    "\n",
    "    print(len(pad_feat), len(pad_subseq))\n",
    "    print('')\n",
    "\n",
    "    # break\n",
    "\n",
    "#####################################################################\n",
    "############################ Attention ###############################\n",
    "#####################################################################\n",
    "### trying with subseq as features\n",
    "\n",
    "# sel_features = np.array(sel_features)\n",
    "sel_features = np.array(sel_subseq)\n",
    "# print('padded feature:', sel_features)\n",
    "# print('padded_subseq:', sel_subseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_subseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example variables\n",
    "# padded_features = np.random.rand(x, 500)  # Replace with your actual feature matrix\n",
    "# test_files = [\"file1\", \"file2\", ..., \"fileX\"]  # Replace with your actual file names\n",
    "# test_class = [\"label1\", \"label2\", ..., \"labelX\"]  # Replace with your actual labels\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "padded_features_normalized = scaler.fit_transform(sel_features)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric=\"euclidean\")\n",
    "dbscan.fit(padded_features_normalized)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = dbscan.labels_\n",
    "\n",
    "# Group file names and labels by cluster\n",
    "clusters = {}\n",
    "for i, cluster in enumerate(cluster_labels):\n",
    "    if cluster not in clusters:\n",
    "        clusters[cluster] = {\"files\": [], \"labels\": [], \"features\": []}\n",
    "    clusters[cluster][\"files\"].append(sel_ap2_files[i])\n",
    "    clusters[cluster][\"labels\"].append(sel_ap2_classes[i])\n",
    "    clusters[cluster][\"features\"].append(sel_features[i]) \n",
    "\n",
    "# Print the clusters\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    print(f\"  Number of files: {len(cluster_data['files'])}\")\n",
    "    \n",
    "\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    # print(f\"  Files: {cluster_data['files']}\")\n",
    "    # print(f\"  Labels: {cluster_data['labels']}\")\n",
    "    files = cluster_data['files']\n",
    "    labels = cluster_data['labels']\n",
    "    features = cluster_data['features']\n",
    "    for file, label, feat in zip(files, labels, features):\n",
    "        print(file)\n",
    "        print(label)\n",
    "        print(feat[:50])\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INST_SEP != 'M0':\n",
    "    # Flatten the labels by joining them into a single string per instance\n",
    "    ground_truth = np.array([\"_\".join(map(str, str(labels))) for labels in sel_ap2_classes])\n",
    "    print('ground_truth:', ground_truth)\n",
    "\n",
    "    # Convert flattened labels to numeric format\n",
    "    unique_classes = list(set(ground_truth))  # Get unique class labels\n",
    "    class_to_int = {cls: idx for idx, cls in enumerate(unique_classes)}  # Map each class to an integer\n",
    "    ground_truth_numeric = np.array([class_to_int[cls] for cls in ground_truth])\n",
    "    int_to_class = {v: k for k, v in class_to_int.items()}  # Reverse mapping\n",
    "else:\n",
    "    # Flatten the labels by joining them into a single string per instance\n",
    "    ground_truth = np.array([\"_\".join(map(str, str(labels))) for labels in sel_ap2_classes])\n",
    "    print('ground_truth:', ground_truth)\n",
    "\n",
    "    # Convert flattened labels to numeric format\n",
    "    unique_classes = list(set(ground_truth))  # Get unique class labels\n",
    "    class_to_int = {cls: idx for idx, cls in enumerate(unique_classes)}  # Map each class to an integer\n",
    "    ground_truth_numeric = np.array([class_to_int[cls] for cls in ground_truth])\n",
    "    print(ground_truth_numeric)\n",
    "    int_to_class = {v: k for k, v in class_to_int.items()}  # Reverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show mapping of class to int\n",
    "print('class_to_int:', class_to_int)\n",
    "print('int_to_class:', int_to_class)\n",
    "# print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def map_clusters_to_ground_truth(labels, ground_truth):\n",
    "    # Find unique cluster labels\n",
    "    unique_clusters = np.unique(labels)\n",
    "\n",
    "    # Map each cluster to its most common ground truth label\n",
    "    cluster_to_label = {}\n",
    "    used_labels = []\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        # Extract ground truth labels for samples in this cluster\n",
    "        # print(labels, cluster)\n",
    "        mask = (labels == cluster)\n",
    "        # print('mask:', mask)\n",
    "        gt_labels_in_cluster = ground_truth[mask]\n",
    "        \n",
    "        # Find the most common ground truth label\n",
    "        # print('gt_labels_in_cluster:', gt_labels_in_cluster)\n",
    "        # print(Counter(gt_labels_in_cluster))\n",
    "        # print(Counter(gt_labels_in_cluster).most_common(1)[0])\n",
    "        # print('')\n",
    "        ### following logic allows multiple cluster for single class, and individual cluster if only one instance of that type, but assigns -1 if a cluster for that label already exisits and a single sample with sample label is found\n",
    "        most_common_label, count = Counter(gt_labels_in_cluster).most_common(1)[0]\n",
    "        if count > 1:\n",
    "            cluster_to_label[cluster] = most_common_label\n",
    "            used_labels.append(most_common_label)\n",
    "        elif most_common_label not in used_labels:\n",
    "            cluster_to_label[cluster] = most_common_label\n",
    "            used_labels.append(most_common_label)\n",
    "        else:\n",
    "            cluster_to_label[cluster] = -1  ### if no majority class, assign -1\n",
    "\n",
    "        # cluster_to_label[cluster] = most_common_label\n",
    "\n",
    "    return cluster_to_label\n",
    "\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "# dbscan.fit(padded_features_normalized)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Filter out noise points (label -1 indicates noise in DBSCAN), consider this as False Negatives\n",
    "filtered_indices = labels != -1\n",
    "noise_indices = labels == -1\n",
    "\n",
    "# print('filtered_indices', filtered_indices)\n",
    "# filtered_labels = labels[filtered_indices]\n",
    "# filtered_ground_truth = ground_truth[filtered_indices]\n",
    "# print('filtered labels', filtered_labels)\n",
    "# print('filtered_ground_truth', filtered_ground_truth)\n",
    "\n",
    "filtered_labels = []\n",
    "filtered_ground_truth = []\n",
    "invalid_label = 100 ### DBSCAN noise label\n",
    "for lb, gtm in zip(labels, ground_truth):\n",
    "    if lb != -1:\n",
    "        filtered_labels.append(lb)\n",
    "        filtered_ground_truth.append(gtm)\n",
    "        # print(lb, gtm)\n",
    "    else:\n",
    "        # print(invalid_label, gtm)\n",
    "        filtered_labels.append(invalid_label)\n",
    "        # filtered_labels.append(100)  ### keep 100 for noise\n",
    "        filtered_ground_truth.append(gtm)\n",
    "        invalid_label += 1\n",
    "filtered_labels = np.array(filtered_labels)\n",
    "filtered_ground_truth = np.array(filtered_ground_truth)\n",
    "print('filtered labels', filtered_labels)\n",
    "print('filtered_ground_truth', filtered_ground_truth)\n",
    "print('')\n",
    "\n",
    "#### every samples identified as noise is assigned a seperate cluster, these are False Negatives for DBSCAN\n",
    "### calculate Homogenity and Completeness of the clusters\n",
    "homogeneity = homogeneity_score(filtered_ground_truth, filtered_labels)\n",
    "completeness = completeness_score(filtered_ground_truth, filtered_labels)\n",
    "print(f\"Homogeneity: {homogeneity:.4f}\")\n",
    "print(f\"Completeness: {completeness:.4f}\")\n",
    "print('')\n",
    "\n",
    "#### get the groundtruth class of majority samples in each cluster. This represents the correct class of the cluster\n",
    "cluster_to_label = map_clusters_to_ground_truth(filtered_labels, filtered_ground_truth)\n",
    "for cluster, label in cluster_to_label.items():\n",
    "    print(f\"Cluster {cluster} â†’ Label {label}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "## Step 2: Predict label for each sample using cluster mapping, as the label of majority sample to the cluster and all its samples, except for noise samples.\n",
    "predicted_labels = np.array([cluster_to_label[cl] for cl in filtered_labels])\n",
    "# predicted_labels = []\n",
    "# for cl in filtered_labels:\n",
    "#     if 100 <= cl <= 999:\n",
    "#         predicted_labels.append(cl)   ### keep the noise samples as is\n",
    "#         # predicted_labels.append(-1)   ### assign all noise samples to a single class -1\n",
    "#     else:\n",
    "#         predicted_labels.append(cluster_to_label[cl])\n",
    "# predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# print('predicted_labels', predicted_labels)\n",
    "# print('filtered_ground_truth', filtered_ground_truth)\n",
    "# print('')\n",
    "\n",
    "print('predicted_labels', 'filtered_ground_truth')\n",
    "for pl, fgt in zip(predicted_labels, filtered_ground_truth):\n",
    "    print(pl, fgt)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Step 3: Compute accuracy and misclassification rate\n",
    "correct = np.sum(predicted_labels == filtered_ground_truth)\n",
    "total = len(filtered_ground_truth)\n",
    "accuracy = correct / total\n",
    "misclassification_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Misclassification Rate: {misclassification_rate:.4f}\")\n",
    "\n",
    "\n",
    "#### calculate NMI to check how well does clustering labels agree with GT\n",
    "nmi = adjusted_mutual_info_score(filtered_ground_truth, predicted_labels)\n",
    "nmi2 = normalized_mutual_info_score(filtered_ground_truth, predicted_labels)\n",
    "\n",
    "\n",
    "print(f\"Adjusted Mutual Information (AMI): {nmi:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi2:.4f}\")\n",
    "\n",
    "\n",
    "# print('')\n",
    "# ### for each cluster print the respective ground truth classes\n",
    "# from collections import defaultdict\n",
    "# cluster_to_classes = defaultdict(list)\n",
    "# for lbl, true_cls in zip(filtered_labels, filtered_ground_truth):\n",
    "#     # true_cls = int_to_class[true_cls]  ### convert back to original class label\n",
    "#     cluster_to_classes[lbl].append(true_cls)\n",
    "# for cluster_id, classes in cluster_to_classes.items():\n",
    "#     unique, counts = np.unique(classes, return_counts=True)\n",
    "#     class_count = dict(zip(unique, counts))\n",
    "#     print(f\"Cluster {cluster_id}: Class distribution: {class_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### map cluster labels with groundtruths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adjusted Mutual Information (AMI)\n",
    "\n",
    "- The Adjusted Mutual Information (AMI) score has a range of approximately -0.5 to 1.0, though it is often centered around 0.0. A score of 1.0 signifies perfect agreement between two labelings, while values close to 0.0 indicate chance-level agreement. Negative values occur when the observed agreement is worse than what would be expected by chance. \n",
    "Interpreting AMI Scores\n",
    "\n",
    "- 1.0: Perfect agreement between the two sets of labels or clusterings. \n",
    "- 0.0: The agreement between the labelings is no better than random chance. \n",
    "- Negative values: Indicate that the clusterings are worse than random, meaning there is less agreement than would be expected by chance. \n",
    "\n",
    "#### Why the Range is Not \n",
    "Unlike simpler metrics like Normalized Mutual Information, AMI is \"adjusted\" for chance. This means it corrects for the possibility of accidentally finding matches in random clusterings. Just as the Adjusted Rand Index can have negative values, so can AMI, representing a level of agreement that is even less than random. \n",
    "\n",
    "### Normalized Mutual Information (NMI)\n",
    "\n",
    "- It is the measure of similarity between two different labels for the same data. For example, the ground truth labels and the labels assigned by a clustering algorithm.\n",
    "- The NMI score ranges from 0 to 1, where 1 indicates perfect agreement between the two sets of labels, and 0 indicates no mutual information (i.e., the labels are independent).\n",
    "- The absolute values of label does not mater, only the relative grouping matters. For eg. if ground truth labels are [0, 0, 1, 1] and predicted labels are [1, 1, 0, 0], the NMI score will be 1 because the clustering structure is identical.\n",
    "- It is also symmetric, meaning that NMI(A, B) = NMI(B, A).\n",
    "\n",
    "### Silhouette Score\n",
    "- The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "- The silhouette score for a single sample is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for that sample. \n",
    "- The silhouette score for a set of samples is given as the mean of the silhouette scores for each sample.\n",
    "- The silhouette score ranges from -1 to +1, where a high value indicates that the sample is well matched to its own cluster and poorly matched to neighboring clusters. \n",
    "- If most samples have a high silhouette score, then the clustering configuration is appropriate. If many points have a low or negative silhouette score, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "### Homogeneity\n",
    "- A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "- Homogeneity is bounded between 0 and 1, with 1 being perfectly homogeneous (i.e., each cluster contains only members of a single class).\n",
    "\n",
    "### Completeness (Ignore)\n",
    "- A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "- Completeness is also bounded between 0 and 1, with 1 being perfectly complete (i.e., all members of a class are assigned to the same cluster).\n",
    "\n",
    "### V-Measure (Ignore)\n",
    "- The V-measure is the harmonic mean of homogeneity and completeness. It provides a single score that balances both aspects of clustering quality.\n",
    "- The V-measure is also bounded between 0 and 1, with 1 indicating perfect clustering in terms of both homogeneity and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print a list of file names for each cluster along with their ground truth classes\n",
    "from collections import defaultdict\n",
    "cluster_to_files = defaultdict(list)\n",
    "for lbl, true_cls, file in zip(filtered_labels, filtered_ground_truth, np.array(sel_ap2_files)[filtered_indices]):\n",
    "    true_cls = int_to_class[true_cls]  ### convert back to original class label\n",
    "    cluster_to_files[lbl].append((file, true_cls))\n",
    "for cluster_id, file_cls_list in cluster_to_files.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    for file, cls in file_cls_list:\n",
    "        print(f\"  File: {file}, Class: {cls}\")\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Example: padded_features is a (46, 31) matrix\n",
    "# padded_features = np.random.rand(46, 31)  # Replace with your actual data\n",
    "# test_class = [\"Class1\", \"Class2\", ..., \"Class46\"]  # Replace with your actual class labels\n",
    "\n",
    "# Define a function to calculate and plot similarity heatmap\n",
    "def plot_similarity_heatmap(features, labels, metric, title):\n",
    "    # Step 1: Calculate pairwise distances\n",
    "    similarity_matrix = cdist(features, features, metric=metric)\n",
    "\n",
    "    #### sort features based on similarity to group similar features together\n",
    "    sorted_indices = np.argsort(similarity_matrix.sum(axis=1))\n",
    "    similarity_matrix = similarity_matrix[sorted_indices][:, sorted_indices]\n",
    "    labels = np.array(labels)[sorted_indices]\n",
    "\n",
    "\n",
    "    # Step 2: Generate a heatmap\n",
    "    plt.figure(figsize=(22, 20))\n",
    "    ax = sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        annot=False,  # Set to True if you want to display values\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"viridis\",\n",
    "        cbar=True,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels\n",
    "    )\n",
    "    plt.title(title)\n",
    "\n",
    "    # Place x-axis ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')  # Move x-axis label to the top\n",
    "\n",
    "    # Rotate tick labels for better readability\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Metrics to calculate\n",
    "metrics = {\n",
    "    \"euclidean\": \"Euclidean Distance\",\n",
    "    \"cityblock\": \"Manhattan Distance\",\n",
    "    \"chebyshev\": \"Chebyshev Distance\",\n",
    "    # \"cosine\": \"Cosine Similarity\",\n",
    "    # \"correlation\": \"Correlation Distance\"\n",
    "}\n",
    "\n",
    "# ### sort the features in ascending order of their class labels\n",
    "# sorted_indices = np.argsort(sel_ap2_classes)\n",
    "# sel_features = sel_features[sorted_indices]\n",
    "# sel_ap2_classes = np.array(sel_ap2_classes)[sorted_indices]\n",
    "\n",
    "\n",
    "# Generate heatmaps for each metric\n",
    "for metric, title in metrics.items():\n",
    "    plot_similarity_heatmap(sel_features, sel_ap2_classes, metric, f\"Pairwise Feature Similarity ({title})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
