{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "- label manually for instances of class 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 - method 2 - step2 (Extract Features)\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'mamba2'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "WINDOW = 500                 ### window size for subsequence\n",
    "SUBSEQ =  'diag_subseq'        # 'diag_subseq' , diag_subseq_multi       ### subsequence type, diag_subseq, subseq\n",
    "\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, f'diag_refsamples{WINDOW}')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/subseq')\n",
    "diag_el_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/el')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "test_labels_basepath = os.path.join(faultybase_path, 'labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "# ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_el_path = [os.path.join(diag_el_basepath, x) for x in os.listdir(diag_el_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "eval_labels_path = [os.path.join(test_labels_basepath, x) for x in os.listdir(test_labels_basepath)]\n",
    "\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "# ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_feature_path = [x for x in test_el_path if '.DS_Store' not in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "eval_labels_path = [x for x in eval_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    # train_data_path = ref_var_samples_path\n",
    "    raise ValueError('Ref samples for variable window missing')\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "# print('train_data:', train_data_path)\n",
    "print(len(train_data_path))\n",
    "# print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "print('eval_labels:\\n', eval_labels_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load diag_sepAP2_m2\n",
    "\n",
    "path_diag_sepAP2_m2 = os.path.join(faultybase_path, 'diag_sepAP2_m2/subseq')\n",
    "path_diag_sepAP2_m2_feature = os.path.join(faultybase_path, 'diag_sepAP2_m2/feature')\n",
    "path_labels_diag_sepAP2_m2 = os.path.join(path_diag_sepAP2_m2, 'subseq_labels')\n",
    "print('path_diag_sepAP2_m2:', path_diag_sepAP2_m2)\n",
    "print('path_labels_diag_sepAP2_m2:', path_labels_diag_sepAP2_m2)\n",
    "\n",
    "files_sepap2 = os.listdir(path_diag_sepAP2_m2)\n",
    "files_sepap2 = [os.path.join(path_diag_sepAP2_m2, x) for x in files_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "files_sepap2 = [x for x in files_sepap2 if os.path.isfile(x)]\n",
    "feature_sepap2 = os.listdir(path_diag_sepAP2_m2_feature)\n",
    "feature_sepap2 = [os.path.join(path_diag_sepAP2_m2_feature, x) for x in feature_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "\n",
    "\n",
    "labels_sepap2 = os.listdir(path_labels_diag_sepAP2_m2)\n",
    "labels_sepap2 = [os.path.join(path_labels_diag_sepAP2_m2, x) for x in labels_sepap2 if '.DS_Store' not in x]    ### remove .DS_Store\n",
    "labels_sepap2 = [x for x in labels_sepap2 if os.path.isfile(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Get file names for desired class\n",
    "### class = 0, 1, 2; -1 for all classes\n",
    "CLASS = 2\n",
    "\n",
    "#### Load labels for diag_sepAP2_m2\n",
    "labels_dict = read_json(labels_sepap2[0])\n",
    "print('labels_dict:', labels_dict)\n",
    "\n",
    "\n",
    "key_names = list(labels_dict.keys())\n",
    "# print('key_names:', key_names)\n",
    "\n",
    "sel_ap2_files = []\n",
    "sel_ap2_classes = []\n",
    "sel_ap2_features = []\n",
    "for km in key_names:\n",
    "    print('km:', km)\n",
    "    _class = labels_dict[km][0]\n",
    "    print('class:', _class)\n",
    "\n",
    "    if _class == CLASS or CLASS == -1:\n",
    "        _file_name = os.path.join(path_diag_sepAP2_m2, km+'.json')\n",
    "        _feature_name = os.path.join(path_diag_sepAP2_m2_feature, km+'.json')\n",
    "        print('file:', _file_name)\n",
    "        ############# check if file exists or not\n",
    "        if os.path.isfile(_file_name):\n",
    "            print('file exists')\n",
    "            sel_ap2_files.append(_file_name)\n",
    "            sel_ap2_classes.append(_class)\n",
    "            sel_ap2_features.append(_feature_name)\n",
    "        else:\n",
    "           raise ValueError('File not found:', _file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_ap2_features), #len(sel_ap2_files), len(sel_ap2_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CODE == 'theft_protection':\n",
    "        EVENT_MEDIAN = 22\n",
    "        WINDOW = round(2 * EVENT_MEDIAN)    ### round(2 * median event range)\n",
    "        SLIDING_WINDOW = round(EVENT_MEDIAN * 1.5)    ### round( 0,4 * WINDOW)\n",
    "        # SLIDING_WINDOW = round(WINDOW - 25)   \n",
    "        print('WINDOW:', WINDOW)\n",
    "        print('SLIDING_WINDOW:', SLIDING_WINDOW) \n",
    "elif CODE == 'mamba2':\n",
    "    EVENT_MEDIAN = 57\n",
    "    WINDOW = round(2 * EVENT_MEDIAN)  ### round(1,8 * median event range)\n",
    "    SLIDING_WINDOW = round(EVENT_MEDIAN * 1.5)    ### round( 0,7 * WINDOW)\n",
    "    # SLIDING_WINDOW = round(WINDOW - 20)    \n",
    "    print('WINDOW:', WINDOW)\n",
    "    print('SLIDING_WINDOW:', SLIDING_WINDOW)\n",
    "\n",
    "#### Load files\n",
    "sel_subseq = []\n",
    "sel_features = []\n",
    "for file, feature, clas in zip(sel_ap2_files, sel_ap2_features, sel_ap2_classes):\n",
    "    print('file:', file)\n",
    "    det_subseq = read_json(file)\n",
    "    print('subseq:', det_subseq)\n",
    "    sel_subseq.append(det_subseq)\n",
    "\n",
    "    print('classes:', clas)\n",
    "    det_feat = read_json(feature)\n",
    "    det_feat = np.array(det_feat)\n",
    "    print('feature:', det_feat)\n",
    "\n",
    "    ### pad features\n",
    "    len_feat = det_feat.shape[0]\n",
    "    pad_num = 500 - len_feat\n",
    "    if pad_num > 0:\n",
    "        pad_feat = np.pad(det_feat, (0, pad_num), 'constant', constant_values=(0))\n",
    "    elif pad_num < 0:\n",
    "        pad_feat = det_feat[:500]\n",
    "        \n",
    "    sel_features.append(pad_feat)\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    # break\n",
    "sel_features = np.array(sel_features)\n",
    "print('padded feature:', sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example variables\n",
    "# padded_features = np.random.rand(x, 500)  # Replace with your actual feature matrix\n",
    "# test_files = [\"file1\", \"file2\", ..., \"fileX\"]  # Replace with your actual file names\n",
    "# test_class = [\"label1\", \"label2\", ..., \"labelX\"]  # Replace with your actual labels\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "padded_features_normalized = scaler.fit_transform(sel_features)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric=\"euclidean\")\n",
    "dbscan.fit(padded_features_normalized)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = dbscan.labels_\n",
    "\n",
    "# Group file names and labels by cluster\n",
    "clusters = {}\n",
    "for i, cluster in enumerate(cluster_labels):\n",
    "    if cluster not in clusters:\n",
    "        clusters[cluster] = {\"files\": [], \"labels\": [], \"features\": []}\n",
    "    clusters[cluster][\"files\"].append(sel_ap2_files[i])\n",
    "    clusters[cluster][\"labels\"].append(sel_ap2_classes[i])\n",
    "    clusters[cluster][\"features\"].append(sel_features[i]) \n",
    "\n",
    "# Print the clusters\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    print(f\"  Number of files: {len(cluster_data['files'])}\")\n",
    "    \n",
    "\n",
    "for cluster_id, cluster_data in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    # print(f\"  Files: {cluster_data['files']}\")\n",
    "    # print(f\"  Labels: {cluster_data['labels']}\")\n",
    "    files = cluster_data['files']\n",
    "    labels = cluster_data['labels']\n",
    "    features = cluster_data['features']\n",
    "    for file, label, feat in zip(files, labels, features):\n",
    "        print(file)\n",
    "        print(label)\n",
    "        print(feat[:50])\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the labels by joining them into a single string per instance\n",
    "ground_truth = np.array([\"_\".join(map(str, str(labels))) for labels in sel_ap2_classes])\n",
    "\n",
    "# Convert flattened labels to numeric format\n",
    "unique_classes = list(set(ground_truth))  # Get unique class labels\n",
    "class_to_int = {cls: idx for idx, cls in enumerate(unique_classes)}  # Map each class to an integer\n",
    "ground_truth_numeric = np.array([class_to_int[cls] for cls in ground_truth])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan.fit(padded_features_normalized)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Filter out noise points (label -1 indicates noise in DBSCAN)\n",
    "filtered_indices = labels != -1\n",
    "print(filtered_indices)\n",
    "filtered_labels = labels[filtered_indices]\n",
    "filtered_ground_truth = ground_truth_numeric[filtered_indices]\n",
    "\n",
    "# Evaluation Metrics\n",
    "ari = adjusted_rand_score(filtered_ground_truth, filtered_labels)\n",
    "nmi = normalized_mutual_info_score(filtered_ground_truth, filtered_labels)\n",
    "silhouette = silhouette_score(padded_features_normalized[filtered_indices], filtered_labels)\n",
    "homogeneity = homogeneity_score(filtered_ground_truth, filtered_labels)\n",
    "completeness = completeness_score(filtered_ground_truth, filtered_labels)\n",
    "v_measure = v_measure_score(filtered_ground_truth, filtered_labels)\n",
    "\n",
    "# Print results\n",
    "# print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"Homogeneity: {homogeneity:.4f}\")\n",
    "print(f\"Completeness: {completeness:.4f}\")\n",
    "print(f\"V-Measure: {v_measure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
