{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Fixed Window - Approach 1\n",
    "- Approach 1 refers to syncing only the first half of traces before anomaly, for feature extraction\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- In this approach we only consider detections that has only single instance of anomaly\n",
    "- We tested this approach across all applications and combinations of multiplr applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "all_applications = ['theft_protection', 'mamba2', 'lora_ducy']  ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "app_paths = defaultdict(dict)\n",
    "\n",
    "for CODE in all_applications:\n",
    "\n",
    "    # CODE = 'theft_protection'       ### application (code)\n",
    "    BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "    BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "    THREAD = 'single'           ### single, multi\n",
    "    VER = 3                     ### format of data collection\n",
    "\n",
    "    base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "    normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "    faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "    \n",
    "    app_paths[CODE]['base_dir'] = base_dir\n",
    "    app_paths[CODE]['normalbase_path'] = normalbase_path\n",
    "    app_paths[CODE]['faultybase_path'] = faultybase_path\n",
    "\n",
    "    print(normalbase_path)\n",
    "    print(faultybase_path)\n",
    "\n",
    "\n",
    "    ################# configuration - diag ################\n",
    "    IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "    #####################################################\n",
    "\n",
    "\n",
    "    ref_samples_basepath = os.path.join(normalbase_path, 'diag_refsamples')\n",
    "    ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "    diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "    subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "\n",
    "\n",
    "    print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "    print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "    print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "    ######### get paths #######################\n",
    "    ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "    ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "    train_varlist_path = os.listdir(normalbase_path)\n",
    "    train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "    ######### get paths #######################\n",
    "    paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "    test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "    test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "\n",
    "    # ### remove.Ds_store from all lists\n",
    "    train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "    varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "    paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "    ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "    ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "    test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "    test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "\n",
    "    varlist_path.sort()\n",
    "\n",
    "    # print(paths_log)\n",
    "    # print(paths_traces)\n",
    "    # print(varlist_path)\n",
    "    # print(paths_label)\n",
    "\n",
    "    if IS_VAR_WINDOW:\n",
    "        train_data_path = ref_var_samples_path\n",
    "    else:\n",
    "        train_data_path = ref_samples_path\n",
    "\n",
    "    test_data_path = test_subseq_path\n",
    "\n",
    "    print('train_data:\\n', train_data_path)\n",
    "    print(len(train_data_path))\n",
    "    print('test_data:\\n', test_data_path)\n",
    "    print(len(test_data_path))\n",
    "    print('test_labels:\\n', test_labels_path)\n",
    "\n",
    "    app_paths[CODE]['train_data_path'] = train_data_path\n",
    "    app_paths[CODE]['test_data_path'] = test_data_path\n",
    "    app_paths[CODE]['train_varlist_path'] = train_varlist_path\n",
    "    app_paths[CODE]['varlist_path'] = varlist_path\n",
    "    app_paths[CODE]['paths_label'] = paths_label\n",
    "    app_paths[CODE]['ref_samples_path'] = ref_samples_path\n",
    "    app_paths[CODE]['ref_var_samples_path'] = ref_var_samples_path\n",
    "    app_paths[CODE]['test_subseq_path'] = test_subseq_path\n",
    "    app_paths[CODE]['test_labels_path'] = test_labels_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data_path[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Feature Vectors\n",
    "\n",
    "- For fixed window size, load all the ref samples before hand\n",
    "- For variable window, load the map_len; further load files only with the suitable len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "all_test_feature_vectors = []\n",
    "for CODE in all_applications:\n",
    "    train_data_path = app_paths[CODE]['train_data_path']\n",
    "    test_data_path = app_paths[CODE]['test_data_path']\n",
    "    ### load all the reference samples (fixed window size)\n",
    "    ref_samples = []\n",
    "    for ref_sample in train_data_path:\n",
    "        ref_samples.append(read_traces(ref_sample))\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    #########################################################\n",
    "\n",
    "    ### load the test samples and compare with the reference samples\n",
    "    test_feature_vectors = []  ### [(test_data, (feat1_vector, feat2_vector)), (), (), ...]\n",
    "    missing_features = []   ### [(test_data, missing_feature), (), (), ...]\n",
    "    for test_data in test_data_path[0:]:\n",
    "        print('test_data:', test_data)\n",
    "        ### read the subseq\n",
    "        test_trace = read_traces(test_data)\n",
    "        print('test_trace:', test_trace)\n",
    "        test_data_len = len(test_trace)\n",
    "        print('test_data_len:', test_data_len)\n",
    "\n",
    "        if test_data_len > 500:\n",
    "            # print('test data length is more than 500, skipping...')\n",
    "            # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "            # continue\n",
    "\n",
    "            print('test data length is more than 500, truncating...')\n",
    "            test_trace = test_trace[:500]\n",
    "            test_data_len = 500\n",
    "        \n",
    "        ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "        test_events = []\n",
    "        test_intervals = []\n",
    "        prev_time = test_trace[0][1]\n",
    "        time_diff = 0\n",
    "        for x in test_trace:\n",
    "            time_diff = x[1] - prev_time\n",
    "            test_intervals.append(time_diff)\n",
    "            prev_time = x[1]\n",
    "            test_events.append(x[0])\n",
    "\n",
    "        assert len(test_events) == len(test_intervals) == test_data_len\n",
    "\n",
    "        ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "        shortlisted_ref_samples = []\n",
    "        for ref_sample in ref_samples:\n",
    "            # print('ref_sample:', ref_sample[0][:5])\n",
    "            if ref_sample[0][:5] == test_events[:5]:\n",
    "                ref_sample = (ref_sample[0][:test_data_len], ref_sample[1][:test_data_len])\n",
    "                shortlisted_ref_samples.append(ref_sample)\n",
    "\n",
    "        ### deduplicate reference samples\n",
    "        # print('shortlisted_ref_samples:', len(shortlisted_ref_samples))\n",
    "        dedup_ref_samples = []\n",
    "        _dedup_events = []\n",
    "        for ref_sample in shortlisted_ref_samples:\n",
    "            # print('ref_sample:', ref_sample[0])\n",
    "            if ref_sample[0] not in _dedup_events:\n",
    "                dedup_ref_samples.append(ref_sample)\n",
    "                _dedup_events.append(ref_sample[0])\n",
    "        # print('dedup_ref_samples:', len(dedup_ref_samples))\n",
    "        shortlisted_ref_samples = dedup_ref_samples\n",
    "                    \n",
    "\n",
    "        ### generate feature vector for the test_trace with respect to each of the shortlisted_ref_samples\n",
    "        '''\n",
    "        Feature generation:\n",
    "        - take difference of the events and intervals of the test_trace with the shortlisted_ref_samples\n",
    "        '''\n",
    "        # print('ref samples with matching first 5 events:', np.array(shortlisted_ref_samples).shape)\n",
    "        if shortlisted_ref_samples != []:\n",
    "            shortlisted_features = []\n",
    "            feature_vectors = []\n",
    "            for ref_sample in shortlisted_ref_samples:\n",
    "                # print('ref_sample:', ref_sample[1])\n",
    "                sel_ref_event = ref_sample[0][:test_data_len]\n",
    "                sel_ref_interval = ref_sample[1][:test_data_len]\n",
    "                # print('sel_ref_event:', len(sel_ref_event), len(sel_ref_interval))\n",
    "                assert (len(sel_ref_event) == len(sel_ref_interval) == test_data_len)\n",
    "\n",
    "                ### generate feature vector\n",
    "                feat1_vector = []\n",
    "                feat2_vector = []\n",
    "                for i in range(test_data_len):\n",
    "                    feat1 = test_events[i] - sel_ref_event[i]\n",
    "                    feat2 = test_intervals[i] - sel_ref_interval[i]\n",
    "                    ### if the difference in interval is within 500 ms, then consider it as same, as we consider tolerance of 500 ms based on observation\n",
    "                    feat2 = [0 if feat2 >= -500 and feat2 <= 500 else feat2 ][0] \n",
    "                    feat1_vector.append(feat1)\n",
    "                    feat2_vector.append(feat2)\n",
    "\n",
    "                feat1_vector = np.array(feat1_vector)\n",
    "                feat2_vector = np.array(feat2_vector)\n",
    "                shortlisted_features.append((feat1_vector, feat2_vector))\n",
    "\n",
    "            \n",
    "            ### count leading zeros in the feature vector\n",
    "            # print('shortlisted_features:', len(shortlisted_features))\n",
    "            zero_count = []\n",
    "            for sf in shortlisted_features:\n",
    "                count = 0\n",
    "                # print(sf[0], sf[1])\n",
    "                for esf, isf in zip(sf[0], sf[1]):\n",
    "                    ### check if events and intervals are same\n",
    "                    if esf == 0 and isf == 0:\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break   ### part of the logic, do not remove\n",
    "\n",
    "                # print('zero_count:', count)\n",
    "                zero_count.append(count)\n",
    "\n",
    "            ### select the feature vector with maximum leading zeros\n",
    "            max_zero_count = max(zero_count)\n",
    "            zero_count = np.array(zero_count)\n",
    "            max_zero_count_ind = np.where(zero_count==max_zero_count)[0]\n",
    "            # print('max number of starting events that are same for ref and test:', max_zero_count)\n",
    "            # print('ref samples with highest matching events in the start:', len(max_zero_count_ind))\n",
    "\n",
    "            ### select the feature vectors with maximum leading zeros\n",
    "            feature_vectors = [ shortlisted_features[i] for i in max_zero_count_ind ]\n",
    "\n",
    "            total_zero_count = []\n",
    "            for features in feature_vectors:\n",
    "                # print('feature:', features)\n",
    "                # print('zero_count:', np.where(features[0]==0)[0].shape)\n",
    "                total_zero_count.append(np.where(features[0]==0)[0].shape[0])\n",
    "            # print('total_zero_count:', total_zero_count)\n",
    "            total_zero_count = np.array(total_zero_count)\n",
    "            min_total_zero_count = min(total_zero_count)\n",
    "            min_total_zero_count_ind = np.where(total_zero_count==min_total_zero_count)[0]\n",
    "            # print('the number of highest number of total zeros:', min_total_zero_count)\n",
    "            print('files that has max number of total zeros:', min_total_zero_count_ind)\n",
    "            feature_vector = [ feature_vectors[i] for i in min_total_zero_count_ind ]\n",
    "            # print('feature_vector:', len(feature_vector))\n",
    "\n",
    "            ### select the first feature vector if multiple shortlisted feature vectors are there\n",
    "            # print(np.array(feature_vector).shape)\n",
    "            if np.array(feature_vector).shape[0] > 1:\n",
    "                print('multiple feature vectors found, selecting the first one')\n",
    "\n",
    "                feature_vector = [feature_vectors[0]]\n",
    "\n",
    "            test_feature_vectors.append((test_data, feature_vector))\n",
    "        else:\n",
    "            print('No shortlisted ref samples found for the test data:', test_data)\n",
    "            missing_features.append((test_data, 'No shortlisted ref samples found'))\n",
    "            \n",
    "            \n",
    "    print('')\n",
    "    # break\n",
    "    all_test_feature_vectors.append((CODE, test_feature_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_axis = np.arange(0, len(test_trace), 1)\n",
    "\n",
    "\n",
    "\n",
    "### prepare test_trace for plotting\n",
    "plot_data = dict()\n",
    "plot_data['subseq'] = test_events   ### y_data (traces)\n",
    "\n",
    "for i, fv in enumerate(feature_vectors):\n",
    "    plot_data[f'feat1_{i}'] = fv[0]\n",
    "    \n",
    "df_feat1 = pd.DataFrame(plot_data)\n",
    "\n",
    "plot_data = dict()\n",
    "plot_data['intervals'] = test_intervals   ### y_data (traces)\n",
    "\n",
    "for i, fv in enumerate(feature_vectors):\n",
    "    plot_data[f'feat2_{i}'] = fv[1]\n",
    "\n",
    "df_feat2 = pd.DataFrame(plot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_feat1, title='features')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_feat2, title='features')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### prepare test trace for plotting\n",
    "# num_trace = []\n",
    "# time_stamp = []\n",
    "# for (t, ts) in test_trace:\n",
    "#     num_trace.extend([t])\n",
    "#     time_stamp.extend([ts])\n",
    "\n",
    "# plot_data = dict()\n",
    "# # plot_data['time'] = time_stamp   ### x_data\n",
    "# print(len(num_trace))\n",
    "# plot_data['test_trace'] = num_trace   ### y_data (traces)\n",
    "\n",
    "# ### prepare ref samples\n",
    "# samples = [ shortlisted_ref_samples[i] for i in max_zero_count_ind ]\n",
    "\n",
    "# for i in max_zero_count_ind:\n",
    "#     print(len(shortlisted_ref_samples[i][0]))\n",
    "#     plot_data[f'sample{i}'] = shortlisted_ref_samples[i][0]\n",
    "    \n",
    "# df_trace = pd.DataFrame(plot_data)\n",
    "\n",
    "# fig = px.line(df_trace, title='event trace')\n",
    "# fig.show()\n",
    "\n",
    "##################################################\n",
    "\n",
    "### prepare test trace for plotting\n",
    "num_trace = []\n",
    "time_stamp = []\n",
    "for (t, ts) in test_trace:\n",
    "    num_trace.extend([t])\n",
    "    time_stamp.extend([ts])\n",
    "\n",
    "plot_data = dict()\n",
    "# plot_data['time'] = time_stamp   ### x_data\n",
    "print(len(num_trace))\n",
    "\n",
    "### prepare ref samples\n",
    "samples = [ shortlisted_ref_samples[i] for i in max_zero_count_ind ]\n",
    "\n",
    "for i in max_zero_count_ind:\n",
    "    print(len(shortlisted_ref_samples[i][0]))\n",
    "    plot_data['test_trace'] = num_trace   ### y_data (traces)\n",
    "    plot_data[f'sample{i}'] = shortlisted_ref_samples[i][0]\n",
    "    df_trace = pd.DataFrame(plot_data)\n",
    "    fig = px.line(df_trace, title='event trace')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = []\n",
    "padded_features = []\n",
    "test_class = []\n",
    "prev_code = ''\n",
    "for i, (CODE, test_feature_vectors) in enumerate(all_test_feature_vectors):\n",
    "    # print(i, CODE)\n",
    "    print('test_feature_vectors:', len(test_feature_vectors))\n",
    "    # continue\n",
    "    test_labels_path = app_paths[CODE]['test_labels_path']\n",
    "    # print('test_labels_path:', test_labels_path)\n",
    "    ### load the labels\n",
    "    test_class_labels = read_json(test_labels_path[0])\n",
    "    print('test_class_labels:', len(test_class_labels))\n",
    "    # print(CODE, test_feature_vectors[0])\n",
    "    # break\n",
    "\n",
    "    ### give unique labels for each anomaly from each application (assumes that features from each application are in sequence)\n",
    "    if prev_code != CODE: \n",
    "        prev_code = CODE   \n",
    "        if test_class == []:\n",
    "            max_class = 0\n",
    "        else:\n",
    "            max_class = max(test_class)\n",
    "        # print('CODE', CODE)\n",
    "        # print('max_class:', max_class)\n",
    "\n",
    "    ### prepare the feature vectors for classification\n",
    "    for test_data, feature_vector in test_feature_vectors:\n",
    "        file_name = test_data.split('/')[-1].split('.')[0]\n",
    "        # print(CODE, test_data)\n",
    "        class_list = test_class_labels[file_name]\n",
    "        # print('class_list:', class_list)\n",
    "        class_label = None\n",
    "        # break\n",
    "\n",
    "        # print('test_data:', test_data)\n",
    "        print('feature_vector:', np.array(feature_vector).shape)\n",
    "        print('test_class_label:', test_class_labels[file_name])\n",
    "\n",
    "        # print(np.array(feature_vector).shape)\n",
    "        if len(class_list) == 1:\n",
    "            ### Hardcode the class label for all applications\n",
    "            if CODE == 'theft_protection':\n",
    "                if class_list[0] == 1:\n",
    "                    class_label = 1\n",
    "                elif class_list[0] == 2:\n",
    "                    class_label = 2\n",
    "                elif class_list[0] == 3:\n",
    "                    class_label = 3\n",
    "            elif CODE == 'mamba2':\n",
    "                if class_list[0] == 1:\n",
    "                    class_label = None    ### exclude comm failure samples\n",
    "                elif class_list[0] == 2 or class_list[0] == 4:   ### combine both the anomaly types since they have same effect\n",
    "                    class_label = 5\n",
    "                elif class_list[0] == 3:\n",
    "                    class_label = 6\n",
    "                # elif class_list[0] == 4:\n",
    "                #     class_label = 7\n",
    "            elif CODE == 'lora_ducy':\n",
    "                if class_list[0] == 5:\n",
    "                    class_label = 8\n",
    "\n",
    "        else:\n",
    "            print('multiple class labels found for the test data:', test_data)\n",
    "            continue\n",
    "            ### only consider samples which has only one type of multiple anomaly instances\n",
    "            # print(set(class_list))\n",
    "            # if len(set(class_list)) == 1:\n",
    "            #     class_label = class_list[0]\n",
    "            # else:\n",
    "            #     class_label = None\n",
    "\n",
    "\n",
    "            \n",
    "        if class_label != None:\n",
    "            feature_vector = np.array(feature_vector)\n",
    "            if feature_vector.shape[0] == 1:\n",
    "                # print('feature_vector:', feature_vector[0].shape)\n",
    "                feat1 = feature_vector[0][0]\n",
    "                feat2 = feature_vector[0][1]\n",
    "                pad_num = 500 - feat1.shape[0]\n",
    "\n",
    "                if pad_num > 0:\n",
    "                    feat1 = np.pad(feat1, (0, pad_num), 'constant', constant_values=(0))\n",
    "                    feat2 = np.pad(feat2, (0, pad_num), 'constant', constant_values=(0))\n",
    "                \n",
    "                \n",
    "                padded_features.append((feat1, feat2))\n",
    "                test_files.append(test_data)\n",
    "                test_class.append(class_label)\n",
    "                # print('class', class_label)\n",
    "\n",
    "                # break\n",
    "            else:\n",
    "                for fv in feature_vector:\n",
    "                    print('feature_vector:', fv.shape)\n",
    "                    feat1 = fv[0]\n",
    "                    feat2 = fv[1]\n",
    "                    pad_num = 500 - feat1.shape[0]\n",
    "\n",
    "                    if pad_num > 0:\n",
    "                        feat1 = np.pad(feat1, (0, pad_num), 'constant', constant_values=(0))\n",
    "                        feat2 = np.pad(feat2, (0, pad_num), 'constant', constant_values=(0))\n",
    "\n",
    "                \n",
    "                    padded_features.append((feat1, feat2))\n",
    "                    test_files.append(test_data)\n",
    "                    test_class.append(class_label)\n",
    "                    # print('class', class_label)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(padded_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_files))\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_class))\n",
    "print(test_class)\n",
    "print(set(test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get count of samples for each label\n",
    "unique, counts = np.unique(test_class, return_counts=True)\n",
    "print('unique:', unique)\n",
    "print('counts:', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N_CLUSTER = len(set(test_class))\n",
    "print('N_CLUSTER:', N_CLUSTER)\n",
    "\n",
    "data = np.array(padded_features)\n",
    "\n",
    "# Reshape the data for clustering\n",
    "num_samples, num_features, num_points = data.shape\n",
    "\n",
    "# Flatten the feature vectors (reshape to (48, 1000))\n",
    "data_reshaped = data.reshape(num_samples, num_features * num_points)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_reshaped)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(init=\"k-means++\", max_iter=1000, n_clusters=N_CLUSTER, n_init='auto' )   # n_clusters=N_CLUSTER, random_state=0, n_init=4 \n",
    "kmeans.fit(data_normalized)\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "print('kmeans:', labels)\n",
    "\n",
    "# dbscan = DBSCAN(eps=0.1, min_samples=2, metric='euclidean', )\n",
    "# dbscan.fit(data_normalized)\n",
    "# labels = dbscan.labels_\n",
    "# print('labels', labels)\n",
    "# ### map negative labels to positive labels\n",
    "# unique_labels = np.unique(labels)\n",
    "# # print('unique_labels:', unique_labels)\n",
    "# map_labels = dict()\n",
    "# for i, l in enumerate(unique_labels):\n",
    "#     map_labels[f'{l}'] = i\n",
    "\n",
    "# new_labels = [map_labels[f'{l}'] for l in labels]\n",
    "# # print('new_labels:', new_labels)\n",
    "# labels = new_labels\n",
    "# print('dbscan:', labels)\n",
    "\n",
    "# print('ground_truth', test_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score, f1_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# # Sample data: Replace with your actual predictions\n",
    "# kmeans_labels = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1])  # Replace with K-Means predictions\n",
    "# ground_truth = np.array([2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1])  # Given ground truth\n",
    "\n",
    "ground_truth = np.array(test_class)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Map cluster labels to ground truth labels using the Hungarian algorithm\n",
    "def best_cluster_mapping(y_true, y_pred):\n",
    "    \"\"\"Finds the best mapping between predicted and true labels using the Hungarian algorithm.\"\"\"\n",
    "    unique_classes = np.unique(y_true)\n",
    "    unique_clusters = np.unique(y_pred)\n",
    "    cost_matrix = np.zeros((len(unique_classes), len(unique_clusters)))\n",
    "\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        for j, cluster in enumerate(unique_clusters):\n",
    "            cost_matrix[i, j] = -np.sum((y_true == cls) & (y_pred == cluster))  # Negative for maximization\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    mapping = {unique_clusters[col]: unique_classes[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    return np.vectorize(mapping.get)(y_pred)  # Map predictions\n",
    "\n",
    "print('ground_truth:', ground_truth)\n",
    "print('labels:', labels)\n",
    "# Remap cluster labels to best-matching class labels\n",
    "remapped_labels = best_cluster_mapping(ground_truth, labels)\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(ground_truth, remapped_labels)\n",
    "ari = adjusted_rand_score(ground_truth, remapped_labels)\n",
    "# nmi = normalized_mutual_info_score(ground_truth, remapped_labels)\n",
    "f1 = f1_score(ground_truth, remapped_labels, average='weighted')\n",
    "conf_matrix = confusion_matrix(ground_truth, remapped_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "# print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l,g in zip(remapped_labels, ground_truth):\n",
    "    print(l,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing with PCA (if needed)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_normalized)\n",
    "\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis', edgecolors='k')\n",
    "# plt.scatter(data_pca[:, 0], data_pca[:, 1], c=ground_truth, cmap='viridis', edgecolors='r', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering Visualization')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
