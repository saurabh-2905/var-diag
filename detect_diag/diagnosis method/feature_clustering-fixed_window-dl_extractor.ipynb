{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Fixed Window - DL Extractor\n",
    "- Use Deep Learning based extractors to get abstract features by giving the entire detection subseq as input\n",
    "- Use these features to cluster the detections with similar anomalies\n",
    "- The feature extraction is NOT dependent on the corresponding normal behaviour subtrace\n",
    "- This will serve as benchmark peroformance with off the shelf models, without any optimization \n",
    "- We tested this approach across all applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import TSFEDL.models_keras as tsfedl\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'mamba2'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True: varibale window size, False: fixed window size; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, 'diag_refsamples')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    train_data_path = ref_var_samples_path\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "print('train_data:\\n', train_data_path)\n",
    "print(len(train_data_path))\n",
    "print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction with TSFE-DL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "####################################### Select Extractor ########################################\n",
    "#################################################################################################\n",
    "\n",
    "# EXTRACTOR = 'forecaster'\n",
    "EXTRACTOR = 'autoencoder'\n",
    "\n",
    "\n",
    "print('Selected Extractor:', EXTRACTOR)\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "\n",
    "if EXTRACTOR == 'forecaster':    \n",
    "    ### load the model\n",
    "    model = tf.keras.models.load_model('./trained_models/forecaster_events_minmax_mamba+theft.keras')\n",
    "    # model.summary()\n",
    "\n",
    "    new_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-5].output, name='forecaster')\n",
    "    new_model.summary()\n",
    "elif EXTRACTOR == 'autoencoder':\n",
    "    model = tf.keras.models.load_model('./trained_models/autoencoder_events_minmax_mamba+theft.keras')\n",
    "    # model.summary()\n",
    "    ### get output of the encoder as features\n",
    "    x = model.layers[-8].output\n",
    "    output = tf.keras.layers.Flatten()(x)\n",
    "    new_model = tf.keras.Model(inputs=model.input, outputs=output, name='autoencoder')\n",
    "    new_model.summary()\n",
    "\n",
    "\n",
    "test_feature_vectors = []\n",
    "test_files = []\n",
    "for test_data in test_data_path[0:]:\n",
    "    print('test_data:', test_data)\n",
    "    ### read the subseq\n",
    "    test_trace = read_traces(test_data)\n",
    "    # print('test_trace:', test_trace)\n",
    "    test_data_len = len(test_trace)\n",
    "    print('test_data_len:', test_data_len)\n",
    "\n",
    "    if test_data_len > 500:\n",
    "        # print('test data length is more than 500, skipping...')\n",
    "        # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "        # continue\n",
    "\n",
    "        print('test data length is more than 500, truncating...')\n",
    "        test_trace = test_trace[:500]\n",
    "        test_data_len = 500\n",
    "    else:\n",
    "        ### pad the test data\n",
    "        test_trace = test_trace + [(0,0)]*(500-test_data_len)\n",
    "        test_data_len = 500\n",
    "    \n",
    "    # df = pd.DataFrame(test_trace, columns=['event', 'ts'],)\n",
    "    # ### seperate event and ts\n",
    "    # test_events = df['event']\n",
    "    # test_ts = df['ts']\n",
    "\n",
    "    ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "    test_events = []\n",
    "    test_intervals = []\n",
    "    prev_time = test_trace[0][1]\n",
    "    time_diff = 0\n",
    "    for x in test_trace:\n",
    "        time_diff = x[1] - prev_time\n",
    "        test_intervals.append(time_diff)\n",
    "        prev_time = x[1]\n",
    "        test_events.append(x[0])\n",
    "\n",
    "    # print('len of test_trace:', len(test_trace))\n",
    "    assert len(test_events) == len(test_intervals) == test_data_len\n",
    "\n",
    "\n",
    "    # print(df)\n",
    "    # print(test_events)\n",
    "\n",
    "    \n",
    "    print(test_events)\n",
    "    scaler_events = joblib.load(f\"./scalers/minmaxscaler_{CODE}.gz\")\n",
    "    test_events = scaler_events.transform(np.array(test_events).reshape(-1, 50))\n",
    "    print(np.array(test_events))\n",
    "    # print(np.array(test_events).reshape(-1, 1).shape)\n",
    "\n",
    "\n",
    "    ########################################################################################################\n",
    "    ############################################ forecaster ###############################################\n",
    "    ########################################################################################################\n",
    "    if EXTRACTOR == 'forecaster':    \n",
    "        # ### extract features\n",
    "        # feat_single = []\n",
    "        # for i in range(0, test_data_len, 50):\n",
    "        #     sub_events = test_events[i:i+50]\n",
    "        #     # print('sub_events:', sub_events)\n",
    "        #     # print('len of sub_events:', len(sub_events))\n",
    "        #     sub_events = np.array(sub_events)\n",
    "        #     sub_events = sub_events.reshape(1, sub_events.shape[0], 1)\n",
    "        #     # print('sub_events shape:', sub_events.shape)\n",
    "\n",
    "        #     sub_features = new_model.predict(sub_events)\n",
    "        #     sub_features = sub_features.flatten()\n",
    "        #     # print('sub_features shape:', sub_features.shape)\n",
    "        #     # print('sub_features:', sub_features)\n",
    "\n",
    "        #     feat_single.extend(sub_features)\n",
    "\n",
    "        # print('test_events shape:', test_events.shape)\n",
    "        _test_events = np.array(test_events).reshape(-1, 50, 1)\n",
    "        # print('_test_events shape:', _test_events.shape)\n",
    "\n",
    "        feat_single = new_model.predict(_test_events)\n",
    "        # print('feat_single shape:', feat_single.shape)\n",
    "        feat_single = feat_single.flatten()\n",
    "        # print('feat_single shape:', feat_single.shape)\n",
    "    elif EXTRACTOR == 'autoencoder':\n",
    "        _test_events = np.array(test_events).reshape(-1, 50, 1)\n",
    "        feat_single = new_model.predict(_test_events)\n",
    "        # print('feat_single shape:', feat_single.shape)\n",
    "        feat_single = feat_single.flatten()\n",
    "        # print('feat_single shape:', feat_single.shape)\n",
    "\n",
    "    ########################################################################################################\n",
    "    ############################################# forecaster ##############################################\n",
    "    ########################################################################################################\n",
    "\n",
    "    test_feature_vectors.append(feat_single)\n",
    "    test_files.append(test_data)\n",
    "\n",
    "    # break\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "print(len(test_data_path))\n",
    "print(np.array(test_feature_vectors).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = []\n",
    "test_class = []\n",
    "\n",
    "    \n",
    "### load the labels\n",
    "test_class_labels = read_json(test_labels_path[0])\n",
    "# print('test_class_labels:', len(test_class_labels))\n",
    "# print('test_class_labels:', test_class_labels)\n",
    "\n",
    "### prepare the feature vectors for classification\n",
    "i = 0\n",
    "for (test_data, feature_vector) in zip(test_files, test_feature_vectors):\n",
    "    file_name = test_data.split('/')[-1].split('.')[0]\n",
    "    # print('file_name:', file_name)\n",
    "    class_list = test_class_labels[file_name]\n",
    "    # print('class_list:', class_list)\n",
    "    # print('feature_vector:', feature_vector.shape)\n",
    "    class_label = None\n",
    "    # break\n",
    "\n",
    "    # print('test_data:', test_data)\n",
    "    # print('feature_vector:', np.array(feature_vector).shape)\n",
    "    # print('test_class_label:', test_class_labels[file_name])\n",
    "\n",
    "    if len(class_list) == 1:\n",
    "        ### Hardcode the class label for all applications\n",
    "        class_label = class_list\n",
    "\n",
    "    else:\n",
    "        # print('multiple class labels found for the test data:', test_data)\n",
    "        class_label = class_list\n",
    "\n",
    "\n",
    "        \n",
    "    if class_label != None:\n",
    "        feature_vector = np.array(feature_vector)\n",
    "        # print('feature_vector:', feature_vector.shape) \n",
    "        # print('before:', feature_vector) \n",
    "\n",
    "        # np.where(np.isnan(feature_vector))\n",
    "        feature_vector = np.nan_to_num(feature_vector)\n",
    "        # print('after:', feature_vector)\n",
    "\n",
    "        padded_features.append(feature_vector)\n",
    "        # test_files.append(test_data)\n",
    "        test_class.append(class_label)\n",
    "        # print('feature_vector:', feature_vector.shape[0])\n",
    "\n",
    "        # pad_num = 500 - feature_vector.shape[0]\n",
    "        # print('pad_num:', pad_num)\n",
    "        # padded_features = np.pad(feature_vector, (0,pad_num), 'constant', constant_values=(0))\n",
    "        # print('padded_features:', padded_features.shape)\n",
    "\n",
    "    # i += 1\n",
    "    # if i==6:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(padded_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "unique_classes = []\n",
    "map_labels = dict()\n",
    "ground_truth = []\n",
    "label_key = 0\n",
    "for tc in test_class:\n",
    "    # print(tc)\n",
    "    if tc in unique_classes:\n",
    "        ground_truth.append(map_labels[(str(tc))])\n",
    "        continue\n",
    "    else:\n",
    "        print('tc:', tc)\n",
    "        unique_classes.append(tc)\n",
    "        map_labels[str(tc)] = label_key\n",
    "        ground_truth.append(map_labels[(str(tc))])\n",
    "        label_key += 1\n",
    "\n",
    "    \n",
    "N_CLUSTER = len(unique_classes)\n",
    "print('N_CLUSTER:', N_CLUSTER)\n",
    "\n",
    "data = np.array(padded_features)\n",
    "\n",
    "# Flatten the feature vectors (reshape to (48, 1000))\n",
    "if len(np.array(padded_features).shape) == 3:\n",
    "    # Reshape the data for clustering\n",
    "    num_samples, num_features, num_points = data.shape\n",
    "    data_reshaped = data.reshape(num_samples, num_features * num_points)\n",
    "else:\n",
    "    data_reshaped = data\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_reshaped)\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(init=\"k-means++\", max_iter=300, n_clusters=N_CLUSTER, n_init=50 )   # n_clusters=N_CLUSTER, random_state=0, n_init=4 \n",
    "kmeans.fit(data_normalized)\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "print('kmeans:', labels)\n",
    "print('gt', ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score, f1_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# # Sample data: Replace with your actual predictions\n",
    "# kmeans_labels = np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1])  # Replace with K-Means predictions\n",
    "# ground_truth = np.array([2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1])  # Given ground truth\n",
    "\n",
    "ground_truth = np.array(ground_truth)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Map cluster labels to ground truth labels using the Hungarian algorithm\n",
    "def best_cluster_mapping(y_true, y_pred):\n",
    "    \"\"\"Finds the best mapping between predicted and true labels using the Hungarian algorithm.\"\"\"\n",
    "    unique_classes = np.unique(y_true)\n",
    "    unique_clusters = np.unique(y_pred)\n",
    "    cost_matrix = np.zeros((len(unique_classes), len(unique_clusters)))\n",
    "\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        for j, cluster in enumerate(unique_clusters):\n",
    "            cost_matrix[i, j] = -np.sum((y_true == cls) & (y_pred == cluster))  # Negative for maximization\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    mapping = {unique_clusters[col]: unique_classes[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    return np.vectorize(mapping.get)(y_pred)  # Map predictions\n",
    "\n",
    "print('ground_truth:', ground_truth)\n",
    "print('labels:', labels)\n",
    "# Remap cluster labels to best-matching class labels\n",
    "remapped_labels = best_cluster_mapping(ground_truth, labels)\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(ground_truth, remapped_labels)\n",
    "f1 = f1_score(ground_truth, remapped_labels, average='weighted')\n",
    "conf_matrix = confusion_matrix(ground_truth, remapped_labels)\n",
    "\n",
    "ari = adjusted_rand_score(ground_truth, labels)\n",
    "nmi = normalized_mutual_info_score(ground_truth, labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction with SegLearn\n",
    "# for test_data in test_data_path[0:]:\n",
    "#     print('test_data:', test_data)\n",
    "#     ### read the subseq\n",
    "#     test_trace = read_traces(test_data)\n",
    "#     # print('test_trace:', test_trace)\n",
    "#     test_data_len = len(test_trace)\n",
    "#     print('test_data_len:', test_data_len)\n",
    "\n",
    "#     test_trace = np.array(test_trace).reshape(1, -1, 2)\n",
    "#     print('test_trace:', test_trace.shape)\n",
    "\n",
    "#     # features = FeatureTransform.fit_transform(test_trace)\n",
    "#     feature_names = all_features().keys()\n",
    "#     feature_functions = all_features()\n",
    "#     for i, feat_label in enumerate(feature_names):\n",
    "#         print(feat_label)\n",
    "#         # print(feature_functions[feat_label])\n",
    "#         func = feature_functions[feat_label]\n",
    "#         feat = func(test_trace)\n",
    "#         print(feat)\n",
    "#         print(feat.shape)\n",
    "#         print('')\n",
    "    \n",
    "    # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
