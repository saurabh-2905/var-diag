{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- corrected_labels_created - 17/05/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 - method 2\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "WINDOW = 500                 ### window size for subsequence\n",
    "SUBSEQ =  'diag_subseq'        # 'diag_subseq'  , diag_subseq_multi      ### subsequence type, diag_subseq, subseq\n",
    "OVERHEAD_LEN = 0          ### if 0 then ref_samples with complete train data considered\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "if OVERHEAD_LEN == 0:\n",
    "    ref_samples_basepath = os.path.join(normalbase_path, f'diag_refsamples{WINDOW}')\n",
    "else:\n",
    "    ref_samples_basepath =  os.path.join(normalbase_path, f'diag_refsamples{WINDOW}_len-test_{OVERHEAD_LEN}')\n",
    "\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/subseq')\n",
    "diag_el_basepath = os.path.join(faultybase_path, f'{SUBSEQ}/el')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "test_labels_basepath = os.path.join(faultybase_path, 'labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "# ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_feature_path = [os.path.join(diag_el_basepath, x) for x in os.listdir(diag_el_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "eval_labels_path = [os.path.join(test_labels_basepath, x) for x in os.listdir(test_labels_basepath)]\n",
    "\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "# ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_feature_path = [x for x in test_feature_path if '.DS_Store' not in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "eval_labels_path = [x for x in eval_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    # train_data_path = ref_var_samples_path\n",
    "    raise ValueError('Ref samples for variable window missing')\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "# print('train_data:', train_data_path)\n",
    "print('No. of ref samples', len(train_data_path))\n",
    "# print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "print('eval_labels:\\n', eval_labels_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### End to end overhead ####\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# process = psutil.Process(os.getpid())\n",
    "# start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "# start_time = time.perf_counter() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Seperate if multiple instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_correct_part(ref_samples, test_events, test_intervals):\n",
    "    '''\n",
    "    check if any matching event trace in present based on first 2 points\n",
    "    if yes, then check the number of matching events and intervals, remove the matching part in the event trace and return the remaining part\n",
    "    if no, then return the same event trace\n",
    "    '''\n",
    "\n",
    "    test_data_len = len(test_events)\n",
    "    # print('test_events:', test_events)\n",
    "    # print('test_data_len:', test_data_len)\n",
    "    # print('ref_samples', ref_samples)\n",
    "    ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "    shortlisted_ref_events = []\n",
    "    shortlisted_ref_intervals = []\n",
    "    zero_count = []\n",
    "    for ref_sample in ref_samples:\n",
    "        # print('ref_sample:', ref_sample[0][:5])\n",
    "        # event_diff = np.array(ref_sample[0][0:len(test_events)]) - np.array(test_events)\n",
    "        # print('event_diff:', event_diff)\n",
    "        # print('event_diff:', len(event_diff))\n",
    "        # print('zeros:', np.where(event_diff==0)[0].shape)\n",
    "        # if len(test_events) == 276:\n",
    "        #     print('ref_sample:', ref_sample[0][:5])\n",
    "        #     print('test_events:', test_events[:5])\n",
    "        if ref_sample[0][:2] == test_events[:2]:\n",
    "            ref_events = ref_sample[0][:test_data_len]\n",
    "            ref_intervals = ref_sample[1][:test_data_len]\n",
    "            # print('ref_events', ref_events)\n",
    "            if len(test_events) > 500:\n",
    "                diff_events = np.array(ref_events) - np.array(test_events[:500])\n",
    "                diff_intervals = np.abs(np.array(ref_intervals) - np.array(test_intervals[:500]))\n",
    "            else:\n",
    "                diff_events = np.array(ref_events) - np.array(test_events)\n",
    "                diff_intervals = np.abs(np.array(ref_intervals) - np.array(test_intervals))\n",
    "    \n",
    "            # if len(test_events) == 276:\n",
    "            #     print('diff_events:', diff_events)\n",
    "            #     print('diff_intervals:', diff_intervals)\n",
    "            # print('diff events', diff_events)\n",
    "            count = 0\n",
    "            # print(sf[0], sf[1])\n",
    "            for esf, esi in zip(diff_events, diff_intervals):\n",
    "                ### check if events and intervals are same\n",
    "                # if esf == 0 and esi < 5:\n",
    "                if esf == 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break   ### part of the logic, do not remove\n",
    "\n",
    "            # print('zero_count:', count)\n",
    "            ### depulicate the ref samples\n",
    "            if ref_events not in shortlisted_ref_events:\n",
    "                zero_count.append(count)\n",
    "                shortlisted_ref_events.append(ref_events)\n",
    "                shortlisted_ref_intervals.append(ref_intervals)\n",
    "            # print('count:', count)  \n",
    "\n",
    "        # break\n",
    "\n",
    "    # print('zero_count:', zero_count)\n",
    "    # print('shortlisted_ref_samples:', len(shortlisted_ref_events))\n",
    "\n",
    "    ### select the ref_sample_events with maximum leading zeros\n",
    "    if len(zero_count) != 0:\n",
    "        max_zero_count = max(zero_count)\n",
    "        zero_count = np.array(zero_count)\n",
    "        max_zero_count_ind = np.where(zero_count==max_zero_count)[0][0]\n",
    "        # print('max_zero_count_ind:', max_zero_count_ind)\n",
    "        selected_ref_events = shortlisted_ref_events[max_zero_count_ind]\n",
    "        selected_ref_intervals = shortlisted_ref_intervals[max_zero_count_ind]\n",
    "        # print('selected_ref_events:', selected_ref_events[:max_zero_count+1])\n",
    "    else:\n",
    "        max_zero_count = 0\n",
    "        selected_ref_events = None\n",
    "        selected_ref_intervals = None\n",
    "\n",
    "    if max_zero_count == 0:\n",
    "        print('func: No match found')\n",
    "        return None, selected_ref_events, selected_ref_intervals\n",
    "    else:\n",
    "        ### select the point where the last match happened\n",
    "        # last_matched_point = max_zero_count-1\n",
    "        # print('last_matched_point:', last_matched_point)\n",
    "        # print('test_events:', test_events[:last_matched_point])\n",
    "        # striped_test_events = test_events[last_matched_point:]\n",
    "        # striped_test_intervals = test_intervals[last_matched_point:]\n",
    "        # striped_test_timestamps = test_timestamps[last_matched_point:]\n",
    "        # print('striped_test_events:', striped_test_events)\n",
    "        # print('max count:', max_zero_count, len(test_events), len(striped_test_events))\n",
    "\n",
    "        \n",
    "        if max_zero_count == len(test_events):\n",
    "            print('func: All events are same')\n",
    "            return max_zero_count, selected_ref_events, selected_ref_intervals\n",
    "        else:\n",
    "            return max_zero_count, selected_ref_events, selected_ref_intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%reload_ext memory_profiler\n",
    "\n",
    "######## computation overhead ######\n",
    "start_time = time.perf_counter() \n",
    "####################################\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "    \n",
    "### load all the reference samples (fixed window size)\n",
    "ref_samples = []\n",
    "for ref_sample in train_data_path:\n",
    "    ref_samples.append(read_traces(ref_sample))\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "### load the test samples and compare with the reference samples\n",
    "anomaly_instances = []\n",
    "anomaly_timestamps = []\n",
    "anomaly_features = []\n",
    "test_files = []\n",
    "for test_data in test_data_path[0:]:\n",
    "    print('test_data:', test_data)\n",
    "    ### read the subseq\n",
    "    test_trace = read_traces(test_data)\n",
    "    # print('test_trace:', test_trace)\n",
    "    test_data_len = len(test_trace)\n",
    "    # print('test_data_len:', test_data_len)\n",
    "\n",
    "    # if test_data_len > 500:\n",
    "    #     # print('test data length is more than 500, skipping...')\n",
    "    #     # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "    #     # continue\n",
    "\n",
    "    #     print('test data length is more than 500, truncating...')\n",
    "    #     test_trace = test_trace[:500]\n",
    "    #     test_data_len = 500\n",
    "    \n",
    "    ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "    ### old implementation with 0 at start of intervals\n",
    "    # test_events = []\n",
    "    # test_intervals = []\n",
    "    # test_timestamps = []\n",
    "    # prev_time = test_trace[0][1]\n",
    "    # time_diff = 0\n",
    "    # for x in test_trace:\n",
    "    #     time_diff = x[1] - prev_time\n",
    "    #     test_intervals.append(time_diff)\n",
    "    #     prev_time = x[1]\n",
    "    #     test_events.append(x[0])\n",
    "    #     test_timestamps.append(x[1])\n",
    "\n",
    "    ### new implementation without 0 at start of intervals\n",
    "    test_events = []\n",
    "    test_intervals = []\n",
    "    test_timestamps = []\n",
    "    for x,y in zip(test_trace[:-1], test_trace[1:]):\n",
    "        test_events.append(x[0])   ### first event\n",
    "        test_intervals.append(y[1] - x[1])   ### difference between the timestamps of second and first event\n",
    "        test_timestamps.append(x[1])   ### first timestamp\n",
    "    print('test_events:', len(test_events))\n",
    "    print('test_data_len', test_data_len)\n",
    "    # print('test_intervals:', test_intervals)\n",
    "\n",
    "    assert len(test_events)+1 == len(test_intervals)+1 == test_data_len\n",
    "\n",
    "    # ### get detection class\n",
    "\n",
    "    # test_class_labels = read_json(test_labels_path[0])\n",
    "\n",
    "    ### store the first to consecutive points as the first anomaly instance\n",
    "    an_instance = []\n",
    "    an_timestamps = []\n",
    "    all_instances = []\n",
    "    all_timestamps = []\n",
    "    all_ref_events = []\n",
    "    all_ref_intervals = []\n",
    "    all_striped_test_events = []\n",
    "    all_striped_test_intervals = []\n",
    "    all_striped_timestamps = []\n",
    "    an_affected_events = []    ### store the additional/missing events compared to ref_samples\n",
    "    all_affected_events = []\n",
    "    striped_ref_events = []\n",
    "\n",
    "    striped_test_events = test_events\n",
    "    striped_test_intervals = test_intervals\n",
    "    striped_timestamps = test_timestamps\n",
    "    # print('striped_test_events:', len(striped_test_events))\n",
    "    # print('striped_test_intervals:', striped_test_intervals, len(striped_test_intervals))\n",
    "    has_checked = False\n",
    "    max_zero_old_count = 0\n",
    "    i = 0\n",
    "    while len(striped_test_events) > 0:\n",
    "    # while i < 6:\n",
    "        ### remove the initial correct part of the trace\n",
    "        # print('ref_samples:', ref_samples, striped_test_events, striped_test_intervals)\n",
    "        print('give input:', len(striped_test_events),)\n",
    "        max_zero_count, selected_ref_events, selected_ref_intervals = strip_correct_part(ref_samples, striped_test_events, striped_test_intervals)\n",
    "        # print('max_zero_count:', max_zero_count, 'len:', len(striped_test_events))\n",
    "        \n",
    "        if selected_ref_events != None:\n",
    "            if len(selected_ref_events) == len(striped_test_events):\n",
    "                ### first collect the trace that is given as input\n",
    "                all_striped_test_events.append(striped_test_events)\n",
    "                all_striped_test_intervals.append(striped_test_intervals)\n",
    "                all_striped_timestamps.append(striped_timestamps)\n",
    "                ### get the ref trace that matched with the test trace\n",
    "                all_ref_events.append(selected_ref_events)\n",
    "                all_ref_intervals.append(selected_ref_intervals)\n",
    "            else: \n",
    "                ### if the len ar not same, then test_events are longer than 500 (max length of ref sample)\n",
    "                print('1', len(striped_test_events), len(selected_ref_events), len(striped_ref_events))\n",
    "                all_striped_test_events.append(striped_test_events[:500])\n",
    "                all_striped_test_intervals.append(striped_test_intervals[:500])\n",
    "                all_striped_timestamps.append(striped_timestamps[:500])\n",
    "                all_ref_events.append(selected_ref_events)\n",
    "                all_ref_intervals.append(selected_ref_intervals)\n",
    "                print('1.1', len(striped_test_events[:500]), len(selected_ref_events[:500]), len(striped_ref_events[:500]))\n",
    "        else:\n",
    "            print('No match found')\n",
    "            print('2', len(striped_test_events), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "            if len(striped_ref_events) == len(striped_test_events):\n",
    "                all_striped_test_events.append(striped_test_events)\n",
    "                all_striped_test_intervals.append(striped_test_intervals)\n",
    "                all_striped_timestamps.append(striped_timestamps)\n",
    "                all_ref_events.append(striped_ref_events)\n",
    "            else:\n",
    "                if len(striped_test_events) > 500:\n",
    "                    all_striped_test_events.append(striped_test_events[:len(striped_ref_events)])\n",
    "                    all_striped_test_intervals.append(striped_test_intervals[:len(striped_ref_events)])\n",
    "                    all_striped_timestamps.append(striped_timestamps[:len(striped_ref_events)])\n",
    "                    all_ref_events.append(striped_ref_events)\n",
    "                    print('2.1', len(striped_test_events[:len(striped_ref_events)]), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "                # else:\n",
    "                    # raise ValueError('Mismatch in lengths of selected_ref_events and striped_test_events')\n",
    "\n",
    "        # print('selected_ref_events:', selected_ref_events)\n",
    "\n",
    "        if max_zero_count == len(striped_test_events):\n",
    "            print('All events are same')\n",
    "            striped_test_events = []\n",
    "            striped_test_intervals = []\n",
    "            striped_timestamps = []\n",
    "        # elif max_zero_count != None:\n",
    "        #     striped_test_events = striped_test_events[max_zero_count-1:]\n",
    "        #     striped_test_intervals = striped_test_intervals[max_zero_count-1:]\n",
    "        #     striped_timestamps = striped_timestamps[max_zero_count-1:]\n",
    "        # print('3', len(striped_test_events), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "\n",
    "        # break\n",
    "        ### store the first anomaly instance (first two consecutive points)\n",
    "        if max_zero_count != None:\n",
    "            # print('debug', max_zero_count, len(striped_test_events))\n",
    "            if len(striped_test_events) != 0:\n",
    "                if has_checked:\n",
    "                    if len(an_instance) != 0:\n",
    "                        ### new anomaly instance detected\n",
    "                        all_instances.append(an_instance)\n",
    "                        all_timestamps.append(an_timestamps)\n",
    "                        all_affected_events.append(an_affected_events)\n",
    "                        an_instance = []\n",
    "                        an_timestamps = []\n",
    "                        an_affected_events = []\n",
    "\n",
    "                    ### start, where first anomaly instance is detected\n",
    "                    print('Checked, new instance created', 'old, new', max_zero_old_count, max_zero_count)\n",
    "                    seq_start = np.clip(max_zero_count-2, 0, None)\n",
    "                    seq_end = np.clip(seq_start+5, 0, len(striped_test_events))\n",
    "                    # print('seq_start:', seq_start, 'seq_end:', seq_end)\n",
    "                    an_instance.extend(striped_test_events[seq_start:seq_end])\n",
    "                    an_timestamps.extend(striped_timestamps[seq_start:seq_end])\n",
    "                    # print('selected_ref_events:', selected_ref_events)\n",
    "                    an_affected_events.extend(selected_ref_events[seq_start:seq_end])\n",
    "                    striped_test_events = striped_test_events[seq_end:]\n",
    "                    striped_test_intervals = striped_test_intervals[seq_end:]\n",
    "                    striped_timestamps = striped_timestamps[seq_end:]\n",
    "                    striped_ref_events = selected_ref_events[seq_end:]\n",
    "                    # print('an_instance:', an_instance)\n",
    "                    print('striped_test_events 3.1:', len(striped_test_events), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "                    has_checked = False\n",
    "                else:\n",
    "                    if len(an_instance) != 0:\n",
    "                        ### new anomaly instance detected\n",
    "                        all_instances.append(an_instance)\n",
    "                        all_timestamps.append(an_timestamps)\n",
    "                        all_affected_events.append(an_affected_events)\n",
    "                        an_instance = []\n",
    "                        an_timestamps = []\n",
    "                        an_affected_events = []\n",
    "\n",
    "                    ### recheck from last 5 matching points to see in really an anomaly\n",
    "                    print('Detected discripancy, rechecking...')\n",
    "                    max_zero_old_count = max_zero_count\n",
    "                    step_back = np.clip(max_zero_count-30, 0, None)\n",
    "                    print('steping back', step_back)\n",
    "                    striped_test_events = striped_test_events[step_back:]\n",
    "                    striped_test_intervals = striped_test_intervals[step_back:]\n",
    "                    striped_timestamps = striped_timestamps[step_back:]\n",
    "                    striped_ref_events = selected_ref_events[step_back:]\n",
    "                    print('striped_test_events 2:', len(striped_test_events), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "                    has_checked = True\n",
    "                    continue ### continue to the next iteration\n",
    "\n",
    "            else:\n",
    "                if len(an_instance) != 0:\n",
    "                    all_instances.append(an_instance)\n",
    "                    all_timestamps.append(an_timestamps)\n",
    "                    all_affected_events.append(an_affected_events)\n",
    "                    an_instance = []\n",
    "                    an_timestamps = []\n",
    "                    an_affected_events = []\n",
    "                anomaly_instances.append(all_instances)\n",
    "                anomaly_timestamps.append(all_timestamps)\n",
    "                anomaly_features.append(all_affected_events)\n",
    "                test_files.append(test_data)\n",
    "                print('DONE 1')\n",
    "                break ### part of the logic, do not remove\n",
    "        else:\n",
    "            if len(striped_test_events) > 1:\n",
    "                ### normal functionality loop\n",
    "                print('extending previous instance')\n",
    "                an_instance.extend(striped_test_events[:1])\n",
    "                an_timestamps.extend(striped_timestamps[:1])\n",
    "                an_affected_events.extend(striped_ref_events[:1])\n",
    "                striped_test_events = striped_test_events[1:]\n",
    "                striped_test_intervals = striped_test_intervals[1:]\n",
    "                striped_timestamps = striped_timestamps[1:]\n",
    "                striped_ref_events = striped_ref_events[1:]\n",
    "                print('striped_test_events 3.2:', len(striped_test_events), (len(selected_ref_events) if selected_ref_events != None else None), len(striped_ref_events))\n",
    "            else:\n",
    "                ### for last iteration, when len(striped_test_events) == 1   \n",
    "                if len(an_instance) != 0:\n",
    "                    all_instances.append(an_instance)\n",
    "                    all_timestamps.append(an_timestamps)\n",
    "                    all_affected_events.append(an_affected_events)\n",
    "                    an_instance = []\n",
    "                    an_timestamps = []\n",
    "                    an_affected_events = []\n",
    "                anomaly_instances.append(all_instances)\n",
    "                anomaly_timestamps.append(all_timestamps)\n",
    "                anomaly_features.append(all_affected_events)\n",
    "                test_files.append(test_data)\n",
    "                print('DONE 2')\n",
    "                break  #### part of the logic, do not remove\n",
    "        print('an_instance:', an_instance)\n",
    "        print('all_instances:', all_instances)\n",
    "        print('an_affected_events:', an_affected_events)\n",
    "        print('all_affected_events:', all_affected_events)\n",
    "        if len(striped_test_events) == 0:\n",
    "            all_instances.append(an_instance)\n",
    "            all_timestamps.append(an_timestamps)\n",
    "            all_affected_events.append(an_affected_events)\n",
    "            an_instance = []\n",
    "            an_timestamps = []\n",
    "            an_affected_events = []\n",
    "            anomaly_instances.append(all_instances)\n",
    "            anomaly_timestamps.append(all_timestamps)\n",
    "            anomaly_features.append(all_affected_events)\n",
    "            test_files.append(test_data)\n",
    "            print('DONE 3')\n",
    "        print('')    \n",
    "        i += 1\n",
    "        # if i == 2:\n",
    "        #     print('break')\n",
    "        #     break\n",
    "    \n",
    "    print('all_instances:', all_instances)\n",
    "    print('all_affected_events:', all_affected_events)\n",
    "    print('-------\\n')\n",
    "    \n",
    "    # break\n",
    "# print('anomaly_instances:', anomaly_instances) \n",
    "# print('anomaly_timestamps:', anomaly_timestamps)\n",
    "# print('anomaly_features:', anomaly_features)\n",
    "\n",
    "\n",
    "######## computation overhead ######\n",
    "# print(f\"Memory used (Training):\", memory_used)\n",
    "# print(f\"Memory used (Training):\", round(np.mean(np.array(memory_used)), 2) )\n",
    "print('overhead_len:', OVERHEAD_LEN)\n",
    "end_time = time.perf_counter()\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "print(f\"\\nTime taken (Training): {elapsed_ms:.2f} ms\")\n",
    "\n",
    "%memit\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# FONTSIZE = 20\n",
    "# PLOTWIDTH = 1500\n",
    "\n",
    "# # # x_axis = np.arange(0, len(test_trace), 1)\n",
    "\n",
    "\n",
    "\n",
    "# # ### prepare test_trace for plotting\n",
    "# # plot_data = dict()\n",
    "# # plot_data['subseq'] = test_events   ### y_data (traces)\n",
    "\n",
    "# # # for i, fv in enumerate(shortlisted_ref_samples):\n",
    "# # #     plot_data[f'feat1_{i}'] = fv[0]\n",
    "# # plot_data['ref_samples'] = selected_ref_events\n",
    "    \n",
    "# # df_feat1 = pd.DataFrame(plot_data)\n",
    "\n",
    "# # plot_data = dict()\n",
    "# # plot_data['intervals'] = test_intervals   ### y_data (traces)\n",
    "\n",
    "# # # for i, fv in enumerate(feature_vectors):\n",
    "# # #     plot_data[f'feat2_{i}'] = fv[1]\n",
    "# # plot_data['ref_intervals'] = selected_ref_intervals\n",
    "\n",
    "# # df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "# # fig = px.line(df_feat1, title='features')\n",
    "# # fig.show()\n",
    "\n",
    "# # fig = px.line(df_feat2, title='features')\n",
    "# # fig.show()\n",
    "\n",
    "# for i in range(len(all_striped_test_events)):\n",
    "#     plot_data = dict()\n",
    "#     plot_data['subseq'] = all_striped_test_events[i]   ### y_data (traces)\n",
    "#     print('ref:', all_ref_events[i])\n",
    "#     print('det:', all_striped_test_events[i])\n",
    "#     print('len ref', len(all_ref_events[i]) if all_ref_events[i] != None else None)\n",
    "#     print('len det', len(all_striped_test_events[i]) if all_striped_test_events[i] != None else None)\n",
    "#     if all_ref_events[i] != None:\n",
    "#         plot_data['ref_samples'] = all_ref_events[i]\n",
    "            \n",
    "#         df_feat1 = pd.DataFrame(plot_data)\n",
    "#         # print('df_feat1:', df_feat1)\n",
    "        \n",
    "#         fig = go.Figure()\n",
    "#         for col in df_feat1.columns:\n",
    "#             fig.add_trace(go.Scatter(y=df_feat1[col], mode='lines+markers', name=col))\n",
    "\n",
    "#         # fig = px.line(df_feat1, title='features')\n",
    "\n",
    "\n",
    "#         #### formatting to match Dash board plots\n",
    "#         fig.update_layout(\n",
    "#             # title_text=\"Event Trace without Time\",\n",
    "#             xaxis=dict(\n",
    "#                 tickfont = dict(size = FONTSIZE),\n",
    "#                 # titlefont = dict(size = FONTSIZE),\n",
    "#                 color='black',\n",
    "#             ),\n",
    "#             yaxis=dict(\n",
    "#                 title=\"Events\",\n",
    "#                 tickfont = dict(size = FONTSIZE),\n",
    "#                 # titlefont = dict(size = FONTSIZE),\n",
    "#                 color='black',\n",
    "#             ),\n",
    "#             autosize=True,\n",
    "#             width=PLOTWIDTH,\n",
    "#             # height=PLOTHEIGHT,\n",
    "#             font_size=FONTSIZE,\n",
    "#             plot_bgcolor='rgba(0,0,0,0)',\n",
    "            \n",
    "#         )\n",
    "\n",
    "#         fig.update_xaxes(\n",
    "#             mirror=True,\n",
    "#             ticks='outside',\n",
    "#             showline=True,\n",
    "#             linecolor='black',\n",
    "#             gridcolor='lightgrey'\n",
    "#         )\n",
    "#         fig.update_yaxes(\n",
    "#             mirror=True,\n",
    "#             ticks='outside',\n",
    "#             showline=True,\n",
    "#             linecolor='black',\n",
    "#             gridcolor='lightgrey',\n",
    "#             autorange=True,\n",
    "#         )\n",
    "\n",
    "#         # style all the traces\n",
    "#         fig.update_traces(\n",
    "#             #hoverinfo=\"name+x+text\",\n",
    "#             line={\"width\": 0.5},\n",
    "#             marker={\"size\": 8},\n",
    "#             mode=\"lines+markers\",\n",
    "#             showlegend=True,\n",
    "            \n",
    "#         )\n",
    "#         #############\n",
    "        \n",
    "#         fig.show()\n",
    "\n",
    "#         # plot_data = dict()\n",
    "#         # print('check:', len(all_striped_test_intervals[i]), len(all_ref_intervals[i]))\n",
    "#         # plot_data['intervals'] = all_striped_test_intervals[i]   ### y_data (traces)\n",
    "#         # plot_data['ref_intervals'] = all_ref_intervals[i]\n",
    "\n",
    "#         # df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "#         # fig = px.line(df_feat2, title='features')\n",
    "#         # fig.show()\n",
    "\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for an in anomaly_instances:\n",
    "#     print('an:', an)\n",
    "#     print('len:', len(an))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_class_labels = read_json(test_labels_path[0])\n",
    "# # print(test_class_labels)\n",
    "\n",
    "# for l, i, k, j in zip(test_files, anomaly_instances, anomaly_timestamps, anomaly_features):\n",
    "#     # print(l)\n",
    "#     file_name = l.split('/')[-1].split('.')[0]\n",
    "#     print(file_name)\n",
    "\n",
    "#     class_labels = test_class_labels[file_name]\n",
    "\n",
    "#     print('length of labels and predictions:', len(class_labels), len(i))\n",
    "#     print('class:', class_labels)\n",
    "#     print('detection', i)\n",
    "#     print('missing/additional events:', j)\n",
    "#     print('timestamp', k)\n",
    "\n",
    "#     print('')\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_VAL = 5\n",
    "\n",
    "ei = exeInt()\n",
    "\n",
    "#### check how many instance are identified correctly\n",
    "\n",
    "### load labels for each trial file\n",
    "all_test_labels = dict()\n",
    "for label_path in eval_labels_path:\n",
    "    # print(label_path)\n",
    "    file_name = label_path.split('/')[-1].split('.')[0][:-7]\n",
    "    print('file_name 1:', file_name)\n",
    "    eval_labels = read_json(label_path)\n",
    "    key = list(eval_labels['labels'].keys())[0]\n",
    "    labels = eval_labels['labels'][key]\n",
    "    all_test_labels[file_name] = labels\n",
    "    # print('labels:', labels)\n",
    "    # print(eval_labels)\n",
    "    # print('')\n",
    "\n",
    "inst_len_all = []\n",
    "gt_in_inst_all = []\n",
    "inst_in_gt_all = defaultdict(list)\n",
    "\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_fn = []\n",
    "all_gt = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "all_features = []  ### collection of features (corresponding events for anomaly from reference samples)\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for file, instances, inst_time, an_feature in zip(test_files, anomaly_instances, anomaly_timestamps, anomaly_features):\n",
    "    print(file)\n",
    "    file_name = file.split('/')[-1].split('.')[0].split('_')\n",
    "    file_name = '_'.join(file_name[:-1])\n",
    "    print('file_name 2:', file_name)\n",
    "    det_file_name = file.split('/')[-1].split('.')[0]\n",
    "    print('det_file_name:', det_file_name)\n",
    "    subseq_ind = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    subseq_start = int(subseq_ind.split('-')[0])\n",
    "    subseq_end = int(subseq_ind.split('-')[1])\n",
    "    label_file = file.split('/')[:-3]\n",
    "    label_file = '/'.join(label_file) + '/labels/' + file_name + '_labels.json'\n",
    "    test_file = file.split('/')[:-3]\n",
    "    test_file = '/'.join(test_file) + '/' + file_name\n",
    "    print('test_file:', test_file)\n",
    "    print('label_file:', label_file)\n",
    "    # print('subseq_ind:', subseq_ind)\n",
    "    print('instances:', instances)\n",
    "    print('inst_time:', inst_time)\n",
    "    print('feature:', an_feature)\n",
    "    \n",
    "    ### check against labels if the instances are correct\n",
    "    labels = all_test_labels[file_name]\n",
    "    # print('instances:', instances)\n",
    "    # print('inst_time:', inst_time)\n",
    "    # print('labels:', labels)\n",
    "\n",
    "    detection = []\n",
    "    feature = []\n",
    "    for i, t, k in zip(instances, inst_time, an_feature):\n",
    "        print(i, t, k)\n",
    "        start_anomaly = t[0]\n",
    "        end_anomaly = t[-1]\n",
    "        start_event = i[0]\n",
    "        end_event = i[-1]\n",
    "\n",
    "        anomaly = [(start_event, end_event), (start_anomaly, end_anomaly), det_file_name]\n",
    "        detection.append(anomaly)\n",
    "        feature.append(k)\n",
    "\n",
    "        # print('anomaly and time:', i, t)\n",
    "\n",
    "        ######### calculate avg length of instances ##########\n",
    "        inst_len = len(i)\n",
    "        # print('inst_len:', inst_len)\n",
    "        inst_len_all.append(inst_len)\n",
    "\n",
    "        ######## calculate number of anomalies (GT) present in each detection (metric to evaluate how well is the seperation done, ideally it should be 1) #######\n",
    "        gt_in_inst = []\n",
    "        for l in labels:\n",
    "            gt_ind1, gt_ind2 = l[2], l[3]\n",
    "            # print('gt:', gt_ind1, gt_ind2)\n",
    "            # print('start_anomaly:', start_anomaly, 'end_anomaly:', end_anomaly)\n",
    "            if (gt_ind1 >= start_anomaly and gt_ind1 < end_anomaly) or (gt_ind2 > start_anomaly and gt_ind2 <= end_anomaly) or (gt_ind1 < start_anomaly < gt_ind2) or (gt_ind1 < end_anomaly < gt_ind2):\n",
    "                gt_in_inst += [l]\n",
    "        # print('gt_in_inst:', gt_in_inst)\n",
    "        num_gt_in_inst = len(gt_in_inst)\n",
    "        if num_gt_in_inst > 0:\n",
    "            gt_in_inst_all.append(num_gt_in_inst)\n",
    "            for gtii in gt_in_inst:\n",
    "                inst_key = str(gtii)\n",
    "                inst_in_gt_all[inst_key].append(anomaly)\n",
    "\n",
    "\n",
    "    gts = []\n",
    "    for l in labels:\n",
    "        # print('l:', l)\n",
    "        # print('subseq_start-5:', subseq_start, 'subseq_end:', subseq_end)\n",
    "        if subseq_start < l[0] < subseq_end or subseq_start < l[1] < subseq_end:\n",
    "        # if (subseq_start-5 < l[0] < subseq_end-5 and subseq_start+5 < l[1] < subseq_end+5):\n",
    "            gts.append(l)\n",
    "        # break ## debugging\n",
    "\n",
    "    print('detection:', detection)\n",
    "    print('gts:', gts)\n",
    "    correct_pred, rest_pred, y_pred, y_true, false_neg = ei.get_correct_detections(detection, gts)\n",
    "    print(correct_pred, rest_pred, false_neg)\n",
    "    print('y_pred, y_true:', y_pred, y_true,)\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "\n",
    "    all_detections += [(test_file, detection, label_file)]  ### used to plot detections\n",
    "    all_features += [feature]\n",
    "    all_tp += [(test_file, correct_pred, label_file)]\n",
    "    all_fp += [(test_file, rest_pred, label_file)]\n",
    "    all_fn += [(test_file, false_neg, label_file)]\n",
    "    all_gt += [(test_file, gts, label_file)]\n",
    "\n",
    "    # print('all_detections:', all_detections)\n",
    "    # print('all_features:', all_features)\n",
    "\n",
    "    print('')\n",
    "    # break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subseq_start-5 < l[1] < subseq_end-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If multiple detections peresent for single GT, calculate averege number of detections per GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### calculate number of instances in each ground truth label\n",
    "inst_key = inst_in_gt_all.keys()\n",
    "# print('inst_key:', inst_key)\n",
    "num_anomaly_per_gt_all = []\n",
    "for ik in inst_key:\n",
    "    print('key:', ik)\n",
    "    anomaly_per_gt = inst_in_gt_all[ik]\n",
    "    # print('anomaly_per_gt:', anomaly_per_gt)\n",
    "    num_anomaly_per_gt = len(anomaly_per_gt)\n",
    "    # print('num_anomaly_per_gt:', num_anomaly_per_gt)\n",
    "    num_anomaly_per_gt_all.append(num_anomaly_per_gt)\n",
    "    \n",
    "print('num_anomaly_per_gt_all:', num_anomaly_per_gt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay, adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print('')\n",
    "\n",
    "### isntance length mean and std\n",
    "mean_inst_len = np.mean(inst_len_all)\n",
    "std_inst_len = np.std(inst_len_all)\n",
    "print('avg_inst_len:', mean_inst_len)\n",
    "print('std_inst_len:', std_inst_len)\n",
    "print('')\n",
    "\n",
    "### avg number of anomalies in each detection \n",
    "mean_gt_in_inst = np.mean(gt_in_inst_all)\n",
    "std_gt_in_inst = np.std(gt_in_inst_all)\n",
    "print('avg_gt_in_inst:', mean_gt_in_inst)\n",
    "print('std_gt_in_inst:', std_gt_in_inst)\n",
    "print('')\n",
    "\n",
    "# ### avg number of detecions per GT\n",
    "# mean_inst_in_gt = np.mean(num_anomaly_per_gt_all)\n",
    "# std_inst_in_gt = np.std(num_anomaly_per_gt_all)\n",
    "# print('mean_inst_in_gt:', mean_inst_in_gt)\n",
    "# print('std_inst_in_gt:', std_inst_in_gt)\n",
    "# print('')\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "if len(conf_matrix) == 1:\n",
    "    conf_matrix = np.array([[0, 0], [0, conf_matrix[0][0]]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "# print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "# end_time = time.perf_counter()\n",
    "# elapsed_ms = (end_time - start_time) * 1000\n",
    "# print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classwise Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_fn = defaultdict(list)\n",
    "classwise_tp = defaultdict(list)\n",
    "gt_len = 0\n",
    "for file_fn, file_gt in zip(all_fn, all_gt):\n",
    "    fn = file_fn[1]\n",
    "    gt = file_gt[1]\n",
    "    print('fn:', fn)\n",
    "    print('gt:', gt)\n",
    "    for label in gt:\n",
    "        print('label:', label)\n",
    "        if label in fn:\n",
    "            classwise_fn[label[4]].append(label)\n",
    "        else:\n",
    "            classwise_tp[label[4]].append(label)\n",
    "            # print('tp:', label)\n",
    "\n",
    "    gt_len += len(gt)\n",
    "    # print('file gt:', len(gt))\n",
    "    # print('file fn:', len(fn))\n",
    "    # print('\\n')\n",
    "    # break\n",
    "\n",
    "total_fn = 0\n",
    "total_tp = 0\n",
    "keys = set(list(classwise_fn.keys()) + list(classwise_tp.keys()))\n",
    "# print('keys:', keys)\n",
    "class_recall = []\n",
    "for key in keys:\n",
    "    print('class:', key)\n",
    "    total_fn += len(classwise_fn[key])\n",
    "    total_tp += len(classwise_tp[key])\n",
    "\n",
    "    crecall = len(classwise_tp[key])/(len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    crecall = round(crecall, 4)\n",
    "\n",
    "    # print('not detected:', len(classwise_fn[key]))\n",
    "    print('detected:', len(classwise_tp[key]))\n",
    "    print('total anomalies:', len(classwise_fn[key])+len(classwise_tp[key]))\n",
    "    print('Recall (classwise):', crecall)\n",
    "    print('\\n')\n",
    "\n",
    "    class_recall.append(crecall)\n",
    "\n",
    "\n",
    "# print('total fn+tp:', total_fn+total_tp)\n",
    "# print('total gt:', gt_len)\n",
    "assert total_fn+total_tp == gt_len, 'total fn+tp not equal to total gt'\n",
    "print('All class recalls:', class_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Seperated Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Store Seperated Instances\n",
    "\n",
    "# ### number of events to take before and after the detection for cropping subsequence\n",
    "# BUFFER_EVENTS = 0\n",
    "\n",
    "# total_subseq_lens = [] \n",
    "# subseq_class = defaultdict(list)\n",
    "# new_all_detections = []\n",
    "# for (test_data, detections, test_label), features in zip(all_detections[0:], all_features):\n",
    "#     print(test_data, test_label)\n",
    "\n",
    "#     ### read traces\n",
    "#     trace = read_traces(test_data)\n",
    "#     print('trace:', len(trace))    \n",
    "\n",
    "#     ### path for sub-sequences\n",
    "#     subseq_path = os.path.dirname(test_label).replace('labels', 'diag_sepAP2_m2/subseq')\n",
    "#     feature_path = os.path.dirname(test_label).replace('labels', 'diag_sepAP2_m2/feature')\n",
    "#     # print(subseq_path)\n",
    "\n",
    "#     ### rules for subsequence\n",
    "#     # print('detections:', detections)\n",
    "#     timestamps = [x[1] for x in trace]\n",
    "#     timestamps = np.array(timestamps)\n",
    "#     # print('timestamps:', timestamps)\n",
    "\n",
    "#     ### load ground truths\n",
    "#     ground_truth_raw = read_traces(test_label)\n",
    "#     ground_truth = ground_truth_raw['labels']\n",
    "#     label_trace_name = list(ground_truth.keys())[0]\n",
    "#     ground_truth = ground_truth[label_trace_name]\n",
    "#     # print('ground truths:', ground_truth)\n",
    "#     # print('detections:', detections)\n",
    "#     # print('features:', features)\n",
    "#     # print(len(ground_truth))\n",
    "#     print(len(detections))\n",
    "\n",
    "#     all_subseq = []\n",
    "#     new_detections = []\n",
    "#     for det, feat in zip(detections, features):\n",
    "#         print('detection:', det)\n",
    "#         print('feature:', feat)\n",
    "#         var, ts, file_name = det\n",
    "#         lb_det, ub_det = ts\n",
    "#         print('ts:', ts)\n",
    "\n",
    "#         # print('bounds:', lb_det, ub_det)\n",
    "#         lb_rel_ts = [abs(x-lb_det) for x in timestamps]\n",
    "#         # print('lb_rel_ts:', lb_rel_ts)\n",
    "#         lb_det_ind = np.argmin(lb_rel_ts)\n",
    "#         # print('lb_trace ind:', lb_det_ind)\n",
    "#         # print('lb_trace:', timestamps[lb_det_ind])\n",
    "\n",
    "#         ub_rel_ts = [abs(x-ub_det) for x in timestamps]\n",
    "#         # print('ub_rel_ts:', ub_rel_ts)\n",
    "#         ub_det_ind = np.argmin(ub_rel_ts)\n",
    "#         # print('ub_trace ind:', ub_det_ind)\n",
    "#         # print('ub_trace:', timestamps[ub_det_ind])\n",
    "\n",
    "#         # ### exact match using numpy (alternate implementation)\n",
    "#         # ub_trace = np.where(timestamps == ub_det)[0][0]\n",
    "#         # print('ub_trace ind:', ub_trace)\n",
    "#         # print('ub_trace:', timestamps[ub_trace])\n",
    "\n",
    "\n",
    "#         # lb_trace_ind = np.clip(lb_det_ind - BUFFER_EVENTS, 0, None)\n",
    "#         lb_trace_ind = lb_det_ind\n",
    "#         ub_trace_ind = np.clip(ub_det_ind + BUFFER_EVENTS, ub_det_ind, len(trace))\n",
    "#         subseq_len = ub_trace_ind-lb_trace_ind\n",
    "\n",
    "#         if subseq_len < 2:\n",
    "#             # lb_trace_ind = np.clip(lb_trace_ind - (2-subseq_len), 0, None)\n",
    "#             ub_trace_ind = np.clip(ub_trace_ind + 2, None, len(trace))\n",
    "\n",
    "#         print('lb_trace ind:', lb_trace_ind)\n",
    "#         print('ub_trace ind:', ub_trace_ind)\n",
    "\n",
    "#         print('len of subseq:', ub_trace_ind-lb_trace_ind)\n",
    "#         sub_seq = trace[lb_trace_ind:ub_trace_ind+1]\n",
    "#         # print('sub_seq:', sub_seq)\n",
    "#         # print('feat:', feat)\n",
    "\n",
    "#         ### update the timestamps for new indices and detections\n",
    "#         lb_det = timestamps[lb_trace_ind]\n",
    "#         ub_det = timestamps[ub_trace_ind]\n",
    "#         # print('lb_det:', lb_det)\n",
    "#         # print('ub_det:', ub_det)\n",
    "#         ts = (int(lb_det), int(ub_det))\n",
    "#         print('new ts:', ts)\n",
    "#         new_detections.append([var, ts, file_name])\n",
    "\n",
    "#         all_subseq.append(sub_seq)\n",
    "\n",
    "#         ### save subsequence\n",
    "#         sub_seq_name = os.path.basename(test_data)+'_'+str(lb_trace_ind)+'-'+str(ub_trace_ind)+'.json'\n",
    "#         file_name = sub_seq_name.strip('.json')\n",
    "#         sub_seq_name = os.path.join(subseq_path,sub_seq_name)\n",
    "#         if not os.path.exists(subseq_path):\n",
    "#             os.makedirs(subseq_path)\n",
    "\n",
    "#         ### save feature\n",
    "#         feat_name = os.path.basename(test_data)+'_'+str(lb_trace_ind)+'-'+str(ub_trace_ind)+'.json'\n",
    "#         feat_name = os.path.join(feature_path, feat_name)\n",
    "#         # print('feat_name:', feat_name)\n",
    "#         if not os.path.exists(feature_path):\n",
    "#             os.makedirs(feature_path)\n",
    "        \n",
    "\n",
    "#         ### get labels for subsequence\n",
    "#         no_gt = True\n",
    "#         for gt in ground_truth:\n",
    "#             # print('gt:', gt)\n",
    "#             gt_ind1, gt_ind2 = gt[0], gt[1]\n",
    "#             # print('gt:', gt_ind1, gt_ind2)\n",
    "#             # print('lb_trace:', lb_trace_ind, 'ub_trace:', ub_trace_ind)\n",
    "#             if (gt_ind1 >= lb_trace_ind and gt_ind1 < ub_trace_ind) or (gt_ind2 > lb_trace_ind and gt_ind2 <= ub_trace_ind) or (gt_ind1 < lb_trace_ind < gt_ind2) or (gt_ind1 < ub_trace_ind < gt_ind2):\n",
    "#                 subseq_class[file_name] += [gt[4]]\n",
    "#                 no_gt = False\n",
    "            \n",
    "#         if no_gt:\n",
    "#             # subseq_class[file_name] += [100]   ### 100 is the label for FP detections\n",
    "#             # dont save the subseq\n",
    "#             pass\n",
    "#         else:\n",
    "#             save_json(sub_seq, sub_seq_name)\n",
    "#             print('subseq:', sub_seq_name)\n",
    "#             save_json(feat, feat_name)\n",
    "#             print('feat:', feat_name)\n",
    "\n",
    "#         ### average length of subsequence\n",
    "#         total_subseq_lens += [len(sub_seq)]\n",
    "#         if len(sub_seq) == 0:\n",
    "#             break\n",
    "\n",
    "        \n",
    "#     new_all_detections.append([test_data, new_detections, test_label])\n",
    "#     print('')\n",
    "#     # break\n",
    "\n",
    "# print('subseq_class:', subseq_class)\n",
    "# ### save the subsequence class labels\n",
    "# subseq_class_path = os.path.join(subseq_path, 'subseq_labels')\n",
    "# if not os.path.exists(subseq_class_path):\n",
    "#     os.makedirs(subseq_class_path)\n",
    "# save_json(subseq_class, os.path.join(subseq_class_path, 'subseq_class.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file, instances, inst_time, feature in zip(test_files, anomaly_instances, anomaly_timestamps, anomaly_features):\n",
    "#     print(file)\n",
    "#     print(instances)\n",
    "#     print(inst_time)\n",
    "#     print(feature)\n",
    "\n",
    "#     file_name = file.split('/')[-1].split('.')[0].split('_')\n",
    "#     file_name = '_'.join(file_name[:-1])\n",
    "#     print('file_name 3:', file_name)\n",
    "\n",
    "#     ### load ground truths\n",
    "#     test_label = os.path.join(test_labels_basepath, file_name+'_labels.json')\n",
    "#     print('test_label:', test_label)\n",
    "\n",
    "#     ground_truth_raw = read_traces(test_label)\n",
    "#     ground_truth = ground_truth_raw['labels']\n",
    "#     label_trace_name = list(ground_truth.keys())[0]\n",
    "#     ground_truth = ground_truth[label_trace_name]\n",
    "#     print('ground truths:', ground_truth)\n",
    "#     print(len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(subseq_class.keys())\n",
    "# print('keys:', keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get detections in correct format for viz\n",
    "filewise_detections = defaultdict(list)\n",
    "filewise_tp = defaultdict(list)\n",
    "filewise_fp = defaultdict(list)\n",
    "filewise_fn = defaultdict(list)\n",
    "# all_detections = new_all_detections.copy()\n",
    "for detection in all_detections:\n",
    "    # print('detection:', detection)\n",
    "    # print(detection[1])\n",
    "    file_name = detection[2]\n",
    "    det = detection[1]\n",
    "    for d in det:\n",
    "        # print('d:', d)\n",
    "        filewise_detections[file_name].append(d)\n",
    "\n",
    "for tp in all_tp:\n",
    "    file_name = tp[2]\n",
    "    t1 = tp[1]\n",
    "    for t in t1:\n",
    "        filewise_tp[file_name].append(t)\n",
    "\n",
    "for fp in all_fp:\n",
    "    file_name = fp[2]\n",
    "    f1 = fp[1]\n",
    "    for f in f1:\n",
    "        filewise_fp[file_name].append(f)\n",
    "\n",
    "for fn in all_fn:\n",
    "    file_name = fn[2]\n",
    "    f1 = fn[1]\n",
    "    for f in f1:\n",
    "        filewise_fn[file_name].append(f)\n",
    "    \n",
    "\n",
    "adapted_detections = []\n",
    "adapted_tp = []\n",
    "adapted_fp = []\n",
    "adapted_fn = []\n",
    "for file_name in filewise_detections.keys():\n",
    "    # print('file_name:', file_name)\n",
    "\n",
    "    for label_path in eval_labels_path:\n",
    "        if file_name in label_path:\n",
    "            break\n",
    "    # print('label_path:', label_path)  \n",
    "\n",
    "    test_file_path = os.path.join(faultybase_path, file_name)\n",
    "    # print('test_file_path:', test_file_path)\n",
    "\n",
    "    adapted_detections.append((test_file_path, filewise_detections[file_name], label_path))\n",
    "    adapted_tp.append((test_file_path, filewise_tp[file_name], label_path))\n",
    "    adapted_fp.append((test_file_path, filewise_fp[file_name], label_path))\n",
    "    adapted_fn.append((test_file_path, filewise_fn[file_name], label_path))\n",
    "\n",
    "    # break\n",
    "\n",
    "og_detection = all_detections.copy()\n",
    "og_tp = all_tp.copy()\n",
    "og_fp = all_fp.copy()\n",
    "og_fn = all_fn.copy()\n",
    "\n",
    "all_detections = adapted_detections\n",
    "all_tp = adapted_tp\n",
    "all_fp = adapted_fp\n",
    "all_fn = adapted_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## save detections for the dashboard to plot #############\n",
    "# import traceback\n",
    "\n",
    "# for test_data, detections, test_label in all_detections:\n",
    "#     # print(test_data, test_label)\n",
    "#     # print(test_label.replace('labels', 'detections'))\n",
    "#     detection_path = test_label.replace('labels', f'diag2_detections')\n",
    "#     detection_path = detection_path.replace('diag2_detections.json', f'diag2_detections_{DIFF_VAL}.json')\n",
    "#     # tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "#     # fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "#     # print(detections)\n",
    "#     # print(detection_path)\n",
    "\n",
    "#     detection_dir = os.path.dirname(detection_path)\n",
    "#     print(detection_dir)\n",
    "#     if not os.path.exists(detection_dir):\n",
    "#         os.makedirs(detection_dir)\n",
    "#         print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "#     try:\n",
    "#         with open(detection_path, 'w') as f:\n",
    "#             json.dump(detections, f)\n",
    "#             print(f'Saved detections in {detection_path}')\n",
    "\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         traceback.print_exception(e)\n",
    "#         print('Error in saving detections')\n",
    "#         raise ValueError('Error in saving detections')\n",
    "\n",
    "# for test_data, detections, test_label in all_tp:\n",
    "#     # print(test_data, test_label)\n",
    "#     # print(test_label.replace('labels', 'detections'))\n",
    "#     detection_path = test_label.replace('labels', 'diag2_detections')\n",
    "#     tp_detection_path = detection_path.replace('diag2_detections.json', f'tp_diag2_detections_{DIFF_VAL}.json')\n",
    "#     # fp_detection_path = detection_path.replace('ei_detections.json', 'fp_ei_detections.json')\n",
    "#     # print(detections)\n",
    "\n",
    "#     detection_dir = os.path.dirname(detection_path)\n",
    "#     # print(detection_dir)\n",
    "#     if not os.path.exists(detection_dir):\n",
    "#         os.makedirs(detection_dir)\n",
    "#         print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         with open(tp_detection_path, 'w') as f:\n",
    "#             json.dump(detections, f)\n",
    "#             print(f'Saved detections in {tp_detection_path}')\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         traceback.print_exception(e)\n",
    "#         print('Error in saving detections')\n",
    "#         raise ValueError('Error in saving detections')\n",
    "\n",
    "\n",
    "# for test_data, detections, test_label in all_fp:\n",
    "#     # print(test_data, test_label)\n",
    "#     # print(test_label.replace('labels', 'detections'))\n",
    "#     detection_path = test_label.replace('labels', 'diag2_detections')\n",
    "#     # tp_detection_path = detection_path.replace('ei_detections.json', 'tp_ei_detections.json')\n",
    "#     fp_detection_path = detection_path.replace('diag2_detections.json', f'fp_diag2_detections_{DIFF_VAL}.json')\n",
    "#     # print(detections)\n",
    "\n",
    "#     detection_dir = os.path.dirname(detection_path)\n",
    "#     # print(detection_dir)\n",
    "#     if not os.path.exists(detection_dir):\n",
    "#         os.makedirs(detection_dir)\n",
    "#         print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         with open(fp_detection_path, 'w') as f:\n",
    "#             json.dump(detections, f)\n",
    "#             print(f'Saved detections in {fp_detection_path}')\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         traceback.print_exception(e)\n",
    "#         print('Error in saving detections')\n",
    "#         raise ValueError('Error in saving detections')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
