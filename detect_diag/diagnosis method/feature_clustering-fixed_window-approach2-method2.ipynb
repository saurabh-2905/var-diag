{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- corrected_labels_created - 17/05/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 - method 2\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "WINDOW = 500                 ### window size for subsequence\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, f'diag_refsamples{WINDOW}')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "test_labels_basepath = os.path.join(faultybase_path, 'labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "# ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "eval_labels_path = [os.path.join(test_labels_basepath, x) for x in os.listdir(test_labels_basepath)]\n",
    "\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "# ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "eval_labels_path = [x for x in eval_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    # train_data_path = ref_var_samples_path\n",
    "    raise ValueError('Ref samples for variable window missing')\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "# print('train_data:', train_data_path)\n",
    "print(len(train_data_path))\n",
    "# print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "print('eval_labels:\\n', eval_labels_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "\n",
    "0. Take detection trace as the input\n",
    "1. Identify the start of the detection that is correct: part that matches with the ref_samples\n",
    "2. Skip the part that is correct and halt at the first incorrect event (anomaly)\n",
    "3. this indicares the start of first anomaly, thus add this and the next consecutive point to a new blank list (anomaly_instance), and halt at the next point\n",
    "4. Identify if there is correct part of the trace after that point by comparing with the ref_samples\n",
    "    a. if there is no matching ref_sample, then shift to the next point an add it to the list (anomaly_instance). Repeat this until the end of the trace\n",
    "    b. if there is matching ref_sample, skip the matching part and halt at the first incorrect event (anomaly). Add this point a new blank list (next anomaly_instance). Repeat it until the end of the trace\n",
    "5. collection of all the anomy_instance will give the instances of the anomaly detected\n",
    "\n",
    "\n",
    "\n",
    "Feature extraction and Clustering:\n",
    "- use the seperated instances to extract features\n",
    "- cluster the features (start with kmeans)\n",
    "- try the same feature extractors as Approach 1\n",
    "    - TSFEL\n",
    "    - SegLeran\n",
    "    - CNN+LSTM\n",
    "    - Autoencoder\n",
    "    - our method\n",
    "\n",
    " \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Seperate if multiple instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_correct_part(ref_samples, test_events, test_intervals):\n",
    "    '''\n",
    "    check if any matching event trace in present based on first 2 points\n",
    "    if yes, then check the number of matching events and intervals, remove the matching part in the event trace and return the remaining part\n",
    "    if no, then return the same event trace\n",
    "    '''\n",
    "\n",
    "    test_data_len = len(test_events)\n",
    "    # print('test_events:', test_events)\n",
    "    print('test_data_len:', test_data_len)\n",
    "    ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "    shortlisted_ref_events = []\n",
    "    shortlisted_ref_intervals = []\n",
    "    zero_count = []\n",
    "    for ref_sample in ref_samples:\n",
    "        # print('ref_sample:', ref_sample[0][:5])\n",
    "        # event_diff = np.array(ref_sample[0][0:len(test_events)]) - np.array(test_events)\n",
    "        # print('event_diff:', event_diff)\n",
    "        # print('event_diff:', len(event_diff))\n",
    "        # print('zeros:', np.where(event_diff==0)[0].shape)\n",
    "        # if len(test_events) == 276:\n",
    "        #     print('ref_sample:', ref_sample[0][:5])\n",
    "        #     print('test_events:', test_events[:5])\n",
    "        if ref_sample[0][:2] == test_events[:2]:\n",
    "            ref_events = ref_sample[0][:test_data_len]\n",
    "            ref_intervals = ref_sample[1][:test_data_len]\n",
    "            if len(test_events) > 500:\n",
    "                diff_events = np.array(ref_events) - np.array(test_events[:500])\n",
    "                diff_intervals = np.abs(np.array(ref_intervals) - np.array(test_intervals[:500]))\n",
    "            else:\n",
    "                diff_events = np.array(ref_events) - np.array(test_events)\n",
    "                diff_intervals = np.abs(np.array(ref_intervals) - np.array(test_intervals))\n",
    "    \n",
    "            # if len(test_events) == 276:\n",
    "            #     print('diff_events:', diff_events)\n",
    "            #     print('diff_intervals:', diff_intervals)\n",
    "            count = 0\n",
    "            # print(sf[0], sf[1])\n",
    "            for esf, esi in zip(diff_events, diff_intervals):\n",
    "                ### check if events and intervals are same\n",
    "                # if esf == 0 and esi < 5:\n",
    "                if esf == 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break   ### part of the logic, do not remove\n",
    "\n",
    "            # print('zero_count:', count)\n",
    "            ### depulicate the ref samples\n",
    "            if ref_events not in shortlisted_ref_events:\n",
    "                zero_count.append(count)\n",
    "                shortlisted_ref_events.append(ref_events)\n",
    "                shortlisted_ref_intervals.append(ref_intervals)\n",
    "            # print('count:', count)  \n",
    "\n",
    "        # break\n",
    "\n",
    "    # print('zero_count:', zero_count)\n",
    "    # print('shortlisted_ref_samples:', len(shortlisted_ref_events))\n",
    "\n",
    "    ### select the ref_sample_events with maximum leading zeros\n",
    "    if len(zero_count) != 0:\n",
    "        max_zero_count = max(zero_count)\n",
    "        zero_count = np.array(zero_count)\n",
    "        max_zero_count_ind = np.where(zero_count==max_zero_count)[0][0]\n",
    "        # print('max_zero_count_ind:', max_zero_count_ind)\n",
    "        selected_ref_events = shortlisted_ref_events[max_zero_count_ind]\n",
    "        selected_ref_intervals = shortlisted_ref_intervals[max_zero_count_ind]\n",
    "        # print('selected_ref_events:', selected_ref_events[:max_zero_count+1])\n",
    "    else:\n",
    "        max_zero_count = 0\n",
    "        selected_ref_events = None\n",
    "        selected_ref_intervals = None\n",
    "\n",
    "    if max_zero_count == 0:\n",
    "        print('func: No match found')\n",
    "        return None, selected_ref_events, selected_ref_intervals\n",
    "    else:\n",
    "        ### select the point where the last match happened\n",
    "        # last_matched_point = max_zero_count-1\n",
    "        # print('last_matched_point:', last_matched_point)\n",
    "        # print('test_events:', test_events[:last_matched_point])\n",
    "        # striped_test_events = test_events[last_matched_point:]\n",
    "        # striped_test_intervals = test_intervals[last_matched_point:]\n",
    "        # striped_test_timestamps = test_timestamps[last_matched_point:]\n",
    "        # print('striped_test_events:', striped_test_events)\n",
    "        # print('max count:', max_zero_count, len(test_events), len(striped_test_events))\n",
    "\n",
    "        \n",
    "        if max_zero_count == len(test_events):\n",
    "            print('func: All events are same')\n",
    "            return max_zero_count, selected_ref_events, selected_ref_intervals\n",
    "        else:\n",
    "            return max_zero_count, selected_ref_events, selected_ref_intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "    \n",
    "### load all the reference samples (fixed window size)\n",
    "ref_samples = []\n",
    "for ref_sample in train_data_path:\n",
    "    ref_samples.append(read_traces(ref_sample))\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "### load the test samples and compare with the reference samples\n",
    "anomaly_instances = []\n",
    "anomaly_timestamps = []\n",
    "anomaly_features = []\n",
    "test_files = []\n",
    "for test_data in test_data_path[0:]:\n",
    "    print('test_data:', test_data)\n",
    "    ### read the subseq\n",
    "    test_trace = read_traces(test_data)\n",
    "    print('test_trace:', test_trace)\n",
    "    test_data_len = len(test_trace)\n",
    "    # print('test_data_len:', test_data_len)\n",
    "\n",
    "    # if test_data_len > 500:\n",
    "    #     # print('test data length is more than 500, skipping...')\n",
    "    #     # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "    #     # continue\n",
    "\n",
    "    #     print('test data length is more than 500, truncating...')\n",
    "    #     test_trace = test_trace[:500]\n",
    "    #     test_data_len = 500\n",
    "    \n",
    "    ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "    ### old implementation with 0 at start of intervals\n",
    "    # test_events = []\n",
    "    # test_intervals = []\n",
    "    # test_timestamps = []\n",
    "    # prev_time = test_trace[0][1]\n",
    "    # time_diff = 0\n",
    "    # for x in test_trace:\n",
    "    #     time_diff = x[1] - prev_time\n",
    "    #     test_intervals.append(time_diff)\n",
    "    #     prev_time = x[1]\n",
    "    #     test_events.append(x[0])\n",
    "    #     test_timestamps.append(x[1])\n",
    "\n",
    "    ### new implementation without 0 at start of intervals\n",
    "    test_events = []\n",
    "    test_intervals = []\n",
    "    test_timestamps = []\n",
    "    for x,y in zip(test_trace[:-1], test_trace[1:]):\n",
    "        test_events.append(x[0])   ### first event\n",
    "        test_intervals.append(y[1] - x[1])   ### difference between the timestamps of second and first event\n",
    "        test_timestamps.append(x[1])   ### first timestamp\n",
    "    # print('test_events:', len(test_events))\n",
    "    # print('test_intervals:', test_intervals)\n",
    "\n",
    "    assert len(test_events)+1 == len(test_intervals)+1 == test_data_len\n",
    "\n",
    "    # ### get detection class\n",
    "\n",
    "    # test_class_labels = read_json(test_labels_path[0])\n",
    "\n",
    "    ### store the first to consecutive points as the first anomaly instance\n",
    "    an_instance = []\n",
    "    an_timestamps = []\n",
    "    all_instances = []\n",
    "    all_timestamps = []\n",
    "    all_ref_events = []\n",
    "    all_ref_intervals = []\n",
    "    all_striped_test_events = []\n",
    "    all_striped_test_intervals = []\n",
    "    all_striped_timestamps = []\n",
    "    an_affected_events = []    ### store the additional/missing events compared to ref_samples\n",
    "    all_affected_events = []\n",
    "\n",
    "\n",
    "    striped_test_events = test_events\n",
    "    striped_test_intervals = test_intervals\n",
    "    striped_timestamps = test_timestamps\n",
    "    print('striped_test_events:', striped_test_events, len(striped_test_events))\n",
    "    # print('striped_test_intervals:', striped_test_intervals, len(striped_test_intervals))\n",
    "    has_checked = False\n",
    "    i = 0\n",
    "    while len(striped_test_events) > 0:\n",
    "    # while i < 6:\n",
    "        ### remove the initial correct part of the trace\n",
    "        # print('ref_samples:', ref_samples, striped_test_events, striped_test_intervals)\n",
    "        print('give input:', len(striped_test_events))\n",
    "        max_zero_count, selected_ref_events, selected_ref_intervals = strip_correct_part(ref_samples, striped_test_events, striped_test_intervals)\n",
    "        print('max_zero_count:', max_zero_count, 'len:', len(striped_test_events))\n",
    "        \n",
    "        if selected_ref_events != None:\n",
    "            ### first collect the trace that is given as input\n",
    "            all_striped_test_events.append(striped_test_events)\n",
    "            all_striped_test_intervals.append(striped_test_intervals)\n",
    "            all_striped_timestamps.append(striped_timestamps)\n",
    "            ### get the ref trace that matched with the test trace\n",
    "            all_ref_events.append(selected_ref_events)\n",
    "            all_ref_intervals.append(selected_ref_intervals)\n",
    "        else:\n",
    "            print('No match found')\n",
    "            all_striped_test_events.append(striped_test_events)\n",
    "            all_striped_test_intervals.append(striped_test_intervals)\n",
    "            all_striped_timestamps.append(striped_timestamps)\n",
    "            all_ref_events.append(striped_ref_events)\n",
    "\n",
    "\n",
    "        print('selected_ref_events:', selected_ref_events)\n",
    "\n",
    "        if max_zero_count == len(striped_test_events):\n",
    "            print('All events are same')\n",
    "            striped_test_events = []\n",
    "            striped_test_intervals = []\n",
    "            striped_timestamps = []\n",
    "        # elif max_zero_count != None:\n",
    "        #     striped_test_events = striped_test_events[max_zero_count-1:]\n",
    "        #     striped_test_intervals = striped_test_intervals[max_zero_count-1:]\n",
    "        #     striped_timestamps = striped_timestamps[max_zero_count-1:]\n",
    "        print('striped_test_events 1:', striped_test_events, len(striped_test_events))\n",
    "\n",
    "        # break\n",
    "        ### store the first anomaly instance (first two consecutive points)\n",
    "        if max_zero_count != None:\n",
    "            # print('debug', max_zero_count, len(striped_test_events))\n",
    "            if len(striped_test_events) != 0:\n",
    "                if has_checked:\n",
    "                    if len(an_instance) != 0:\n",
    "                        ### new anomaly instance detected\n",
    "                        all_instances.append(an_instance)\n",
    "                        all_timestamps.append(an_timestamps)\n",
    "                        all_affected_events.append(an_affected_events)\n",
    "                        an_instance = []\n",
    "                        an_timestamps = []\n",
    "                        an_affected_events = []\n",
    "\n",
    "                    ### start, where first anomaly instance is detected\n",
    "                    print('Checked, new instance created')\n",
    "                    seq_start = np.clip(max_zero_count-2, 0, None)\n",
    "                    seq_end = np.clip(seq_start+3, 0, len(striped_test_events))\n",
    "                    print('seq_start:', seq_start, 'seq_end:', seq_end)\n",
    "                    an_instance.extend(striped_test_events[seq_start:seq_end])\n",
    "                    an_timestamps.extend(striped_timestamps[seq_start:seq_end])\n",
    "                    print('selected_ref_events:', selected_ref_events)\n",
    "                    an_affected_events.extend(selected_ref_events[seq_start:seq_end])\n",
    "                    striped_test_events = striped_test_events[seq_end:]\n",
    "                    striped_test_intervals = striped_test_intervals[seq_end:]\n",
    "                    striped_timestamps = striped_timestamps[seq_end:]\n",
    "                    striped_ref_events = selected_ref_events[seq_end:]\n",
    "                    # print('an_instance:', an_instance)\n",
    "                    print('striped_test_events 3.1:', striped_test_events, len(striped_test_events))\n",
    "                    has_checked = False\n",
    "                else:\n",
    "                    if len(an_instance) != 0:\n",
    "                        ### new anomaly instance detected\n",
    "                        all_instances.append(an_instance)\n",
    "                        all_timestamps.append(an_timestamps)\n",
    "                        all_affected_events.append(an_affected_events)\n",
    "                        an_instance = []\n",
    "                        an_timestamps = []\n",
    "                        an_affected_events = []\n",
    "\n",
    "                    ### recheck from last 5 matching points to see in really an anomaly\n",
    "                    print('Detected discripancy, rechecking...')\n",
    "                    step_back = np.clip(max_zero_count-30, 0, None)\n",
    "                    print('steping back', step_back)\n",
    "                    striped_test_events = striped_test_events[step_back:]\n",
    "                    striped_test_intervals = striped_test_intervals[step_back:]\n",
    "                    striped_timestamps = striped_timestamps[step_back:]\n",
    "                    print('striped_test_events 2:', striped_test_events, len(striped_test_events))\n",
    "                    has_checked = True\n",
    "                    continue ### continue to the next iteration\n",
    "\n",
    "            else:\n",
    "                if len(an_instance) != 0:\n",
    "                    all_instances.append(an_instance)\n",
    "                    all_timestamps.append(an_timestamps)\n",
    "                    all_affected_events.append(an_affected_events)\n",
    "                    an_instance = []\n",
    "                    an_timestamps = []\n",
    "                    an_affected_events = []\n",
    "                anomaly_instances.append(all_instances)\n",
    "                anomaly_timestamps.append(all_timestamps)\n",
    "                anomaly_features.append(all_affected_events)\n",
    "                test_files.append(test_data)\n",
    "                print('DONE 1')\n",
    "                break ### part of the logic, do not remove\n",
    "        else:\n",
    "            if len(striped_test_events) > 1:\n",
    "                ### normal functionality loop\n",
    "                print('extending previous instance')\n",
    "                an_instance.extend(striped_test_events[:1])\n",
    "                an_timestamps.extend(striped_timestamps[:1])\n",
    "                an_affected_events.extend(striped_ref_events[:1])\n",
    "                striped_test_events = striped_test_events[1:]\n",
    "                striped_test_intervals = striped_test_intervals[1:]\n",
    "                striped_timestamps = striped_timestamps[1:]\n",
    "                striped_ref_events = striped_ref_events[1:]\n",
    "                print('striped_test_events 3.2:', striped_test_events, len(striped_test_events))\n",
    "            else:\n",
    "                ### for last iteration, when len(striped_test_events) == 1   \n",
    "                if len(an_instance) != 0:\n",
    "                    all_instances.append(an_instance)\n",
    "                    all_timestamps.append(an_timestamps)\n",
    "                    all_affected_events.append(an_affected_events)\n",
    "                    an_instance = []\n",
    "                    an_timestamps = []\n",
    "                    an_affected_events = []\n",
    "                anomaly_instances.append(all_instances)\n",
    "                anomaly_timestamps.append(all_timestamps)\n",
    "                anomaly_features.append(all_affected_events)\n",
    "                test_files.append(test_data)\n",
    "                print('DONE 2')\n",
    "                break  #### part of the logic, do not remove\n",
    "        print('an_instance:', an_instance)\n",
    "        print('all_instances:', all_instances)\n",
    "        print('an_affected_events:', an_affected_events)\n",
    "        print('all_affected_events:', all_affected_events)\n",
    "        if len(striped_test_events) == 0:\n",
    "            all_instances.append(an_instance)\n",
    "            all_timestamps.append(an_timestamps)\n",
    "            all_affected_events.append(an_affected_events)\n",
    "            an_instance = []\n",
    "            an_timestamps = []\n",
    "            an_affected_events = []\n",
    "            anomaly_instances.append(all_instances)\n",
    "            anomaly_timestamps.append(all_timestamps)\n",
    "            anomaly_features.append(all_affected_events)\n",
    "            test_files.append(test_data)\n",
    "            print('DONE 3')\n",
    "        print('')    \n",
    "        i += 1\n",
    "        # if i == 4:\n",
    "        #     print('break')\n",
    "        #     break\n",
    "    # break\n",
    "print('anomaly_instances:', anomaly_instances) \n",
    "print('anomaly_timestamps:', anomaly_timestamps)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_affected_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_axis = np.arange(0, len(test_trace), 1)\n",
    "\n",
    "\n",
    "\n",
    "# ### prepare test_trace for plotting\n",
    "# plot_data = dict()\n",
    "# plot_data['subseq'] = test_events   ### y_data (traces)\n",
    "\n",
    "# # for i, fv in enumerate(shortlisted_ref_samples):\n",
    "# #     plot_data[f'feat1_{i}'] = fv[0]\n",
    "# plot_data['ref_samples'] = selected_ref_events\n",
    "    \n",
    "# df_feat1 = pd.DataFrame(plot_data)\n",
    "\n",
    "# plot_data = dict()\n",
    "# plot_data['intervals'] = test_intervals   ### y_data (traces)\n",
    "\n",
    "# # for i, fv in enumerate(feature_vectors):\n",
    "# #     plot_data[f'feat2_{i}'] = fv[1]\n",
    "# plot_data['ref_intervals'] = selected_ref_intervals\n",
    "\n",
    "# df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "# fig = px.line(df_feat1, title='features')\n",
    "# fig.show()\n",
    "\n",
    "# fig = px.line(df_feat2, title='features')\n",
    "# fig.show()\n",
    "\n",
    "for i in range(len(all_striped_test_events)):\n",
    "    plot_data = dict()\n",
    "    plot_data['subseq'] = all_striped_test_events[i]   ### y_data (traces)\n",
    "    print('ref:', all_ref_events[i])\n",
    "    print('det:', all_striped_test_events[i])\n",
    "    if all_ref_events[i] != None:\n",
    "        plot_data['ref_samples'] = all_ref_events[i]\n",
    "            \n",
    "        df_feat1 = pd.DataFrame(plot_data)\n",
    "        # print('df_feat1:', df_feat1)\n",
    "        \n",
    "        plot_data = dict()\n",
    "        plot_data['intervals'] = all_striped_test_intervals[i]   ### y_data (traces)\n",
    "        plot_data['ref_intervals'] = all_ref_intervals[i]\n",
    "\n",
    "        df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "        fig = px.line(df_feat1, title='features')\n",
    "        fig.show()\n",
    "\n",
    "        fig = px.line(df_feat2, title='features')\n",
    "        fig.show()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for an in anomaly_instances:\n",
    "#     print('an:', an)\n",
    "#     print('len:', len(an))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_labels = read_json(test_labels_path[0])\n",
    "# print(test_class_labels)\n",
    "\n",
    "for l, i, k, j in zip(test_files, anomaly_instances, anomaly_timestamps, anomaly_features):\n",
    "    # print(l)\n",
    "    file_name = l.split('/')[-1].split('.')[0]\n",
    "    print(file_name)\n",
    "\n",
    "    class_labels = test_class_labels[file_name]\n",
    "\n",
    "    print('length of labels and predictions:', len(class_labels), len(i))\n",
    "    print('class:', class_labels)\n",
    "    print('detection', i)\n",
    "    print('missing/additional events:', j)\n",
    "    print('timestamp', k)\n",
    "\n",
    "    print('')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_VAL = 5\n",
    "\n",
    "ei = exeInt()\n",
    "\n",
    "#### check how many instance are identified correctly\n",
    "\n",
    "### load labels for each trial file\n",
    "all_test_labels = dict()\n",
    "for label_path in eval_labels_path:\n",
    "    # print(label_path)\n",
    "    file_name = label_path.split('/')[-1].split('.')[0][:-7]\n",
    "    print('file_name 1:', file_name)\n",
    "    eval_labels = read_json(label_path)\n",
    "    key = list(eval_labels['labels'].keys())[0]\n",
    "    labels = eval_labels['labels'][key]\n",
    "    all_test_labels[file_name] = labels\n",
    "    # print('labels:', labels)\n",
    "    # print(eval_labels)\n",
    "    # print('')\n",
    "\n",
    "inst_len_all = []\n",
    "gt_in_inst_all = []\n",
    "inst_in_gt_all = defaultdict(list)\n",
    "\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_fn = []\n",
    "all_gt = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for file, instances, inst_time in zip(test_files, anomaly_instances, anomaly_timestamps):\n",
    "    print(file)\n",
    "    file_name = file.split('/')[-1].split('.')[0].split('_')\n",
    "    file_name = '_'.join(file_name[:-1])\n",
    "    print('file_name 2:', file_name)\n",
    "    det_file_name = file.split('/')[-1].split('.')[0]\n",
    "    print('det_file_name:', det_file_name)\n",
    "    subseq_ind = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    subseq_start = int(subseq_ind.split('-')[0])\n",
    "    subseq_end = int(subseq_ind.split('-')[1])\n",
    "    label_file = file.split('/')[:-2]\n",
    "    label_file = '/'.join(label_file) + '/labels/' + file_name + '_labels.json'\n",
    "    test_file = file.split('/')[:-2]\n",
    "    test_file = '/'.join(test_file) + '/' + file_name\n",
    "    print('test_file:', test_file)\n",
    "    # print('subseq_ind:', subseq_ind)\n",
    "    # print('instances:', instances)\n",
    "    # print('inst_time:', inst_time)\n",
    "    \n",
    "    ### check against labels if the instances are correct\n",
    "    labels = all_test_labels[file_name]\n",
    "    # print('instances:', instances)\n",
    "    # print('inst_time:', inst_time)\n",
    "    # print('labels:', labels)\n",
    "\n",
    "    detection = []\n",
    "    for i, t in zip(instances, inst_time):\n",
    "        start_anomaly = t[0]\n",
    "        end_anomaly = t[-1]\n",
    "        start_event = i[0]\n",
    "        end_event = i[-1]\n",
    "\n",
    "        anomaly = [(start_event, end_event), (start_anomaly, end_anomaly), det_file_name]\n",
    "        detection.append(anomaly)\n",
    "\n",
    "        print('anomaly and time:', i, t)\n",
    "\n",
    "        ######### calculate avg length of instances ##########\n",
    "        inst_len = len(i)\n",
    "        # print('inst_len:', inst_len)\n",
    "        inst_len_all.append(inst_len)\n",
    "\n",
    "        ######## calculate number of anomalies (GT) present in each detection (metric to evaluate how well is the seperation done, ideally it should be 1) #######\n",
    "        gt_in_inst = []\n",
    "        for l in labels:\n",
    "            gt_ind1, gt_ind2 = l[2], l[3]\n",
    "            # print('gt:', gt_ind1, gt_ind2)\n",
    "            # print('start_anomaly:', start_anomaly, 'end_anomaly:', end_anomaly)\n",
    "            if (gt_ind1 >= start_anomaly and gt_ind1 < end_anomaly) or (gt_ind2 > start_anomaly and gt_ind2 <= end_anomaly) or (gt_ind1 < start_anomaly < gt_ind2) or (gt_ind1 < end_anomaly < gt_ind2):\n",
    "                gt_in_inst += [l]\n",
    "        print('gt_in_inst:', gt_in_inst)\n",
    "        num_gt_in_inst = len(gt_in_inst)\n",
    "        if num_gt_in_inst > 0:\n",
    "            gt_in_inst_all.append(num_gt_in_inst)\n",
    "            for gtii in gt_in_inst:\n",
    "                inst_key = str(gtii)\n",
    "                inst_in_gt_all[inst_key].append(anomaly)\n",
    "\n",
    "\n",
    "    gts = []\n",
    "    for l in labels:\n",
    "        # if subseq_start < l[0] < subseq_end or subseq_start < l[1] < subseq_end:\n",
    "        if (subseq_start-5 < l[0] < subseq_end-5 and subseq_start+5 < l[1] < subseq_end+5):\n",
    "\n",
    "            gts.append(l)\n",
    "\n",
    "    print('detection:', detection)\n",
    "    print('gts:', gts)\n",
    "    correct_pred, rest_pred, y_pred, y_true, false_neg = ei.get_correct_detections(detection, gts)\n",
    "    print(correct_pred, rest_pred, y_pred, y_true, false_neg)\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "\n",
    "    all_detections += [(test_file, detection, label_file)]  ### used to plot detections\n",
    "    all_tp += [(test_file, correct_pred, label_file)]\n",
    "    all_fp += [(test_file, rest_pred, label_file)]\n",
    "    all_fn += [(test_file, false_neg, label_file)]\n",
    "    all_gt += [(test_file, gts, label_file)]\n",
    "\n",
    "    print('')\n",
    "    # break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If multiple detections peresent for single GT, calculate averege number of detections per GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### calculate number of instances in each ground truth label\n",
    "inst_key = inst_in_gt_all.keys()\n",
    "# print('inst_key:', inst_key)\n",
    "num_anomaly_per_gt_all = []\n",
    "for ik in inst_key:\n",
    "    print('key:', ik)\n",
    "    anomaly_per_gt = inst_in_gt_all[ik]\n",
    "    # print('anomaly_per_gt:', anomaly_per_gt)\n",
    "    num_anomaly_per_gt = len(anomaly_per_gt)\n",
    "    # print('num_anomaly_per_gt:', num_anomaly_per_gt)\n",
    "    num_anomaly_per_gt_all.append(num_anomaly_per_gt)\n",
    "    \n",
    "print('num_anomaly_per_gt_all:', num_anomaly_per_gt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print('')\n",
    "\n",
    "\n",
    "### isntance length mean and std\n",
    "mean_inst_len = np.mean(inst_len_all)\n",
    "std_inst_len = np.std(inst_len_all)\n",
    "print('avg_inst_len:', mean_inst_len)\n",
    "print('std_inst_len:', std_inst_len)\n",
    "print('')\n",
    "\n",
    "### avg number of anomalies in each detection \n",
    "mean_gt_in_inst = np.mean(gt_in_inst_all)\n",
    "std_gt_in_inst = np.std(gt_in_inst_all)\n",
    "print('avg_gt_in_inst:', mean_gt_in_inst)\n",
    "print('std_gt_in_inst:', std_gt_in_inst)\n",
    "print('')\n",
    "\n",
    "### avg number of detecions per GT\n",
    "mean_inst_in_gt = np.mean(num_anomaly_per_gt_all)\n",
    "std_inst_in_gt = np.std(num_anomaly_per_gt_all)\n",
    "print('mean_inst_in_gt:', mean_inst_in_gt)\n",
    "print('std_inst_in_gt:', std_inst_in_gt)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "if len(conf_matrix) == 1:\n",
    "    conf_matrix = np.array([[0, 0], [0, conf_matrix[0][0]]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Seperated Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store Seperated Instances\n",
    "\n",
    "### number of events to take before and after the detection for cropping subsequence\n",
    "BUFFER_EVENTS = 0\n",
    "\n",
    "total_subseq_lens = [] \n",
    "subseq_class = defaultdict(list)\n",
    "new_all_detections = []\n",
    "for test_data, detections, test_label in all_detections[0:]:\n",
    "    print(test_data, test_label)\n",
    "\n",
    "    ### read traces\n",
    "    trace = read_traces(test_data)\n",
    "    print('trace:', len(trace))    \n",
    "\n",
    "    ### path for sub-sequences\n",
    "    subseq_path = os.path.dirname(test_label).replace('labels', 'diag_sepAP2_m2')\n",
    "    # print(subseq_path)\n",
    "\n",
    "    ### rules for subsequence\n",
    "    # print('detections:', detections)\n",
    "    timestamps = [x[1] for x in trace]\n",
    "    timestamps = np.array(timestamps)\n",
    "    # print('timestamps:', timestamps)\n",
    "\n",
    "    ### load ground truths\n",
    "    ground_truth_raw = read_traces(test_label)\n",
    "    ground_truth = ground_truth_raw['labels']\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "    # print('ground truths:', ground_truth)\n",
    "    print(len(ground_truth))\n",
    "    print(len(detections))\n",
    "\n",
    "    all_subseq = []\n",
    "    new_detections = []\n",
    "    for det in detections:\n",
    "        print('detection:', det)\n",
    "        var, ts, file_name = det\n",
    "        lb_det, ub_det = ts\n",
    "        print('ts:', ts)\n",
    "\n",
    "        # print('bounds:', lb_det, ub_det)\n",
    "        lb_rel_ts = [abs(x-lb_det) for x in timestamps]\n",
    "        # print('lb_rel_ts:', lb_rel_ts)\n",
    "        lb_det_ind = np.argmin(lb_rel_ts)\n",
    "        # print('lb_trace ind:', lb_det_ind)\n",
    "        # print('lb_trace:', timestamps[lb_det_ind])\n",
    "\n",
    "        ub_rel_ts = [abs(x-ub_det) for x in timestamps]\n",
    "        # print('ub_rel_ts:', ub_rel_ts)\n",
    "        ub_det_ind = np.argmin(ub_rel_ts)\n",
    "        # print('ub_trace ind:', ub_det_ind)\n",
    "        # print('ub_trace:', timestamps[ub_det_ind])\n",
    "\n",
    "        # ### exact match using numpy (alternate implementation)\n",
    "        # ub_trace = np.where(timestamps == ub_det)[0][0]\n",
    "        # print('ub_trace ind:', ub_trace)\n",
    "        # print('ub_trace:', timestamps[ub_trace])\n",
    "\n",
    "\n",
    "        # lb_trace_ind = np.clip(lb_det_ind - BUFFER_EVENTS, 0, None)\n",
    "        lb_trace_ind = lb_det_ind\n",
    "        ub_trace_ind = np.clip(ub_det_ind + BUFFER_EVENTS, ub_det_ind, len(trace))\n",
    "        subseq_len = ub_trace_ind-lb_trace_ind\n",
    "\n",
    "        if subseq_len < 2:\n",
    "            # lb_trace_ind = np.clip(lb_trace_ind - (2-subseq_len), 0, None)\n",
    "            ub_trace_ind = np.clip(ub_trace_ind + 2, None, len(trace))\n",
    "\n",
    "        print('lb_trace ind:', lb_trace_ind)\n",
    "        print('ub_trace ind:', ub_trace_ind)\n",
    "\n",
    "        print('len of subseq:', ub_trace_ind-lb_trace_ind)\n",
    "        sub_seq = trace[lb_trace_ind:ub_trace_ind]\n",
    "\n",
    "        ### update the timestamps for new indices and detections\n",
    "        lb_det = timestamps[lb_trace_ind]\n",
    "        ub_det = timestamps[ub_trace_ind]\n",
    "        # print('lb_det:', lb_det)\n",
    "        # print('ub_det:', ub_det)\n",
    "        ts = (int(lb_det), int(ub_det))\n",
    "        print('new ts:', ts)\n",
    "        new_detections.append([var, ts, file_name])\n",
    "\n",
    "        all_subseq.append(sub_seq)\n",
    "\n",
    "        ### save subsequence\n",
    "        sub_seq_name = os.path.basename(test_data)+'_'+str(lb_trace_ind)+'-'+str(ub_trace_ind)+'.json'\n",
    "        file_name = sub_seq_name.strip('.json')\n",
    "        sub_seq_name = os.path.join(subseq_path,sub_seq_name)\n",
    "        if not os.path.exists(subseq_path):\n",
    "            os.makedirs(subseq_path)\n",
    "        \n",
    "\n",
    "        ### get labels for subsequence\n",
    "        no_gt = True\n",
    "        for gt in ground_truth:\n",
    "            # print('gt:', gt)\n",
    "            gt_ind1, gt_ind2 = gt[0], gt[1]\n",
    "            # print('gt:', gt_ind1, gt_ind2)\n",
    "            # print('lb_trace:', lb_trace_ind, 'ub_trace:', ub_trace_ind)\n",
    "            if (gt_ind1 >= lb_trace_ind and gt_ind1 < ub_trace_ind) or (gt_ind2 > lb_trace_ind and gt_ind2 <= ub_trace_ind) or (gt_ind1 < lb_trace_ind < gt_ind2) or (gt_ind1 < ub_trace_ind < gt_ind2):\n",
    "                subseq_class[file_name] += [gt[4]]\n",
    "                no_gt = False\n",
    "            \n",
    "        if no_gt:\n",
    "            # subseq_class[file_name] += [100]   ### 100 is the label for FP detections\n",
    "            # dont save the subseq\n",
    "            pass\n",
    "        else:\n",
    "            save_json(sub_seq, sub_seq_name)\n",
    "            print('subseq:', sub_seq_name)\n",
    "\n",
    "        ### average length of subsequence\n",
    "        total_subseq_lens += [len(sub_seq)]\n",
    "        if len(sub_seq) == 0:\n",
    "            break\n",
    "\n",
    "        \n",
    "    new_all_detections.append([test_data, new_detections, test_label])\n",
    "    print('')\n",
    "    # break\n",
    "\n",
    "print('subseq_class:', subseq_class)\n",
    "### save the subsequence class labels\n",
    "subseq_class_path = os.path.join(subseq_path, 'subseq_labels')\n",
    "if not os.path.exists(subseq_class_path):\n",
    "    os.makedirs(subseq_class_path)\n",
    "save_json(subseq_class, os.path.join(subseq_class_path, 'subseq_class.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(subseq_class.keys())\n",
    "print('keys:', keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get detections in correct format for viz\n",
    "filewise_detections = defaultdict(list)\n",
    "filewise_tp = defaultdict(list)\n",
    "filewise_fp = defaultdict(list)\n",
    "filewise_fn = defaultdict(list)\n",
    "# all_detections = new_all_detections.copy()\n",
    "for detection in all_detections:\n",
    "    # print('detection:', detection)\n",
    "    # print(detection[1])\n",
    "    file_name = detection[2]\n",
    "    det = detection[1]\n",
    "    for d in det:\n",
    "        # print('d:', d)\n",
    "        filewise_detections[file_name].append(d)\n",
    "\n",
    "for tp in all_tp:\n",
    "    file_name = tp[2]\n",
    "    t1 = tp[1]\n",
    "    for t in t1:\n",
    "        filewise_tp[file_name].append(t)\n",
    "\n",
    "for fp in all_fp:\n",
    "    file_name = fp[2]\n",
    "    f1 = fp[1]\n",
    "    for f in f1:\n",
    "        filewise_fp[file_name].append(f)\n",
    "\n",
    "for fn in all_fn:\n",
    "    file_name = fn[2]\n",
    "    f1 = fn[1]\n",
    "    for f in f1:\n",
    "        filewise_fn[file_name].append(f)\n",
    "    \n",
    "\n",
    "adapted_detections = []\n",
    "adapted_tp = []\n",
    "adapted_fp = []\n",
    "adapted_fn = []\n",
    "for file_name in filewise_detections.keys():\n",
    "    # print('file_name:', file_name)\n",
    "\n",
    "    for label_path in eval_labels_path:\n",
    "        if file_name in label_path:\n",
    "            break\n",
    "    # print('label_path:', label_path)  \n",
    "\n",
    "    test_file_path = os.path.join(faultybase_path, file_name)\n",
    "    # print('test_file_path:', test_file_path)\n",
    "\n",
    "    adapted_detections.append((test_file_path, filewise_detections[file_name], label_path))\n",
    "    adapted_tp.append((test_file_path, filewise_tp[file_name], label_path))\n",
    "    adapted_fp.append((test_file_path, filewise_fp[file_name], label_path))\n",
    "    adapted_fn.append((test_file_path, filewise_fn[file_name], label_path))\n",
    "\n",
    "    # break\n",
    "\n",
    "og_detection = all_detections.copy()\n",
    "og_tp = all_tp.copy()\n",
    "og_fp = all_fp.copy()\n",
    "og_fn = all_fn.copy()\n",
    "\n",
    "all_detections = adapted_detections\n",
    "all_tp = adapted_tp\n",
    "all_fp = adapted_fp\n",
    "all_fn = adapted_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## save detections for the dashboard to plot #############\n",
    "import traceback\n",
    "\n",
    "for test_data, detections, test_label in all_detections:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', f'diag2_detections')\n",
    "    detection_path = detection_path.replace('diag2_detections.json', f'diag2_detections_{DIFF_VAL}.json')\n",
    "    # tp_detection_path = detection_path.replace('ei_detections.json', f'tp_ei_detections_{DIFF_VAL}.json')\n",
    "    # fp_detection_path = detection_path.replace('ei_detections.json', f'fp_ei_detections_{DIFF_VAL}.json')\n",
    "    # print(detections)\n",
    "    # print(detection_path)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "        with open(detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {detection_path}')\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        raise ValueError('Error in saving detections')\n",
    "\n",
    "for test_data, detections, test_label in all_tp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'diag2_detections')\n",
    "    tp_detection_path = detection_path.replace('diag2_detections.json', f'tp_diag2_detections_{DIFF_VAL}.json')\n",
    "    # fp_detection_path = detection_path.replace('ei_detections.json', 'fp_ei_detections.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(tp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {tp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        raise ValueError('Error in saving detections')\n",
    "\n",
    "\n",
    "for test_data, detections, test_label in all_fp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'diag2_detections')\n",
    "    # tp_detection_path = detection_path.replace('ei_detections.json', 'tp_ei_detections.json')\n",
    "    fp_detection_path = detection_path.replace('diag2_detections.json', f'fp_diag2_detections_{DIFF_VAL}.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(fp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {fp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        raise ValueError('Error in saving detections')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
