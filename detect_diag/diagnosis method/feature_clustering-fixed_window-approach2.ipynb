{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Fixed Window - Approach 2\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, 'diag_refsamples')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    train_data_path = ref_var_samples_path\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "print('train_data:\\n', train_data_path)\n",
    "print(len(train_data_path))\n",
    "print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "1. save ref_samples with window of 10\n",
    "2. On the detections, slide with window of 10 and sliding interval of 10\n",
    "3. if the pattern exists in the ref_samples, then it is a normal pattern\n",
    "4. if the pattern does not exist in the ref_samples, then it is a faulty pattern (increment anomaly count)\n",
    " 4.1 if pattern does not exist find the most similar pattern in the ref_samples (take the diff and look for max zeros)\n",
    "5. with the matching ref sample for each windwow, form a new seq by joinig them one after the other\n",
    "6. This new seq is the new unique ref_sample for that detection, take the difference and get the feature vector\n",
    "7. split the detection into number of instances detected, seperated by zeros.\n",
    "\n",
    "Feature extraction and Clustering:\n",
    "- use the seperated instances to extract features\n",
    "- cluster the features (start with kmeans)\n",
    "- try the same feature extractors as Approach 1\n",
    "    - TSFEL\n",
    "    - SegLeran\n",
    "    - CNN+LSTM\n",
    "    - Autoencoder\n",
    "    - our method\n",
    "\n",
    " \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync the Detection and Ref_Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "    \n",
    "### load all the reference samples (fixed window size)\n",
    "ref_samples = []\n",
    "for ref_sample in train_data_path:\n",
    "    ref_samples.append(read_traces(ref_sample))\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "### load the test samples and compare with the reference samples\n",
    "test_feature_vectors = []  ### [(test_data, (feat1_vector, feat2_vector)), (), (), ...]\n",
    "missing_features = []   ### [(test_data, missing_feature), (), (), ...]\n",
    "for test_data in test_data_path[0:]:\n",
    "    print('test_data:', test_data)\n",
    "    ### read the subseq\n",
    "    test_trace = read_traces(test_data)\n",
    "    print('test_trace:', test_trace)\n",
    "    test_data_len = len(test_trace)\n",
    "    print('test_data_len:', test_data_len)\n",
    "\n",
    "    if test_data_len > 500:\n",
    "        # print('test data length is more than 500, skipping...')\n",
    "        # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "        # continue\n",
    "\n",
    "        print('test data length is more than 500, truncating...')\n",
    "        test_trace = test_trace[:500]\n",
    "        test_data_len = 500\n",
    "    \n",
    "    ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "    test_events = []\n",
    "    test_intervals = []\n",
    "    prev_time = test_trace[0][1]\n",
    "    time_diff = 0\n",
    "    for x in test_trace:\n",
    "        time_diff = x[1] - prev_time\n",
    "        test_intervals.append(time_diff)\n",
    "        prev_time = x[1]\n",
    "        test_events.append(x[0])\n",
    "\n",
    "    assert len(test_events) == len(test_intervals) == test_data_len\n",
    "\n",
    "    ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "    startsync_ref_samples = []\n",
    "    for ref_sample in ref_samples:\n",
    "        # print('ref_sample:', ref_sample[0][:5])\n",
    "        if ref_sample[0][:5] == test_events[:5]:\n",
    "            ref_sample = (ref_sample[0][:test_data_len], ref_sample[1][:test_data_len])\n",
    "            startsync_ref_samples.append(ref_sample)\n",
    "        \n",
    "    print('startsync_ref_samples:', len(startsync_ref_samples))\n",
    "    print('test_events', test_events)\n",
    "\n",
    "\n",
    "\n",
    "    ### deduplicate reference samples\n",
    "    # print('shortlisted_ref_samples:', len(shortlisted_ref_samples))\n",
    "    # dedup_ref_samples = []\n",
    "    # _dedup_events = []\n",
    "    # for ref_sample in shortlisted_ref_samples:\n",
    "    #     # print('ref_sample:', ref_sample[0])\n",
    "    #     if ref_sample[0] not in _dedup_events:\n",
    "    #         dedup_ref_samples.append(ref_sample)\n",
    "    #         _dedup_events.append(ref_sample[0])\n",
    "    # # print('dedup_ref_samples:', len(dedup_ref_samples))\n",
    "    # shortlisted_ref_samples = dedup_ref_samples\n",
    "                \n",
    "\n",
    "    # print('dedup_ref_samples:', len(shortlisted_ref_samples))\n",
    "    # print('ref_samples:', len(ref_samples))\n",
    "\n",
    "    # ### generate feature vector for the test_trace with respect to each of the shortlisted_ref_samples\n",
    "    # '''\n",
    "    # Feature generation:\n",
    "    # - take difference of the events and intervals of the test_trace with the shortlisted_ref_samples\n",
    "    # '''\n",
    "    # # print('ref samples with matching first 5 events:', np.array(shortlisted_ref_samples).shape)\n",
    "    # if shortlisted_ref_samples != []:\n",
    "    #     shortlisted_features = []\n",
    "    #     feature_vectors = []\n",
    "    #     for ref_sample in shortlisted_ref_samples:\n",
    "    #         # print('ref_sample:', ref_sample[1])\n",
    "    #         sel_ref_event = ref_sample[0][:test_data_len]\n",
    "    #         sel_ref_interval = ref_sample[1][:test_data_len]\n",
    "    #         # print('sel_ref_event:', len(sel_ref_event), len(sel_ref_interval))\n",
    "    #         assert (len(sel_ref_event) == len(sel_ref_interval) == test_data_len)\n",
    "\n",
    "    #         ### generate feature vector\n",
    "    #         feat1_vector = []\n",
    "    #         feat2_vector = []\n",
    "    #         for i in range(test_data_len):\n",
    "    #             feat1 = test_events[i] - sel_ref_event[i]\n",
    "    #             feat2 = test_intervals[i] - sel_ref_interval[i]\n",
    "    #             ### if the difference in interval is within 500 ms, then consider it as same, as we consider tolerance of 500 ms based on observation\n",
    "    #             feat2 = [0 if feat2 >= -500 and feat2 <= 500 else feat2 ][0] \n",
    "    #             feat1_vector.append(feat1)\n",
    "    #             feat2_vector.append(feat2)\n",
    "\n",
    "    #         feat1_vector = np.array(feat1_vector)\n",
    "    #         feat2_vector = np.array(feat2_vector)\n",
    "    #         shortlisted_features.append((feat1_vector, feat2_vector))\n",
    "\n",
    "        \n",
    "    #     ### count leading zeros in the feature vector\n",
    "    #     # print('shortlisted_features:', len(shortlisted_features))\n",
    "    #     zero_count = []\n",
    "    #     for sf in shortlisted_features:\n",
    "    #         count = 0\n",
    "    #         # print(sf[0], sf[1])\n",
    "    #         for esf, isf in zip(sf[0], sf[1]):\n",
    "    #             ### check if events and intervals are same\n",
    "    #             if esf == 0 and isf == 0:\n",
    "    #                 count += 1\n",
    "    #             else:\n",
    "    #                 break   ### part of the logic, do not remove\n",
    "\n",
    "    #         # print('zero_count:', count)\n",
    "    #         zero_count.append(count)\n",
    "\n",
    "    #     ### select the feature vector with maximum leading zeros\n",
    "    #     max_zero_count = max(zero_count)\n",
    "    #     zero_count = np.array(zero_count)\n",
    "    #     max_zero_count_ind = np.where(zero_count==max_zero_count)[0]\n",
    "    #     # print('max number of starting events that are same for ref and test:', max_zero_count)\n",
    "    #     # print('ref samples with highest matching events in the start:', len(max_zero_count_ind))\n",
    "\n",
    "    #     ### select the feature vectors with maximum leading zeros\n",
    "    #     feature_vectors = [ shortlisted_features[i] for i in max_zero_count_ind ]\n",
    "\n",
    "    #     total_zero_count = []\n",
    "    #     for features in feature_vectors:\n",
    "    #         # print('feature:', features)\n",
    "    #         # print('zero_count:', np.where(features[0]==0)[0].shape)\n",
    "    #         total_zero_count.append(np.where(features[0]==0)[0].shape[0])\n",
    "    #     # print('total_zero_count:', total_zero_count)\n",
    "    #     total_zero_count = np.array(total_zero_count)\n",
    "    #     min_total_zero_count = min(total_zero_count)\n",
    "    #     min_total_zero_count_ind = np.where(total_zero_count==min_total_zero_count)[0]\n",
    "    #     # print('the number of highest number of total zeros:', min_total_zero_count)\n",
    "    #     print('files that has max number of total zeros:', min_total_zero_count_ind)\n",
    "    #     feature_vector = [ feature_vectors[i] for i in min_total_zero_count_ind ]\n",
    "    #     # print('feature_vector:', len(feature_vector))\n",
    "\n",
    "    #     ### select the first feature vector if multiple shortlisted feature vectors are there\n",
    "    #     # print(np.array(feature_vector).shape)\n",
    "    #     if np.array(feature_vector).shape[0] > 1:\n",
    "    #         print('multiple feature vectors found, selecting the first one')\n",
    "\n",
    "    #         feature_vector = [feature_vectors[0]]\n",
    "\n",
    "    #     test_feature_vectors.append((test_data, feature_vector))\n",
    "    # else:\n",
    "    #     print('No shortlisted ref samples found for the test data:', test_data)\n",
    "    #     missing_features.append((test_data, 'No shortlisted ref samples found'))\n",
    "\n",
    "    print('')\n",
    "    break\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_axis = np.arange(0, len(test_trace), 1)\n",
    "\n",
    "\n",
    "\n",
    "### prepare test_trace for plotting\n",
    "plot_data = dict()\n",
    "plot_data['subseq'] = test_events   ### y_data (traces)\n",
    "\n",
    "for i, fv in enumerate(shortlisted_ref_samples):\n",
    "    plot_data[f'feat1_{i}'] = fv[0]\n",
    "    \n",
    "df_feat1 = pd.DataFrame(plot_data)\n",
    "\n",
    "# plot_data = dict()\n",
    "# plot_data['intervals'] = test_intervals   ### y_data (traces)\n",
    "\n",
    "# for i, fv in enumerate(feature_vectors):\n",
    "#     plot_data[f'feat2_{i}'] = fv[1]\n",
    "\n",
    "# df_feat2 = pd.DataFrame(plot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_feat1, title='features')\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line(df_feat2, title='features')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
