{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Fixed Window - Approach 2\n",
    "- Precisely crop the anomaly from the detections by syncing the subtrace before and after the anomaly w.r.t ref_samples\n",
    "- to keep the lenght of feature vector same, we pad the features with trailing zeros to get length of 500 (max length of detection)\n",
    "- The feature extraction is the dependent on the corresponding normal behaviour subtrace\n",
    "- We tested this approach across all applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import plotly.express as px\n",
    "from statistics import mode\n",
    "\n",
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'mamba2'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "WINDOW = 500                 ### window size for subsequence\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True, False; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "ref_samples_basepath = os.path.join(normalbase_path, f'diag_refsamples{WINDOW}')\n",
    "ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "if IS_VAR_WINDOW:\n",
    "    train_data_path = ref_var_samples_path\n",
    "else:\n",
    "    train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "print('train_data:\\n', train_data_path)\n",
    "print(len(train_data_path))\n",
    "print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "\n",
    "0. Take detection trace as the input\n",
    "1. Identify the start of the detection that is correct: part that matches with the ref_samples\n",
    "2. Skip the part that is correct and halt at the first incorrect event (anomaly)\n",
    "3. this indicares the start of first anomaly, thus add this and the next consecutive point to a new blank list (anomaly_instance), and halt at the next point\n",
    "4. Identify if there is correct part of the trace after that point by comparing with the ref_samples\n",
    "    a. if there is no matching ref_sample, then shift to the next point an add it to the list (anomaly_instance). Repeat this until the end of the trace\n",
    "    b. if there is matching ref_sample, skip the matching part and halt at the first incorrect event (anomaly). Add this point a new blank list (next anomaly_instance). Repeat it until the end of the trace\n",
    "5. collection of all the anomy_instance will give the instances of the anomaly detected\n",
    "\n",
    "\n",
    "\n",
    "Feature extraction and Clustering:\n",
    "- use the seperated instances to extract features\n",
    "- cluster the features (start with kmeans)\n",
    "- try the same feature extractors as Approach 1\n",
    "    - TSFEL\n",
    "    - SegLeran\n",
    "    - CNN+LSTM\n",
    "    - Autoencoder\n",
    "    - our method\n",
    "\n",
    " \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Seperate if multiple instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_detection_labels(test_labels_path, test_data_path):\n",
    "\n",
    "#     test_class = {}\n",
    "#     ### load the labels\n",
    "#     test_class_labels = read_json(test_labels_path[0])\n",
    "#     print('test_class_labels:', len(test_class_labels))\n",
    "#     print('test_class_labels:', test_class_labels)\n",
    "\n",
    "#     ### prepare the feature vectors for classification\n",
    "#     for test_data in test_data_path:\n",
    "#         file_name = test_data.split('/')[-1].split('.')[0]\n",
    "#         # print(CODE, test_data)\n",
    "#         class_list = test_class_labels[file_name]\n",
    "#         test_class[test_data] = class_list\n",
    "        \n",
    "#     return test_class\n",
    "\n",
    "def split_instances(ref_samples, test_events, test_intervals, test_timestamps):\n",
    "    '''\n",
    "    check which part of detections are in sync with the corresponding ref_samples\n",
    "    first check was max matching events at the start, use this to select correspoinding ref_sample\n",
    "    for the selected ref_sample, if there are parts of the trace that are in sync with detection use them to split the trace in mutiple instance\n",
    "\n",
    "    ref_samples: list of reference samples of length 50\n",
    "    '''\n",
    "    WINDOW = 50\n",
    "    SLIDING_WINDOW = 20\n",
    "\n",
    "    assert np.array(ref_samples).shape[2] == 50, 'ref_samples should be of length 50'\n",
    "\n",
    "    test_data_len = len(test_events)\n",
    "    # print('test_events:', test_events)\n",
    "    ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "    selected_ref_events = []\n",
    "    selected_ref_intervals = []\n",
    "\n",
    "    feature_vector_event = np.zeros((test_data_len,))\n",
    "    feature_vector_interval = np.zeros((test_data_len,))\n",
    "    feature_vector_timestamps = np.zeros((test_data_len,))\n",
    "    # print(feature_vector_event.shape)\n",
    "    \n",
    "    if test_data_len < SLIDING_WINDOW:\n",
    "        _test_events = test_events\n",
    "        _test_intervals = test_intervals\n",
    "        _test_timestamps = test_timestamps\n",
    "\n",
    "        _test_len = len(_test_events)\n",
    "        print('test_events:', len(_test_events))\n",
    "        print('COPY the logic from the next block')\n",
    "        # for ref_sample in ref_samples:\n",
    "        #     # print(ref_sample)\n",
    "        #     _ref_events = ref_sample[0]\n",
    "        #     _ref_intervals = ref_sample[1]\n",
    "\n",
    "        #     ###'ref_sample should be of len 50'\n",
    "        #     assert(len(_ref_events) == WINDOW)\n",
    "\n",
    "        #     _ref_events = _ref_events[:_test_len]\n",
    "        #     _ref_intervals = _ref_intervals[:_test_len]\n",
    "\n",
    "        #     print('ref_event:', len(_ref_events))\n",
    "\n",
    "            # break\n",
    "    else:\n",
    "        for i in range(0, test_data_len, SLIDING_WINDOW):\n",
    "            window_start = i\n",
    "            window_end = i+WINDOW\n",
    "            print('window:', window_start, window_end)\n",
    "            _test_events = test_events[window_start:window_end]\n",
    "            _test_intervals = test_intervals[window_start:window_end]\n",
    "            _test_timestamps = test_timestamps[window_start:window_end]\n",
    "\n",
    "            _test_len = len(_test_events)\n",
    "            print('test_events:', len(_test_events))\n",
    "\n",
    "            shortlisted_ref_events = []\n",
    "            shortlisted_ref_intervals = []\n",
    "            zero_count = []\n",
    "            sample_selected = False\n",
    "            for ref_sample in ref_samples:\n",
    "                # print(ref_sample)\n",
    "                _ref_events = ref_sample[0]\n",
    "                _ref_intervals = ref_sample[1]\n",
    "\n",
    "                ###'ref_sample should be of len 50'\n",
    "                # assert(len(_ref_events) == WINDOW)\n",
    "\n",
    "                _ref_events = _ref_events[:_test_len]\n",
    "                _ref_intervals = _ref_intervals[:_test_len]\n",
    "\n",
    "                # print('ref_event:', len(_ref_events))\n",
    "                if _ref_events[:2] == _test_events[:2]:\n",
    "                    diff_events = np.array(_ref_events) - np.array(_test_events)\n",
    "                    diff_intervals = np.abs(np.array(_ref_intervals) - np.array(_test_intervals))\n",
    "                    # print('diff_events:', diff_events)\n",
    "\n",
    "                    if all(diff_events == 0):\n",
    "                        print('All events are same')\n",
    "                        selected_ref_events.append(_ref_events)\n",
    "                        selected_ref_intervals.append(_ref_intervals)\n",
    "                        feature_vector_event[window_start:window_end] = diff_events\n",
    "                        feature_vector_interval[window_start:window_end] = diff_intervals\n",
    "                        feature_vector_timestamps[window_start:window_end] = _test_timestamps\n",
    "                        sample_selected = True\n",
    "                        break   ### part of the logic, do not remove\n",
    "                    else:\n",
    "                        count = 0\n",
    "                        # print(sf[0], sf[1])\n",
    "                        for esf, esi in zip(diff_events, diff_intervals):\n",
    "                            ### check if events and intervals are same\n",
    "                            # if esf == 0 and esi < 5:\n",
    "                            if esf == 0:\n",
    "                                count += 1\n",
    "                            else:\n",
    "                                break   ### part of the logic, do not remove\n",
    "\n",
    "                        if _ref_events not in shortlisted_ref_events:\n",
    "                            zero_count.append(count)\n",
    "                            shortlisted_ref_events.append(_ref_events)\n",
    "                            shortlisted_ref_intervals.append(_ref_intervals)\n",
    "\n",
    "            if not sample_selected:\n",
    "                if len(zero_count) != 0:\n",
    "                    max_zero_count = max(zero_count)\n",
    "                    zero_count = np.array(zero_count)\n",
    "                    max_zero_count_ind = np.where(zero_count==max_zero_count)[0][0]\n",
    "                    print('zero_count:', zero_count)\n",
    "                    print('max_zero_count:', max_zero_count)\n",
    "\n",
    "                    # print('max_zero_count_ind:', max_zero_count_ind)\n",
    "                    _ref_events = shortlisted_ref_events[max_zero_count_ind]\n",
    "                    _ref_intervals = shortlisted_ref_intervals[max_zero_count_ind]\n",
    "                    selected_ref_events.append(_ref_events)\n",
    "                    selected_ref_intervals.append(_ref_intervals)\n",
    "\n",
    "                    diff_events = np.array(_ref_events) - np.array(_test_events)\n",
    "                    diff_intervals = np.abs(np.array(_ref_intervals) - np.array(_test_intervals))\n",
    "                    # print('selected_ref_events:', selected_ref_events[:max_zero_count+1])\n",
    "                    feature_vector_event[window_start:window_end] = diff_events\n",
    "                    feature_vector_interval[window_start:window_end] = diff_intervals\n",
    "                    feature_vector_timestamps[window_start:window_end] = _test_timestamps\n",
    "                else:\n",
    "                    ### no matching ref_sample found\n",
    "                    pass ### do nothing, part of the logic\n",
    "\n",
    "            # break\n",
    "        \n",
    "    return feature_vector_event, feature_vector_interval, feature_vector_timestamps\n",
    "\n",
    "\n",
    "\n",
    "def strip_correct_part(ref_samples, test_events, test_intervals, test_timestamps):\n",
    "    '''\n",
    "    check if any matching event trace in present based on first 2 points\n",
    "    if yes, then check the number of matching events and intervals, remove the matching part in the event trace and return the remaining part\n",
    "    if no, then return the same event trace\n",
    "    '''\n",
    "\n",
    "    test_data_len = len(test_events)\n",
    "    # print('test_events:', test_events)\n",
    "    ### shortlist the reference samples which has first 5 elements same as the test_trace\n",
    "    shortlisted_ref_events = []\n",
    "    shortlisted_ref_intervals = []\n",
    "    zero_count = []\n",
    "    for ref_sample in ref_samples:\n",
    "        # print('ref_sample:', ref_sample[0][:5])\n",
    "        # event_diff = np.array(ref_sample[0][0:len(test_events)]) - np.array(test_events)\n",
    "        # print('event_diff:', event_diff)\n",
    "        # print('event_diff:', len(event_diff))\n",
    "        # print('zeros:', np.where(event_diff==0)[0].shape)\n",
    "        # if len(test_events) == 276:\n",
    "        #     print('ref_sample:', ref_sample[0][:5])\n",
    "        #     print('test_events:', test_events[:5])\n",
    "        if ref_sample[0][:2] == test_events[:2]:\n",
    "            ref_events = ref_sample[0][:test_data_len]\n",
    "            ref_intervals = ref_sample[1][:test_data_len]\n",
    "            diff_events = np.array(ref_events) - np.array(test_events)\n",
    "            # print('len:', len(ref_intervals), len(test_intervals))\n",
    "            diff_intervals = np.abs(np.array(ref_intervals) - np.array(test_intervals))\n",
    "            if len(test_events) == 276:\n",
    "                print('diff_events:', diff_events)\n",
    "                print('diff_intervals:', diff_intervals)\n",
    "            count = 0\n",
    "            # print(sf[0], sf[1])\n",
    "            for esf, esi in zip(diff_events, diff_intervals):\n",
    "                ### check if events and intervals are same\n",
    "                # if esf == 0 and esi < 5:\n",
    "                if esf == 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break   ### part of the logic, do not remove\n",
    "\n",
    "            # print('zero_count:', count)\n",
    "            ### depulicate the ref samples\n",
    "            if ref_events not in shortlisted_ref_events:\n",
    "                zero_count.append(count)\n",
    "                shortlisted_ref_events.append(ref_events)\n",
    "                shortlisted_ref_intervals.append(ref_intervals)\n",
    "            # print('count:', count)  \n",
    "\n",
    "        # break\n",
    "\n",
    "    print('zero_count:', zero_count)\n",
    "    print('shortlisted_ref_samples:', len(shortlisted_ref_events))\n",
    "\n",
    "    ### select the ref_sample_events with maximum leading zeros\n",
    "    if len(zero_count) != 0:\n",
    "        max_zero_count = max(zero_count)\n",
    "        zero_count = np.array(zero_count)\n",
    "        max_zero_count_ind = np.where(zero_count==max_zero_count)[0][0]\n",
    "        # print('max_zero_count_ind:', max_zero_count_ind)\n",
    "        selected_ref_events = shortlisted_ref_events[max_zero_count_ind]\n",
    "        selected_ref_intervals = shortlisted_ref_intervals[max_zero_count_ind]\n",
    "        # print('selected_ref_events:', selected_ref_events[:max_zero_count+1])\n",
    "    else:\n",
    "        max_zero_count = 0\n",
    "        selected_ref_events = None\n",
    "        selected_ref_intervals = None\n",
    "\n",
    "    if max_zero_count == 0:\n",
    "        print('No match found')\n",
    "        return test_events, test_intervals, test_timestamps, None, None, None\n",
    "    else:\n",
    "        ### select the point where the last match happened\n",
    "        last_matched_point = max_zero_count-1\n",
    "        # print('last_matched_point:', last_matched_point)\n",
    "        # print('test_events:', test_events[:last_matched_point])\n",
    "        striped_test_events = test_events[last_matched_point:]\n",
    "        striped_test_intervals = test_intervals[last_matched_point:]\n",
    "        striped_test_timestamps = test_timestamps[last_matched_point:]\n",
    "        # print('striped_test_events:', striped_test_events)\n",
    "        print('max count:', max_zero_count, len(test_events), len(striped_test_events))\n",
    "\n",
    "        \n",
    "        if max_zero_count == len(test_events):\n",
    "            print('All events are same')\n",
    "            return [], None, None, None, None, None\n",
    "        else:\n",
    "            return striped_test_events, striped_test_intervals, striped_test_timestamps, max_zero_count, selected_ref_events, selected_ref_intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "- optimize the method to solve the problem of detection of communication anomaly for theft protection\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##########################################################\n",
    "    \n",
    "### load all the reference samples (fixed window size)\n",
    "ref_samples = []\n",
    "for ref_sample in train_data_path:\n",
    "    ref_samples.append(read_traces(ref_sample))\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################\n",
    "\n",
    "### load the test samples and compare with the reference samples\n",
    "anomaly_instances = []\n",
    "anomaly_timestamps = []\n",
    "for test_data in test_data_path[0:]:\n",
    "    print('test_data:', test_data)\n",
    "    ### read the subseq\n",
    "    test_trace = read_traces(test_data)\n",
    "    print('test_trace:', test_trace)\n",
    "    test_data_len = len(test_trace)\n",
    "    # print('test_data_len:', test_data_len)\n",
    "\n",
    "    if test_data_len > 500:\n",
    "        # print('test data length is more than 500, skipping...')\n",
    "        # missing_features.append((test_data, 'test data length is more than 500'))\n",
    "        # continue\n",
    "\n",
    "        print('test data length is more than 500, truncating...')\n",
    "        test_trace = test_trace[:500]\n",
    "        test_data_len = 500\n",
    "    \n",
    "    ### transform the test trace from [(var,ts1), (var,ts2), (var, ts3)] to [[var1, var2, var3], [ts1, ts2, ts3]]\n",
    "    ### old implementation with 0 at start of intervals\n",
    "    # test_events = []\n",
    "    # test_intervals = []\n",
    "    # test_timestamps = []\n",
    "    # prev_time = test_trace[0][1]\n",
    "    # time_diff = 0\n",
    "    # for x in test_trace:\n",
    "    #     time_diff = x[1] - prev_time\n",
    "    #     test_intervals.append(time_diff)\n",
    "    #     prev_time = x[1]\n",
    "    #     test_events.append(x[0])\n",
    "    #     test_timestamps.append(x[1])\n",
    "\n",
    "    ### new implementation without 0 at start of intervals\n",
    "    test_events = []\n",
    "    test_intervals = []\n",
    "    test_timestamps = []\n",
    "    for x,y in zip(test_trace[:-1], test_trace[1:]):\n",
    "        test_events.append(x[0])   ### first event\n",
    "        test_intervals.append(y[1] - x[1])   ### difference between the timestamps of second and first event\n",
    "        test_timestamps.append(x[1])   ### first timestamp\n",
    "    # print('test_events:', len(test_events))\n",
    "    # print('test_intervals:', test_intervals)\n",
    "\n",
    "    assert len(test_events)+1 == len(test_intervals)+1 == test_data_len\n",
    "\n",
    "    # ### get detection class\n",
    "\n",
    "    # test_class_labels = read_json(test_labels_path[0])\n",
    "\n",
    "    ### store the first to consecutive points as the first anomaly instance\n",
    "    an_instance = []\n",
    "    an_timestamps = []\n",
    "    all_instances = []\n",
    "    all_timestamps = []\n",
    "    all_ref_events = []\n",
    "    all_ref_intervals = []\n",
    "    all_striped_test_events = []\n",
    "    all_striped_test_intervals = []\n",
    "    all_striped_timestamps = []\n",
    "\n",
    "    striped_test_events = test_events\n",
    "    striped_test_intervals = test_intervals\n",
    "    striped_timestamps = test_timestamps\n",
    "    print('striped_test_events:', striped_test_events, len(striped_test_events))\n",
    "    # print('striped_test_intervals:', striped_test_intervals, len(striped_test_intervals))\n",
    "    i = 0\n",
    "    first_loop = True\n",
    "    while len(striped_test_events) > 0:\n",
    "    # while i < 6:\n",
    "        ### first collect the trace that is given as input\n",
    "        all_striped_test_events.append(striped_test_events)\n",
    "        all_striped_test_intervals.append(striped_test_intervals)\n",
    "        all_striped_timestamps.append(striped_timestamps)\n",
    "        ### remove the initial correct part of the trace\n",
    "        striped_test_events, striped_test_intervals, striped_timestamps, max_zero_count, selected_ref_events, selected_ref_intervals = strip_correct_part(ref_samples, striped_test_events, striped_test_intervals, striped_timestamps)\n",
    "        print('striped_test_events 1:', striped_test_events, len(striped_test_events))\n",
    "        ### get the ref trace that matched with the test trace\n",
    "        all_ref_events.append(selected_ref_events)\n",
    "        all_ref_intervals.append(selected_ref_intervals)\n",
    "\n",
    "        # break\n",
    "        ### store the first anomaly instance (first two consecutive points)\n",
    "        if max_zero_count != None:\n",
    "            if len(an_instance) != 0:\n",
    "                ### new anomaly instance detected\n",
    "                all_instances.append(an_instance)\n",
    "                all_timestamps.append(an_timestamps)\n",
    "                an_instance = []\n",
    "                an_timestamps = []\n",
    "                first_loop = True\n",
    "            # ### start, where first anomaly instance is detected\n",
    "            # an_instance.extend(striped_test_events[:2])\n",
    "            # an_timestamps.extend(striped_timestamps[:2])\n",
    "            # striped_test_events = striped_test_events[1:]\n",
    "            # striped_test_intervals = striped_test_intervals[1:]\n",
    "            # striped_timestamps = striped_timestamps[1:]\n",
    "            print('striped_test_events 2:', striped_test_events, len(striped_test_events))\n",
    "        else:\n",
    "            if len(striped_test_events) > 0:\n",
    "                ### normal functionality loop\n",
    "                if first_loop:\n",
    "                    an_instance.extend(striped_test_events[:2])\n",
    "                    an_timestamps.extend(striped_timestamps[:2])\n",
    "                    striped_test_events = striped_test_events[1:]\n",
    "                    striped_test_intervals = striped_test_intervals[1:]\n",
    "                    striped_timestamps = striped_timestamps[1:]\n",
    "                    first_loop = False\n",
    "                    print('striped_test_events 3.1:', striped_test_events, len(striped_test_events))\n",
    "                else:\n",
    "                    an_instance.extend(striped_test_events[1:2])\n",
    "                    an_timestamps.extend(striped_timestamps[1:2])\n",
    "                    striped_test_events = striped_test_events[1:]\n",
    "                    striped_test_intervals = striped_test_intervals[1:]\n",
    "                    striped_timestamps = striped_timestamps[1:]\n",
    "                    print('striped_test_events 3.2:', striped_test_events, len(striped_test_events))\n",
    "            else:\n",
    "                ### for last iteration, when len(striped_test_events) == 1   \n",
    "                if len(an_instance) != 0:\n",
    "                    all_instances.append(an_instance)\n",
    "                    all_timestamps.append(an_timestamps)\n",
    "                anomaly_instances.append(all_instances)\n",
    "                anomaly_timestamps.append(all_timestamps)\n",
    "                if len(striped_test_events) == 0:\n",
    "                    break\n",
    "        # print('an_instance:', an_instance)\n",
    "        # print('')    \n",
    "        i += 1\n",
    "print('anomaly_instances:', anomaly_instances) \n",
    "print('anomaly_timestamps:', anomaly_timestamps)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in zip(feature_event, test_events, feature_time_stamps):\n",
    "    print(i, '', j, '', k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### backup logic that works\n",
    "\n",
    "# ### store the first anomaly instance (first two consecutive points)\n",
    "        # if max_zero_count != None:\n",
    "        #     if len(an_instance) != 0:\n",
    "        #         ### new anomaly instance detected\n",
    "        #         all_instances.append(an_instance)\n",
    "        #         all_timestamps.append(an_timestamps)\n",
    "        #         an_instance = []\n",
    "        #         an_timestamps = []\n",
    "        #     ### start, where first anomaly instance is detected\n",
    "        #     an_instance.extend(striped_test_events[:2])\n",
    "        #     an_timestamps.extend(striped_timestamps[:2])\n",
    "        #     striped_test_events = striped_test_events[1:]\n",
    "        #     striped_test_intervals = striped_test_intervals[1:]\n",
    "        #     striped_timestamps = striped_timestamps[1:]\n",
    "        #     print('striped_test_events 2:', striped_test_events, len(striped_test_events))\n",
    "        # else:\n",
    "        #     if len(striped_test_events) > 0:\n",
    "        #         ### normal functionality loop\n",
    "        #         an_instance.extend(striped_test_events[1:2])\n",
    "        #         an_timestamps.extend(striped_timestamps[1:2])\n",
    "        #         striped_test_events = striped_test_events[1:]\n",
    "        #         striped_test_intervals = striped_test_intervals[1:]\n",
    "        #         striped_timestamps = striped_timestamps[1:]\n",
    "        #         print('striped_test_events 3:', striped_test_events, len(striped_test_events))\n",
    "        #     else:\n",
    "        #         ### for last iteration, when len(striped_test_events) == 1   \n",
    "        #         all_instances.append(an_instance)\n",
    "        #         all_timestamps.append(an_timestamps)\n",
    "        #         anomaly_instances.append(all_instances)\n",
    "        #         anomaly_timestamps.append(all_timestamps)\n",
    "        #         if len(striped_test_events) == 0:\n",
    "        #             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance Evaluation for anomaly seperation algorithm\n",
    "\n",
    "### load the labels\n",
    "test_class_labels = read_json(test_labels_path[0])\n",
    "# print('test_class_labels:', len(test_class_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_labels = read_json(test_labels_path[0])\n",
    "print(test_class_labels)\n",
    "\n",
    "for l, i, k in zip(test_data_path, anomaly_instances, anomaly_timestamps):\n",
    "    # print(l)\n",
    "    file_name = l.split('/')[-1].split('.')[0]\n",
    "    print(file_name)\n",
    "\n",
    "    class_labels = test_class_labels[file_name]\n",
    "\n",
    "    print('length of labels and predictions:', len(class_labels), len(i))\n",
    "    print(class_labels)\n",
    "    print(i)\n",
    "    print(k)\n",
    "\n",
    "    print('')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_axis = np.arange(0, len(test_trace), 1)\n",
    "\n",
    "\n",
    "\n",
    "# ### prepare test_trace for plotting\n",
    "# plot_data = dict()\n",
    "# plot_data['subseq'] = test_events   ### y_data (traces)\n",
    "\n",
    "# # for i, fv in enumerate(shortlisted_ref_samples):\n",
    "# #     plot_data[f'feat1_{i}'] = fv[0]\n",
    "# plot_data['ref_samples'] = selected_ref_events\n",
    "    \n",
    "# df_feat1 = pd.DataFrame(plot_data)\n",
    "\n",
    "# plot_data = dict()\n",
    "# plot_data['intervals'] = test_intervals   ### y_data (traces)\n",
    "\n",
    "# # for i, fv in enumerate(feature_vectors):\n",
    "# #     plot_data[f'feat2_{i}'] = fv[1]\n",
    "# plot_data['ref_intervals'] = selected_ref_intervals\n",
    "\n",
    "# df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "# fig = px.line(df_feat1, title='features')\n",
    "# fig.show()\n",
    "\n",
    "# fig = px.line(df_feat2, title='features')\n",
    "# fig.show()\n",
    "\n",
    "for i in range(len(all_striped_test_events)):\n",
    "    plot_data = dict()\n",
    "    plot_data['subseq'] = all_striped_test_events[i]   ### y_data (traces)\n",
    "    print(all_ref_events[i])\n",
    "    if all_ref_events[i] != None:\n",
    "        plot_data['ref_samples'] = all_ref_events[i]\n",
    "            \n",
    "        df_feat1 = pd.DataFrame(plot_data)\n",
    "        # print('df_feat1:', df_feat1)\n",
    "        \n",
    "        plot_data = dict()\n",
    "        plot_data['intervals'] = all_striped_test_intervals[i]   ### y_data (traces)\n",
    "        plot_data['ref_intervals'] = all_ref_intervals[i]\n",
    "\n",
    "        df_feat2 = pd.DataFrame(plot_data)\n",
    "\n",
    "        fig = px.line(df_feat1, title='features')\n",
    "        fig.show()\n",
    "\n",
    "        fig = px.line(df_feat2, title='features')\n",
    "        fig.show()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
