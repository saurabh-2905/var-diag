{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSFEDL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from libraries.utils import *\n",
    "\n",
    "import multiprocess\n",
    "from multiprocess import Pool, cpu_count\n",
    "from itertools import repeat\n",
    "import time\n",
    "\n",
    "\n",
    "pool = Pool(processes=(cpu_count() - 2))\n",
    "print(cpu_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ configuration - trace ################\n",
    "# ############################################\n",
    "\n",
    "\n",
    "CODE = 'mamba2'       ### application (code)       ###  'theft_protection', 'mamba2', 'lora_ducy'\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "\n",
    "################# configuration - diag ################\n",
    "IS_VAR_WINDOW = False             ### True: varibale window size, False: fixed window size; wether to use variable window size or not\n",
    "\n",
    "#####################################################\n",
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "ref_samples_basepath = os.path.join(normalbase_path, 'diag_refsamples')\n",
    "# ref_var_samples_basepath = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "diag_subseq_basepath = os.path.join(faultybase_path, 'diag_subseq')\n",
    "subseq_label_basepath = os.path.join(diag_subseq_basepath, 'subseq_labels')\n",
    "\n",
    "\n",
    "print('ref_samples_path:\\n', ref_samples_basepath)\n",
    "# print('ref_var_samples_path:\\n', ref_var_samples_basepath)\n",
    "print('diag_subseq_path:\\n', diag_subseq_basepath)\n",
    "\n",
    "######### get paths #######################\n",
    "# ref_samples_path = [os.path.join(ref_samples_basepath, x) for x in os.listdir(ref_samples_basepath)]\n",
    "# ref_var_samples_path = [os.path.join(ref_var_samples_basepath, x) for x in os.listdir(ref_var_samples_basepath)]   \n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "\n",
    "\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "test_subseq_path = [os.path.join(diag_subseq_basepath, x) for x in os.listdir(diag_subseq_basepath)]\n",
    "test_labels_path = [os.path.join(subseq_label_basepath, x) for x in os.listdir(subseq_label_basepath)]\n",
    "\n",
    "# ### remove.Ds_store from all lists\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "# ref_samples_path = [x for x in ref_samples_path if '.DS_Store' not in x]\n",
    "# ref_var_samples_path = [x for x in ref_var_samples_path if '.DS_Store' not in x]\n",
    "test_subseq_path = [x for x in test_subseq_path if '.DS_Store' not in x if '.json' in x]\n",
    "test_labels_path = [x for x in test_labels_path if '.DS_Store' not in x]\n",
    "\n",
    "\n",
    "varlist_path.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "# if IS_VAR_WINDOW:\n",
    "#     train_data_path = ref_var_samples_path\n",
    "# else:\n",
    "#     train_data_path = ref_samples_path\n",
    "\n",
    "test_data_path = test_subseq_path\n",
    "\n",
    "print('train_data:\\n', train_data_path)\n",
    "print(len(train_data_path))\n",
    "print('test_data:\\n', test_data_path)\n",
    "print(len(test_data_path))\n",
    "print('test_labels:\\n', test_labels_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3 or VER == 4:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]]+ varlist_path) ### compare with train varlist\n",
    "\n",
    "    if check_con != False:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        ### load normal varlist\n",
    "        print('loading normal varlist')\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_number = read_json(train_varlist_path[0])\n",
    "from_number = mapint2var(to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### key finder ####\n",
    "# from_number[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load all the reference samples (fixed window size)\n",
    "### the ref_samples: list of list of events, list of intervals for the subseq of size 500\n",
    "ref_samples = []\n",
    "for train_single in train_data_path:\n",
    "    ref_trace = read_traces(train_single)\n",
    "    events = []\n",
    "    intervals = []\n",
    "    for x,y in zip(ref_trace[:-1], ref_trace[1:]):\n",
    "        events.append(x[0])\n",
    "        intervals.append(y[1] - x[1])\n",
    "    ref_samples.append([events, intervals])\n",
    "\n",
    "    # print(ref_samples)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "# - deduplicate the samples\n",
    "\n",
    "### make subseq of 50 events, with sliding interval of 1\n",
    "WINDOW = 50\n",
    "\n",
    "def prepare_data(ref_trace, WINDOW):\n",
    "    ref_events = ref_trace[0]\n",
    "    ref_intervals = ref_trace[1]\n",
    "    # print(len(ref_events))\n",
    "    # print(len(ref_intervals))\n",
    "    ### we take one less event for training because we need last event as label\n",
    "\n",
    "    X_train_event = []\n",
    "    Y_train_event = []\n",
    "    for i in range(len(ref_events)-WINDOW): \n",
    "        # print(i)\n",
    "        # print(ref_events[i])\n",
    "        # sub_seq_events.append(ref_events[i:i+WINDOW])\n",
    "        # sub_seq_intervals.append(ref_intervals[i:i+WINDOW])\n",
    "\n",
    "        ### both events and intervals are taken as input\n",
    "        # _x_train = [ref_events[i:i+WINDOW], ref_intervals[i:i+WINDOW]]\n",
    "        # _x_train = np.array(_x_train)\n",
    "        # _x_train = np.transpose(_x_train)\n",
    "        # _y_train = np.array([ref_events[i+WINDOW], ref_intervals[i+WINDOW]])\n",
    "\n",
    "        ### only events are taken as input\n",
    "        _x_train_event = [ref_events[i:i+WINDOW]]\n",
    "        _y_train_event = [ref_events[i+WINDOW]]\n",
    "        # _y_train_event = [ref_events[i+WINDOW], ref_intervals[i+WINDOW]]\n",
    "\n",
    "        X_train_event.append(_x_train_event)\n",
    "        Y_train_event.append(_y_train_event)\n",
    "\n",
    "    return [X_train_event, Y_train_event]\n",
    "\n",
    "\n",
    "results = pool.starmap(prepare_data, zip(ref_samples[0:], repeat(WINDOW)))\n",
    "# print(results)\n",
    "\n",
    "X_train_event = []\n",
    "Y_train_event = []\n",
    "for result in results:\n",
    "    X_train_event.extend(result[0])\n",
    "    Y_train_event.extend(result[1])\n",
    "    # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### make subseq of 50 events, with sliding interval of 1\n",
    "# WINDOW = 50\n",
    "\n",
    "# sub_seq_events = []\n",
    "# sub_seq_intervals = []\n",
    "# X_train_event = []\n",
    "# Y_train_event = []\n",
    "# X_train_interval = []\n",
    "# Y_train_interval = []\n",
    "# for ref_trace in ref_samples[:100]:\n",
    "#     # print(ref_trace)\n",
    "#     ref_events = ref_trace[0]\n",
    "#     ref_intervals = ref_trace[1]\n",
    "#     # print(len(ref_events))\n",
    "#     # print(len(ref_intervals))\n",
    "#     ### we take one less event for training because we need last event as label\n",
    "\n",
    "\n",
    "#     for i in range(len(ref_events)-WINDOW): \n",
    "#         # print(i)\n",
    "#         # print(ref_events[i])\n",
    "#         # sub_seq_events.append(ref_events[i:i+WINDOW])\n",
    "#         # sub_seq_intervals.append(ref_intervals[i:i+WINDOW])\n",
    "\n",
    "#         ### both events and intervals are taken as input\n",
    "#         # _x_train = [ref_events[i:i+WINDOW], ref_intervals[i:i+WINDOW]]\n",
    "#         # _x_train = np.array(_x_train)\n",
    "#         # _x_train = np.transpose(_x_train)\n",
    "#         # _y_train = np.array([ref_events[i+WINDOW], ref_intervals[i+WINDOW]])\n",
    "\n",
    "#         ### only events are taken as input\n",
    "#         _x_train_event = [ref_events[i:i+WINDOW]]\n",
    "#         _y_train_event = [ref_events[i+WINDOW]]\n",
    "#         # _y_train_event = [ref_events[i+WINDOW], ref_intervals[i+WINDOW]]\n",
    "\n",
    "#         X_train_event.append(_x_train_event)\n",
    "#         Y_train_event.append(_y_train_event)\n",
    "\n",
    "\n",
    "#         # ### only intervals are taken as input\n",
    "#         # _x_train_interval = [ref_intervals[i:i+WINDOW]]\n",
    "#         # _y_train_interval = [ref_intervals[i+WINDOW]]\n",
    "#         # X_train_interval.append(_x_train_interval)\n",
    "#         # Y_train_interval.append(_y_train_interval)\n",
    "        \n",
    "\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(X_train_event).shape)\n",
    "# print(np.array(Y_train_event).shape)\n",
    "# print(np.array(X_train_interval).shape)\n",
    "# print(np.array(Y_train_interval).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess training data\n",
    "X_train_event = np.array(X_train_event)\n",
    "Y_train_event = np.array(Y_train_event)\n",
    "\n",
    "X_train_event = X_train_event.reshape(X_train_event.shape[0], X_train_event.shape[2], X_train_event.shape[1])\n",
    "Y_train_event = Y_train_event.reshape(Y_train_event.shape[0], Y_train_event.shape[1], 1)\n",
    "\n",
    "### shuffle the data\n",
    "X_train_event, Y_train_event = shuffle(X_train_event, Y_train_event, random_state=0)\n",
    "\n",
    "### split the data in train, validation and test sets\n",
    "X_train_event, x_test_event, Y_train_event, y_test_event = train_test_split(X_train_event, Y_train_event, test_size=0.2, random_state=0)\n",
    "# X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5, random_state=0)\n",
    "\n",
    "print(X_train_event.shape)\n",
    "print(Y_train_event.shape)\n",
    "\n",
    "print(x_test_event.shape)\n",
    "print(y_test_event.shape)\n",
    "\n",
    "# ### preprocess training data\n",
    "# X_train_interval = np.array(X_train_interval)\n",
    "# Y_train_interval = np.array(Y_train_interval)\n",
    "\n",
    "# X_train_interval = X_train_interval.reshape(X_train_interval.shape[0], X_train_interval.shape[2], X_train_interval.shape[1])\n",
    "# Y_train_interval = Y_train_interval.reshape(Y_train_interval.shape[0], Y_train_interval.shape[1], 1)\n",
    "\n",
    "# ### shuffle the data\n",
    "# X_train_interval, Y_train_interval = shuffle(X_train_interval, Y_train_interval, random_state=0)\n",
    "\n",
    "# ### split the data in train, validation and test sets\n",
    "# X_train_interval, x_test_interval, Y_train_interval, y_test_interval = train_test_split(X_train_interval, Y_train_interval, test_size=0.2, random_state=0)\n",
    "\n",
    "# print(X_train_interval.shape)\n",
    "# print(Y_train_interval.shape)\n",
    "\n",
    "# print(x_test_interval.shape)\n",
    "# print(y_test_interval.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_event.reshape(X_train_event.shape[0], -1).reshape(X_train_event.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "x_test_raw = x_test_event\n",
    "### check if scaler exists\n",
    "\n",
    "if os.path.exists(f\"./scalers/minmaxscaler_{CODE}_V{VER}.gz\"):\n",
    "    scaler = joblib.load(f\"./scalers/minmaxscaler_{CODE}_V{VER}.gz\")\n",
    "    print('scaler loaded')\n",
    "\n",
    "    # print(X_train_event[0])\n",
    "    X_train_event = scaler.transform(X_train_event.reshape(X_train_event.shape[0], -1)).reshape(X_train_event.shape)\n",
    "    x_test_event = scaler.transform(x_test_event.reshape(x_test_event.shape[0], -1)).reshape(x_test_event.shape)\n",
    "    # print(X_train_event[0])\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # print(X_train_event[0])\n",
    "    X_train_event = scaler.fit_transform(X_train_event.reshape(X_train_event.shape[0], -1)).reshape(X_train_event.shape)\n",
    "    x_test_event = scaler.transform(x_test_event.reshape(x_test_event.shape[0], -1)).reshape(x_test_event.shape)\n",
    "    # print(X_train_event[0])\n",
    "\n",
    "    # print(X_train_event.shape)\n",
    "    # print(x_test_event.shape)\n",
    "\n",
    "    scaler_filename = f\"./scalers/minmaxscaler_{CODE}_V{VER}.gz\"\n",
    "    joblib.dump(scaler, scaler_filename) \n",
    "    print('scaler saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_event[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_event[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train DL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import TSFEDL.models_keras as tsfedl\n",
    "\n",
    "FINE_TUNE = False\n",
    "# BACKBONE = 'forecaster'\n",
    "BACKBONE = 'autoencoder'\n",
    "# BACKBONE = 'rta'\n",
    "\n",
    "if not FINE_TUNE:\n",
    "    if BACKBONE == 'forecaster':\n",
    "        print('training new model: forecaster')\n",
    "        #### build model ####\n",
    "        input = tf.keras.Input(shape=(50,1))\n",
    "        model = tsfedl.HuangMeiLing(input_tensor=input, include_top=False)  ### include_top=False, to remove the top layer responsible for classification/forecast for the designed application\n",
    "        print(model)\n",
    "        x = model.output\n",
    "        x = tf.keras.layers.LSTM(units=20)(x)\n",
    "        ### Add the top module\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(50)(x)\n",
    "        # x = tf.keras.layers.Dense(10)(x)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "        out = tf.keras.layers.Reshape([1, 1])(x)\n",
    "\n",
    "        ### create new model\n",
    "        forecaster_event = tf.keras.Model(inputs=input, outputs=out, name=\"forecaster\")\n",
    "        # forecaster_interval = tf.keras.Model(inputs=input, outputs=out, name=\"forecaster\")\n",
    "\n",
    "        # print(model.summary())\n",
    "        # print(forecaster_event.name)\n",
    "        print(forecaster_event.summary())\n",
    "\n",
    "    elif BACKBONE == 'autoencoder':\n",
    "        print('training new model: autoencoder')\n",
    "        #### build model ####\n",
    "        input = tf.keras.Input(shape=(50,1))\n",
    "        model_list = tsfedl.YildirimOzal(input_tensor=input, include_top=False)\n",
    "        ### select the autoencode model from the list\n",
    "        autoencoder = model_list[0]  \n",
    "\n",
    "        # autoencoder = tf.keras.Model(inputs=input, outputs=model.output, name=\"autoencoder\")\n",
    "        \n",
    "        # print(model_list[0].summary())\n",
    "        # print(model_list[1].summary())\n",
    "        # print(model_list[2].summary())\n",
    "\n",
    "        print(autoencoder.summary())\n",
    "    # ### error in training, may be due to the input shape, expects multivariate input\n",
    "    # elif BACKBONE == 'rta':\n",
    "    #     print('training new model: rta')\n",
    "    #     #### build model ####\n",
    "    #     input = tf.keras.Input(shape=(50,1))\n",
    "    #     model = tsfedl.YiboGao(input_tensor=input, include_top=False)\n",
    "    #     # model.summary()\n",
    "    #     x = model.output\n",
    "    #     # x = tf.keras.layers.LSTM(units=20)(x)\n",
    "    #     ### Add the top module\n",
    "    #     x = tf.keras.layers.Flatten()(x)\n",
    "    #     x = tf.keras.layers.Dense(50)(x)\n",
    "    #     # x = tf.keras.layers.Dense(10)(x)\n",
    "    #     x = tf.keras.layers.Dense(1)(x)\n",
    "    #     out = tf.keras.layers.Reshape([1, 1])(x)\n",
    "\n",
    "    #     rta_model = tf.keras.Model(inputs=input, outputs=out, name=\"rta\")\n",
    "    #     rta_model.summary()\n",
    "\n",
    "        \n",
    "\n",
    "else:\n",
    "    if BACKBONE == 'forecaster':\n",
    "        print('fine tuning model: forecaster')\n",
    "        ### load the model\n",
    "        model = tf.keras.models.load_model(f'./trained_models/{BACKBONE}_events_minmax_mamba2.keras')\n",
    "        model.summary()\n",
    "    elif BACKBONE == 'autoencoder':\n",
    "        print('fine tuning model: autoencoder')\n",
    "        ### load the model\n",
    "        model = tf.keras.models.load_model(f'./trained_models/{BACKBONE}_events_minmax_mamba2.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINE_TUNE:\n",
    "    if BACKBONE == 'forecaster':\n",
    "        model = forecaster_event\n",
    "        print('Training {} model'.format(model.name))\n",
    "\n",
    "        # print('Training {} model'.format(BACKBONE))\n",
    "        ### train\n",
    "        model.compile(loss='mae', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "        # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/model.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir='./logs_forecaster_event'),]\n",
    "        history_event = model.fit(X_train_event, Y_train_event, epochs=50, batch_size=64, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "    elif BACKBONE == 'autoencoder':\n",
    "        model = autoencoder\n",
    "        print('Training {} model'.format(model.name))\n",
    "\n",
    "        # print('Training {} model'.format(BACKBONE))\n",
    "        ### train\n",
    "        model.compile(loss='mae', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "        # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/model.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir='./logs_forecaster_event'),]\n",
    "        history_event = model.fit(X_train_event, X_train_event, epochs=50, batch_size=128, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "    ### error in training, may be due to the input shape, expects multivariate input\n",
    "    # elif BACKBONE == 'rta':\n",
    "    #     model = rta_model\n",
    "    #     print('Training {} model'.format(model.name))\n",
    "\n",
    "    #     # print('Training {} model'.format(BACKBONE))\n",
    "    #     ### train\n",
    "    #     model.compile(loss='mae', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "    #     # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "    #     callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
    "    #             tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/model.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "    #             tf.keras.callbacks.TensorBoard(log_dir='./logs_forecaster_event'),]\n",
    "    #     history_event = model.fit(X_train_event, Y_train_event, epochs=30, batch_size=64, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    if BACKBONE == 'forecaster':\n",
    "        print('Fine tuning {} model'.format(model.name))\n",
    "        model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1e-04), metrics=['mae', 'mse'])\n",
    "\n",
    "        # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/forecaster_event.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir='./logs'),]\n",
    "        history_event = model.fit(X_train_event, Y_train_event , epochs=50, batch_size=128, validation_split=0.2, callbacks=callbacks)\n",
    "        \n",
    "    elif BACKBONE == 'autoencoder':\n",
    "        print('Fine tuning {} model'.format(model.name))\n",
    "        model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1e-04), metrics=['mae', 'mse'])\n",
    "\n",
    "        # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                # tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/autoencoder_event.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir='./logs'),]\n",
    "        history_event = model.fit(X_train_event, X_train_event, epochs=50, batch_size=128, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "    # elif BACKBONE == 'rta':\n",
    "    #     print('Fine tuning {} model'.format(model.name))\n",
    "    #     model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1e-04), metrics=['mae', 'mse'])\n",
    "\n",
    "    #     # history = forecaster.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "    #     callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    #             tf.keras.callbacks.ModelCheckpoint(filepath='./trained_models/tmp/rta_event.{epoch:02d}-{loss:.2f}.keras', monitor=\"loss\", save_best_only=True),\n",
    "    #             tf.keras.callbacks.TensorBoard(log_dir='./logs'),]\n",
    "    #     history_event = model.fit(X_train_event, Y_train_event, epochs=30, batch_size=128, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.models.load_model('./trained_models/autoencoder_events_minmax_mamba2_V4.keras')\n",
    "\n",
    "print('testing model {}'.format(model.name))\n",
    "y_pred_event = model.predict(x_test_event)\n",
    "# y_pred_interval = model.predict(x_test_interval)\n",
    "\n",
    "print(y_pred_event.shape)\n",
    "# print(y_test_interval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(y_test_interval[i], y_pred_interval[i])\n",
    "\n",
    "for i in range(10):\n",
    "    # print(x_test_event[i], y_pred_event[i])\n",
    "    # print(x_test_event[i].shape, y_pred_event[i].shape)\n",
    "    # print(x_test_raw[i])\n",
    "    descaled_x_test = scaler.inverse_transform(x_test_event[i].reshape(1, 50))\n",
    "    # descaled_y_pred = scaler.inverse_transform(y_pred_event[i].reshape(1, 50))\n",
    "    # print(descaled_x_test, descaled_y_pred)\n",
    "\n",
    "    # print(descaled_x_test.shape)\n",
    "    diff = y_pred_event[i] - descaled_x_test\n",
    "    # print(diff.shape)\n",
    "    # print(diff)\n",
    "    # print(np.abs(diff))\n",
    "    mae = np.mean(np.abs(diff))\n",
    "    mse = np.mean(np.square(diff))\n",
    "    print(mae)\n",
    "    print(mse)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_event.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing\n",
    "correct = []\n",
    "incorrect = []\n",
    "\n",
    "if y_test_event.shape[1] == 2:\n",
    "    for i in range(len(y_test_event)):\n",
    "        yt_event = y_test_event[i][0]\n",
    "        yp_event = y_pred_event[i][0]\n",
    "\n",
    "        yt_interval = y_test_event[i][1]\n",
    "        yp_interval = y_pred_event[i][1]\n",
    "\n",
    "        # print(yt_event, yp_event)\n",
    "        # print(yt_interval, yp_interval)\n",
    "        yt_event = yt_event.reshape(1,)\n",
    "        yp_event = yp_event.reshape(1,)\n",
    "        yt_interval = yt_interval.reshape(1,)\n",
    "        yp_interval = yp_interval.reshape(1,)\n",
    "\n",
    "        if np.abs(yt_event-yp_event) < 1 and np.abs(yt_interval-yp_interval) < 20:\n",
    "            # print('correct prediction')\n",
    "            correct.append(y_test_event[i])\n",
    "        else:\n",
    "            # print('incorrect prediction')\n",
    "            incorrect.append(y_test_event[i])\n",
    "            # print(yt, yp)\n",
    "        # break\n",
    "\n",
    "else:\n",
    "    if BACKBONE == 'forecaster':\n",
    "        for i in range(len(y_test_event)):\n",
    "\n",
    "            yt_event = y_test_event[i]\n",
    "            yp_event = y_pred_event[i]\n",
    "\n",
    "            yt_event = yt_event.reshape(1,)\n",
    "            yp_event = yp_event.reshape(1,)\n",
    "            print(yt_event, yp_event)\n",
    "            if np.abs(yt_event-yp_event) < 1:\n",
    "                # print('correct prediction')\n",
    "                correct.append(y_test_event[i])\n",
    "            else:\n",
    "                # print('incorrect prediction')\n",
    "                incorrect.append(y_test_event[i])\n",
    "                # print(yt, yp)\n",
    "            # break\n",
    "    elif BACKBONE == 'autoencoder':\n",
    "        all_mae = []\n",
    "        all_mse = []\n",
    "        for i in range(len(y_test_event)):\n",
    "\n",
    "            yt_event = x_test_event[i]\n",
    "            yp_event = y_pred_event[i]\n",
    "\n",
    "            yt_event = scaler.inverse_transform(yt_event.reshape(1, 50))[0]\n",
    "            yp_event = scaler.inverse_transform(yp_event.reshape(1, 50))[0]\n",
    "            print(yt_event, yp_event)\n",
    "            # print(yt_event.shape, yp_event.shape)\n",
    "            # yt_event = [round(x) for x in yt_event[0]]\n",
    "            # yp_event = [round(x) for x in yp_event[0]]\n",
    "            # print(yt_event, yp_event)\n",
    "\n",
    "            mse = 0\n",
    "            mae = 0\n",
    "            correct_match = True\n",
    "            ######### pointwise ##########\n",
    "            for x,y in zip(yt_event, yp_event):\n",
    "                _mse = (x-y)**2\n",
    "                _mae = np.abs(x-y)\n",
    "                # print('mae:', mae)\n",
    "\n",
    "                if not _mae < 1:\n",
    "                    correct_match = False\n",
    "                mse += _mse\n",
    "                mae += _mae\n",
    "\n",
    "\n",
    "            mse = mse/len(yt_event)\n",
    "            mae = mae/len(yt_event)\n",
    "            ##############################\n",
    "\n",
    "            ########### mae>1 ############\n",
    "            # mae = np.mean(np.abs(yt_event-yp_event))\n",
    "            # mse = np.mean((yt_event-yp_event)**2)\n",
    "            # if mae > 1:\n",
    "            #     correct_match = False\n",
    "\n",
    "            # print('mae:', mae)\n",
    "            # mse = np.mean((yt_event-yp_event)**2)\n",
    "            # mae = np.mean(np.abs(yt_event-yp_event))\n",
    "            ##############################\n",
    "            \n",
    "            all_mae.append(mae)\n",
    "            all_mse.append(mse)\n",
    "            \n",
    "            \n",
    "            if correct_match:\n",
    "            # if mse < 1:\n",
    "                # print('correct prediction')\n",
    "                correct.append(y_test_event[i])\n",
    "                # print(yt_event == yp_event)\n",
    "                # print([(int(x),y) for x,y in zip(yt_event, yp_event) ])\n",
    "                # break\n",
    "            else:\n",
    "                # print('incorrect prediction')\n",
    "                incorrect.append(y_test_event[i])\n",
    "                # print(yt, yp)\n",
    "                # print(yt_event == yp_event)\n",
    "                # print([(int(x),y) for x,y in zip(yt_event, yp_event) ])\n",
    "                # break\n",
    "        \n",
    "            # break\n",
    "\n",
    "print('correct:', len(correct))\n",
    "print('incorrect:', len(incorrect))\n",
    "\n",
    "### accuracy\n",
    "accuracy = len(correct)/len(y_test_event)\n",
    "print('accuracy:', accuracy)\n",
    "\n",
    "if BACKBONE == 'forecaster':\n",
    "    ### mae\n",
    "    mae = np.mean(np.abs(y_test_event-y_pred_event))\n",
    "    print('mae:', mae)\n",
    "    ### mse\n",
    "    mse = np.mean((y_test_event-y_pred_event)**2)\n",
    "    print('mse:', mse)\n",
    "elif BACKBONE == 'autoencoder':\n",
    "    ### mae\n",
    "    mae = np.mean(all_mae)\n",
    "    print('mae:', mae)\n",
    "    ### mse\n",
    "    mse = np.mean(all_mse)\n",
    "    print('mse:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_event.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FINE_TUNE:\n",
    "    print('saving new model: {}'.format(BACKBONE))\n",
    "    ### check if existing model is there\n",
    "    model_name = f'./trained_models/{BACKBONE}_events_minmax_{CODE}_V{VER}.keras'\n",
    "else:\n",
    "    print('saving fine tuned model')\n",
    "    model_name = f'./trained_models/{BACKBONE}_events_minmax_mamba+theft_V{VER}.keras'\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "    print('model exists')\n",
    "    raise FileExistsError\n",
    "else:\n",
    "    model.save(model_name)\n",
    "# model.save('./trained_models/forecaster_intervals_1.keras')\n",
    "# model.save_weights('./trained_models/forecaster_events.weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
