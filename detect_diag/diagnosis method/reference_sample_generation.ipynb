{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')  ### to detect libraries in the parent directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# process = psutil.Process(os.getpid())\n",
    "# start_mem = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "# start_time = time.perf_counter() \n",
    "\n",
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 4                     ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n",
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "### remove.Ds_store from all lists\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label\n",
    "\n",
    "print(train_data_path)\n",
    "print(test_data_path)\n",
    "print(test_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3 or VER == 4:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]]+ varlist_path) ### compare with train varlist\n",
    "\n",
    "    if check_con != False:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        ### load normal varlist\n",
    "        print('loading normal varlist')\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ref subseq (500 events window size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_LEN = 500\n",
    "\n",
    "### check if folder 'diag_refsamples' exists\n",
    "ref_samples_path = os.path.join(normalbase_path, f'diag_refsamples{SAMPLE_LEN}')\n",
    "# print(ref_samples_path)\n",
    "if not os.path.exists(ref_samples_path):\n",
    "    os.makedirs(ref_samples_path)\n",
    "    print(f'Folder does not exist. Creating folder {ref_samples_path}')\n",
    "else:\n",
    "    # print('Folder exists')\n",
    "    ### delete all files in the folder\n",
    "    for file in os.listdir(ref_samples_path):\n",
    "        print(os.path.join(ref_samples_path, file))\n",
    "        os.remove(os.path.join(ref_samples_path, file))\n",
    "    # os.rmdir(ref_samples_path)\n",
    "    # os.makedirs(ref_samples_path)\n",
    "    print(f'Folder exists. Deleting all files in {ref_samples_path}')\n",
    "\n",
    "### logic for creating reference samples\n",
    "'''\n",
    "- store reference samples of 500 events\n",
    "- store only unique samples (uniqueness based on sequence of events, intervals are ignored)\n",
    "- store the reference samples as json files\n",
    "- store the reference samples in the folder 'diag_refsamples'\n",
    "'''\n",
    "ind_count = 0   ### to keep track of the number of reference samples across all files\n",
    "unique_samples = []   ### to store unique reference samples across all files\n",
    "for train_data in train_data_path:\n",
    "    print(train_data)\n",
    "    trace_data = read_traces(train_data)\n",
    "    # print(len(trace_data))\n",
    "    # print(trace_data[0:50])\n",
    "\n",
    "    ### slide window of 500 events and save as reference samples\n",
    "    for i in range(0, len(trace_data)-SAMPLE_LEN):\n",
    "        # print(i)\n",
    "        ref_sample = trace_data[i:i+SAMPLE_LEN+1]\n",
    "        # print('window', i, i+SAMPLE_LEN)\n",
    "        # print('len of ref sample', len(ref_sample))\n",
    "\n",
    "        ### transform the trace data to events and intervals. Interval is the time difference between timestamps of consecutive events\n",
    "        events = []\n",
    "        intervals = []\n",
    "        #### old implementation with 0 at start of intervals\n",
    "        # prev_time = ref_sample[0][1]\n",
    "        # time_diff = 0\n",
    "        # for x in ref_sample:\n",
    "        #     time_diff = x[1] - prev_time\n",
    "        #     intervals.append(time_diff)\n",
    "        #     prev_time = x[1]\n",
    "        #     events.append(x[0])\n",
    "        ### revised implementation with interval as the time difference between consecutive events, the difference between 1st and 2nd event is the first interval\n",
    "        for x,y in zip(ref_sample[:-1], ref_sample[1:]):\n",
    "            events.append(x[0])\n",
    "            intervals.append(y[1] - x[1])\n",
    "\n",
    "        ref_sample = (events, intervals)\n",
    "        # print(ref_sample)\n",
    "        # print(len(ref_sample[0]), len(ref_sample[1]))\n",
    "        is_unique = False\n",
    "        if unique_samples == []:\n",
    "            unique_samples.append(ref_sample)\n",
    "            is_unique = True\n",
    "        else:\n",
    "            found = False\n",
    "            for unique_sample in unique_samples:\n",
    "                if unique_sample[0] == events:\n",
    "                    found = True\n",
    "                    # print(ind_count, 'duplicate')\n",
    "                    # print(unique_sample[0])\n",
    "                    # print(events)\n",
    "                    break\n",
    "            if not found:\n",
    "                unique_samples.append(ref_sample)\n",
    "                is_unique = True\n",
    "                # print(ind_count, 'unique')\n",
    "\n",
    "        if is_unique:\n",
    "            ref_samples_name = os.path.join(ref_samples_path, str(ind_count)+'.json')\n",
    "            print(len(ref_sample[0]), len(ref_sample[1]))\n",
    "            save_json(ref_sample, ref_samples_name)\n",
    "            print(f'Saved {ref_samples_name}')\n",
    "            # print('Saving Disabled for testing')\n",
    "\n",
    "        ind_count += 1 \n",
    "\n",
    "        print('---------------------------------\\n')\n",
    "    #     break\n",
    "\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_mem = process.memory_info().rss / (1024 * 1024)\n",
    "# print(f\"Memory used: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "# end_time = time.perf_counter()\n",
    "# elapsed_ms = (end_time - start_time) * 1000\n",
    "# print(f\"\\nTime taken: {elapsed_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overhead test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%reload_ext memory_profiler\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "start_time = time.perf_counter() \n",
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "\n",
    "SAMPLE_LEN = 500\n",
    "TEST_LEN = 25000\n",
    "\n",
    "### check if folder 'diag_refsamples' exists\n",
    "# ref_samples_path = os.path.join(normalbase_path, f'diag_refsamples{SAMPLE_LEN}')\n",
    "ref_samples_path = os.path.join(normalbase_path, f'diag_refsamples{SAMPLE_LEN}_len-test')\n",
    "# print(ref_samples_path)\n",
    "if not os.path.exists(ref_samples_path):\n",
    "    os.makedirs(ref_samples_path)\n",
    "    print(f'Folder does not exist. Creating folder {ref_samples_path}')\n",
    "else:\n",
    "    # print('Folder exists')\n",
    "    ### delete all files in the folder\n",
    "    for file in os.listdir(ref_samples_path):\n",
    "        # print(os.path.join(ref_samples_path, file))\n",
    "        os.remove(os.path.join(ref_samples_path, file))\n",
    "    # os.rmdir(ref_samples_path)\n",
    "    # os.makedirs(ref_samples_path)\n",
    "    print(f'Folder exists. Deleting all files in {ref_samples_path}')\n",
    "\n",
    "ind_count = 0   ### to keep track of the number of reference samples across all files\n",
    "unique_samples = []   ### to store unique reference samples across all files\n",
    "_REM_LEN = TEST_LEN\n",
    "for train_data in train_data_path:\n",
    "    print(train_data)\n",
    "    trace_data = read_traces(train_data)\n",
    "\n",
    "    if _REM_LEN > 0:\n",
    "        trace_data = trace_data[:_REM_LEN]\n",
    "        _REM_LEN -= len(trace_data)\n",
    "        print('data_len:', len(trace_data))\n",
    "        print('remaining_len:', _REM_LEN)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    # print(len(trace_data))\n",
    "    # print(trace_data[0:50])\n",
    "\n",
    "    ### slide window of 500 events and save as reference samples\n",
    "    for i in range(0, len(trace_data)-SAMPLE_LEN):\n",
    "        # print(i)\n",
    "        ref_sample = trace_data[i:i+SAMPLE_LEN+1]\n",
    "        # print('window', i, i+SAMPLE_LEN)\n",
    "        # print('len of ref sample', len(ref_sample))\n",
    "\n",
    "        ### transform the trace data to events and intervals. Interval is the time difference between timestamps of consecutive events\n",
    "        events = []\n",
    "        intervals = []\n",
    "        #### old implementation with 0 at start of intervals\n",
    "        # prev_time = ref_sample[0][1]\n",
    "        # time_diff = 0\n",
    "        # for x in ref_sample:\n",
    "        #     time_diff = x[1] - prev_time\n",
    "        #     intervals.append(time_diff)\n",
    "        #     prev_time = x[1]\n",
    "        #     events.append(x[0])\n",
    "        ### revised implementation with interval as the time difference between consecutive events, the difference between 1st and 2nd event is the first interval\n",
    "        for x,y in zip(ref_sample[:-1], ref_sample[1:]):\n",
    "            events.append(x[0])\n",
    "            intervals.append(y[1] - x[1])\n",
    "\n",
    "        ref_sample = (events, intervals)\n",
    "        # print(ref_sample)\n",
    "        # print(len(ref_sample[0]), len(ref_sample[1]))\n",
    "        is_unique = False\n",
    "        if unique_samples == []:\n",
    "            unique_samples.append(ref_sample)\n",
    "            is_unique = True\n",
    "        else:\n",
    "            found = False\n",
    "            for unique_sample in unique_samples:\n",
    "                if unique_sample[0] == events:\n",
    "                    found = True\n",
    "                    # print(ind_count, 'duplicate')\n",
    "                    # print(unique_sample[0])\n",
    "                    # print(events)\n",
    "                    break\n",
    "            if not found:\n",
    "                unique_samples.append(ref_sample)\n",
    "                is_unique = True\n",
    "                # print(ind_count, 'unique')\n",
    "\n",
    "        if is_unique:\n",
    "            ref_samples_name = os.path.join(ref_samples_path, str(ind_count)+'.json')\n",
    "            # print(len(ref_sample[0]), len(ref_sample[1]))\n",
    "            save_json(ref_sample, ref_samples_name)\n",
    "            # print(f'Saved {ref_samples_name}')\n",
    "\n",
    "        ind_count += 1 \n",
    "\n",
    "        # print('---------------------------------\\n')\n",
    "    #     break\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_ms = (end_time - start_time) * 1000\n",
    "print(f\"\\nTime taken (Training): {elapsed_ms:.2f} ms\")\n",
    "\n",
    "%memit\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "- instead of storing the trace as it is, store the variable and difference of two consecutive TS\n",
    "- store it as two lists instead of single list of tuples \n",
    "- logic for only unique samples\n",
    "- store the reference samples as json files\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ref subseq (variable window size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_WINDOW = 10\n",
    "# MAX_WINDOW = 500\n",
    "# SLIDING_INTERVAL = 50\n",
    "\n",
    "# ### check if folder 'diag_refsamples' exists\n",
    "# ref_samples_path = os.path.join(normalbase_path, 'diag_var_refsamples')\n",
    "# if not os.path.exists(ref_samples_path):\n",
    "#     os.makedirs(ref_samples_path)\n",
    "#     print(f'Folder does not exist. Creating folder {ref_samples_path}')\n",
    "# else:\n",
    "#     print('Folder exists')\n",
    "\n",
    "# ind_count = 0\n",
    "# unique_samples = []\n",
    "# map_len = defaultdict(list)\n",
    "# for train_data in train_data_path:\n",
    "#     print(train_data)\n",
    "#     trace_data = read_traces(train_data)\n",
    "#     print(len(trace_data))\n",
    "\n",
    "#     for sample_len in range(MIN_WINDOW, MAX_WINDOW+1, SLIDING_INTERVAL):\n",
    "#         print(f'Window size: {sample_len}')\n",
    "#         for i in range(0, len(trace_data)-sample_len+1):\n",
    "#             ref_sample = trace_data[i:i+sample_len]\n",
    "\n",
    "#             ### transform the trace data to events and intervals. Interval is the time difference between timestamps of consecutive events\n",
    "#             events = []\n",
    "#             intervals = []\n",
    "#             prev_time = ref_sample[0][1]\n",
    "#             time_diff = 0\n",
    "#             for x in ref_sample:\n",
    "#                 time_diff = x[1] - prev_time\n",
    "#                 intervals.append(time_diff)\n",
    "#                 prev_time = x[1]\n",
    "#                 events.append(x[0])\n",
    "\n",
    "#             ref_sample = (events, intervals)\n",
    "#             is_unique = False\n",
    "#             if unique_samples == []:\n",
    "#                 unique_samples.append(ref_sample)\n",
    "#                 map_len[sample_len] += [ind_count]\n",
    "#                 is_unique = True\n",
    "#             else:\n",
    "#                 found = False\n",
    "#                 for unique_sample in unique_samples:\n",
    "#                     if unique_sample[0] == events:\n",
    "#                         found = True\n",
    "#                         print(ind_count, 'duplicate')\n",
    "#                         # print(unique_sample[0])\n",
    "#                         # print(events)\n",
    "#                         break\n",
    "#                 if not found:\n",
    "#                     unique_samples.append(ref_sample)\n",
    "#                     map_len[sample_len] += [ind_count]\n",
    "#                     is_unique = True\n",
    "#                     # print(ind_count, 'unique')\n",
    "\n",
    "#             if is_unique:\n",
    "#                 ref_samples_name = os.path.join(ref_samples_path, str(ind_count)+'.json')\n",
    "#                 save_json(ref_sample, ref_samples_name)\n",
    "#                 print(f'Saved {ref_samples_name}')\n",
    "\n",
    "#             ind_count += 1 \n",
    "\n",
    "#             print('---------------------------------\\n')\n",
    "\n",
    "#     save_json(map_len, os.path.join(ref_samples_path, 'map_len.json'))\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# for k in map_len.keys():\n",
    "#     print(k, len(map_len[k]))\n",
    "#     print(map_len[k])\n",
    "#     total += len(map_len[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
