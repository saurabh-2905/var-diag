{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.hybrid import hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE = 'mamba2'       ### application (code)\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "\n",
    "base_dir = '../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = os.listdir(normalbase_path)\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in train_varlist_path if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "### remove.Ds_store from all lists\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "print(train_data_path)\n",
    "print(paths_log)\n",
    "print(paths_traces)\n",
    "print(varlist_path)\n",
    "print(paths_label)\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]]+ varlist_path) ### compare with train varlist\n",
    "\n",
    "    if check_con != False:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        ### load normal varlist\n",
    "        print('loading normal varlist')\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize the hybrid model\n",
    "hybrid = hybrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid.train(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = hybrid.transitions\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### viz transitions\n",
    "\n",
    "for key in transitions.keys():\n",
    "    print(from_number[key], ':', end=' ')\n",
    "    for val in transitions[key]:\n",
    "        print(from_number[val], end=', ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = hybrid.thresholds\n",
    "### visualize the thresholds for varlist\n",
    "for key in thresholds.keys():\n",
    "    print(from_number[key], ':', end=' ')\n",
    "    print(thresholds[key], end=', ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot exe_list to vsiualize the distribution of execution intervals\n",
    "hybrid.viz_thresholds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect anomalies in faulty traces\n",
    "DIFF_VAL = 2\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "all_group_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]\n",
    "all_merged_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "for ti, (test_data, test_label) in enumerate(zip(test_data_path, test_label_path)):\n",
    "    print(ti, test_data, test_label)\n",
    "    # if ti == 1:\n",
    "         \n",
    "    hybrid_detections = hybrid.test_single(test_data, thresholds)   ### detection in format: [var, (ts1,ts2), file_name]\n",
    "    ei_detection = hybrid.ei_detections\n",
    "    st_detection = hybrid.st_detections\n",
    "\n",
    "    all_detections += [(test_data, hybrid_detections, test_label)]  ### used to plot detections\n",
    "    # all_group_detections += [(test_data, grouped_det, test_label)]  ### used to plot grouped detections\n",
    "    # all_merged_detections += [(test_data, merged_detection, test_label)]  ### used to plot merged detections\n",
    "\n",
    "    ### load ground truths\n",
    "    ground_truth_raw = read_traces(test_label)\n",
    "    ground_truth = ground_truth_raw['labels']\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "    print('ground truths:', ground_truth)\n",
    "    print(len(ground_truth))\n",
    "\n",
    "    # correct_pred, rest_pred, y_pred, y_true = get_ypred_ytrue(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "    correct_pred, rest_pred, y_pred, y_true = hybrid.get_correct_detections(hybrid_detections, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "\n",
    "    assert( len(hybrid_detections) == len(correct_pred) + len(rest_pred) )\n",
    "\n",
    "    all_tp += [(test_data, correct_pred, test_label)]\n",
    "    all_fp += [(test_data, rest_pred, test_label)]\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Approach 1: (based on the assumption that ST can make very precise detections but can miss some detections, while EI can detect every event but gives more wide detections)\n",
    "\n",
    "for each ei detection detect all the st detection within it, store it in a list\n",
    "if st detection exist for any ei detection, output only st.\n",
    "if st detection does not exist for any ei detection, output ei\n",
    "\n",
    "'''\n",
    "\n",
    "# all_detections = []\n",
    "# for ei_det in ei_detection:\n",
    "#     #### structure of detection, get elements\n",
    "#     ei_var = ei_det[0]\n",
    "#     eits1, eits2 = ei_det[1]\n",
    "#     print('EI', eits1, eits2)\n",
    "\n",
    "#     #### get all st detections within the ei detection\n",
    "#     st_detections_within_ei = []\n",
    "#     for i, st_det in enumerate(st_detection):\n",
    "#         st_var = st_det[0]\n",
    "#         stts1, stts2 = st_det[1]\n",
    "#         print('ST', i, stts1, stts2)\n",
    "\n",
    "#         if eits1 <= stts1 and eits2 >= stts2:\n",
    "#             st_detections_within_ei.append(st_det)\n",
    "            \n",
    "#     if len(st_detections_within_ei) > 0:\n",
    "#         for det in st_detections_within_ei:\n",
    "#             print('Removed', det)\n",
    "#             st_detection.remove(det)\n",
    "#         all_detections.extend(st_detections_within_ei)\n",
    "#         print(ei_det, 'replaced with', st_detections_within_ei)\n",
    "#     else:\n",
    "#         all_detections.append([ei_det])\n",
    "#         print('added', ei_det)\n",
    "\n",
    "# print('Any Detections in ST that are not in EI:')\n",
    "# print(st_detection)\n",
    "# all_detections.extend(st_detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(st_detection), len(ei_detection))\n",
    "st_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "if len(conf_matrix) == 1:\n",
    "    conf_matrix = np.array([[0, 0], [0, conf_matrix[0][0]]])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## save detections for the dashboard to plot #############\n",
    "import traceback\n",
    "\n",
    "for test_data, detections, test_label in all_detections:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'st_detections')\n",
    "    tp_detection_path = detection_path.replace('st_detections.json', 'tp_st_detections.json')\n",
    "    fp_detection_path = detection_path.replace('st_detections.json', 'fp_st_detections.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "        with open(detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {detection_path}')\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue\n",
    "\n",
    "for test_data, detections, test_label in all_tp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'st_detections')\n",
    "    tp_detection_path = detection_path.replace('st_detections.json', 'tp_st_detections.json')\n",
    "    fp_detection_path = detection_path.replace('st_detections.json', 'fp_st_detections.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(tp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {tp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue\n",
    "\n",
    "for test_data, detections, test_label in all_fp:\n",
    "    # print(test_data, test_label)\n",
    "    # print(test_label.replace('labels', 'detections'))\n",
    "    detection_path = test_label.replace('labels', 'st_detections')\n",
    "    tp_detection_path = detection_path.replace('st_detections.json', 'tp_st_detections.json')\n",
    "    fp_detection_path = detection_path.replace('st_detections.json', 'fp_st_detections.json')\n",
    "    # print(detections)\n",
    "\n",
    "    detection_dir = os.path.dirname(detection_path)\n",
    "    # print(detection_dir)\n",
    "    if not os.path.exists(detection_dir):\n",
    "        os.makedirs(detection_dir)\n",
    "        print(f'Created Directory: {detection_dir}')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(fp_detection_path, 'w') as f:\n",
    "            json.dump(detections, f)\n",
    "            print(f'Saved detections in {fp_detection_path}')\n",
    "            \n",
    "    except Exception as e:\n",
    "        traceback.print_exception(e)\n",
    "        print('Error in saving detections')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### plot gt and detections\n",
    "# for test_data, detections, test_label in all_detections:\n",
    "# # for test_data, detections, test_label in all_fp:\n",
    "#     # print('test_data:', test_data)\n",
    "#     # print('detections:', detections)\n",
    "#     # print(test_label)\n",
    "\n",
    "#     ### prepare trace to plot\n",
    "#     col_data = preprocess_traces([test_data])\n",
    "#     all_df = get_dataframe(col_data) \n",
    "#     # print(all_df[0])\n",
    "\n",
    "#     ### prepare detections to plot\n",
    "#     timestamps = col_data[0][1]\n",
    "#     print('timestamps:', timestamps)\n",
    "#     plot_val = []\n",
    "#     plot_x_ticks = []\n",
    "#     plot_class = []\n",
    "#     for det in detections:\n",
    "#         # print(det)\n",
    "#         det_ts1, det_ts2 = det[1]\n",
    "#         # print(det_ts1, det_ts2)\n",
    "\n",
    "#         det_ind1_pre = [ abs(t-det_ts1) for t in timestamps]\n",
    "#         det_ind1 = det_ind1_pre.index(min(det_ind1_pre))\n",
    "\n",
    "#         det_ind2_pre = [ abs(t-det_ts2) for t in timestamps]\n",
    "#         det_ind2 = det_ind2_pre.index(min(det_ind2_pre))\n",
    "#         # print(det_ind1, det_ind2)\n",
    "#         # print(timestamps[det_ind1], timestamps[det_ind2])\n",
    "\n",
    "#         plot_val += [(det_ind1, det_ind2)]\n",
    "#         plot_x_ticks += [(timestamps[det_ind1], timestamps[det_ind2])]\n",
    "#         plot_class += [0]\n",
    "\n",
    "#     plot_detections = [plot_val, plot_x_ticks, plot_class]\n",
    "\n",
    "#     ### get ground truths\n",
    "#     gt_plot = prepare_gt(test_label)\n",
    "\n",
    "#     ### plot\n",
    "#     for df in all_df:\n",
    "#         # print(df.columns)\n",
    "#         plot_single_trace(df, \n",
    "#                           var_list, \n",
    "#                           with_time=False, \n",
    "#                           is_xticks=True, \n",
    "#                           detections=plot_detections, \n",
    "#                           dt_classlist=['detection'],\n",
    "#                           ground_truths=gt_plot,\n",
    "#                           gt_classlist=['gt_communication', 'gt_sensor', 'gt_bitflip'],\n",
    "#                           )\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
