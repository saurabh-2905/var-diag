{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class SVM - credit card example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import OneClassSVM \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_OCSVM(i): \n",
    "\tplt.scatter(data_50k_df.iloc[:,i],data_50k_df.iloc[:,i+1],c='red',s=40, edgecolor=\"k\") \n",
    "\tplt.scatter(svm_anomalies.iloc[:,i],svm_anomalies.iloc[:,i+1],c='green', s=40, edgecolor=\"k\") \n",
    "\tplt.title(\"OC-SVM Outlier detection between Feature Pair: V{} and V{}\".format(i,i+1)) \n",
    "\tplt.xlabel(\"V{}\".format(i)) \n",
    "\tplt.ylabel(\"V{}\".format(i+1)) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('../creditcard_data/creditcard.csv', nrows=50000) # https://www.kaggle.com/mlg-ulb/creditcardfraud \n",
    "standardized_data_without_class = StandardScaler().fit_transform(credit_data.loc[:,credit_data.columns!='Class']) \n",
    "data_50k_new = standardized_data_without_class[0:50000] \n",
    "data_50k_df = pd.DataFrame(data=data_50k_new) \n",
    "# Separate features and target variable \n",
    "X = credit_data.drop(columns=['Class']) \n",
    "y = credit_data['Class'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = OneClassSVM(kernel=\"rbf\", degree=3, gamma=0.1, nu=0.01) \n",
    "y_predict = clf_svm.fit_predict(data_50k_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predict = pd.Series(y_predict).replace([-1,1],[1,0]) \n",
    "svm_anomalies = data_50k_df[svm_predict==1] \n",
    "# Calculate accuracy \n",
    "accuracy = accuracy_score(y, svm_predict) \n",
    "print(\"Accuracy in separating Outlier:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_OCSVM(1) # chnage the integer value to visualize different pairs of features \n",
    "# plot_OCSVM(2) \n",
    "plot_OCSVM(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty Detection using Unsupervised Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from libraries.utils import *\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "\n",
    "base_dir = '../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "print(train_data_path)\n",
    "\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "### remove.Ds_store from all lists\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "print(paths_log)\n",
    "print(paths_traces)\n",
    "print(varlist_path)\n",
    "print(paths_label)\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3:\n",
    "    to_number = is_consistent(varlist_path)\n",
    "\n",
    "    if to_number != False:\n",
    "        from_number = mapint2var(to_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_path[0])\n",
    "print(train_data_path[0].find('.npy'))\n",
    "print(train_data_path[0].find('.json') )\n",
    "\n",
    "if train_data_path[0].find('.npy') != -1:\n",
    "    sample_data = load_sample(train_data_path[0])\n",
    "    print('.npy')\n",
    "elif train_data_path[0].find('.json') != -1:\n",
    "    sample_data = read_traces(train_data_path[0])\n",
    "    print('.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Novelty Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "for sample_path in train_data_path:\n",
    "    ### load the trace\n",
    "    if sample_path.find('.npy') != -1:\n",
    "        sample_data = load_sample(sample_path)\n",
    "        print(sample_path)\n",
    "    elif sample_path.find('.json') != -1:\n",
    "        sample_data = read_traces(sample_path)\n",
    "        print(sample_path)\n",
    "\n",
    "    ### generate instances from the event trace with window sizw of 100 and sliding interval of 1\n",
    "    print(sample_data)\n",
    "    instances.extend(generate_instances(sample_data, 100, 1))\n",
    "\n",
    "    ### save as numpy array\n",
    "    # np.save(f'../data-novelty/subtraces{CODE}_ver{VER}_W100.npy', instances, allow_pickle=True)\n",
    "\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features for LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load the data\n",
    "data = np.load(f'../data-novelty/subtraces_{CODE}_ver{VER}_W100.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Approach 1 ################\n",
    "features_all = []\n",
    "features_all_1d = []\n",
    "for inst in data:\n",
    "    # print(len(inst))\n",
    "    # print(inst)\n",
    "    features_2d = []\n",
    "    features_1d = []\n",
    "    for e1, e2 in zip(inst[:-1], inst[1:]):\n",
    "        # print(e1, e2)\n",
    "\n",
    "        ########## first feature extraction method ##########\n",
    "        '''\n",
    "        it is a two dimension feature vector\n",
    "        for len of 100 events it will be 100x2\n",
    "        for each event we have two features, the first feature is the difference between the two events, \n",
    "        and the second feature is the difference between the two events divided by 1000\n",
    "        '''\n",
    "        feat1 = e2[0] - e1[0]\n",
    "        feat2 = (e2[1] - e1[1]) / 1000\n",
    "        # print(feat1, feat2)\n",
    "        features_2d.append((feat1, feat2))\n",
    "        features_1d.append(feat1)\n",
    "        features_1d.append(feat2)\n",
    "        ############ end of first feature extraction method ##########\n",
    "\n",
    "    \n",
    "    assert(len(features_2d)==len(inst)-1)\n",
    "    features_all.append(features_2d)\n",
    "    features_all_1d.append(features_1d)\n",
    "\n",
    "    # break\n",
    "assert(len(features_all)==len(data))\n",
    "\n",
    "### save features as csv file for training\n",
    "# df = pd.DataFrame(np.array(features_all))\n",
    "df_1d = pd.DataFrame(np.array(features_all_1d))\n",
    "\n",
    "# df.to_csv(f'../data-novelty/train_{CODE}_ver{VER}_W100_features_2d.csv', index=False)\n",
    "# df_1d.to_csv(f'../data-novelty/train_{CODE}_ver{VER}_W100_features_1d.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction 2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = []\n",
    "features_all_1d = []\n",
    "for inst in data:\n",
    "    # print(len(inst))\n",
    "    # print(inst)\n",
    "    features_2d = []\n",
    "    features_1d = []\n",
    "    for e1, e2 in zip(inst[:-1], inst[1:]):\n",
    "        # print(e1, e2)\n",
    "\n",
    "        ########## second feature extraction method ##########\n",
    "        '''\n",
    "        use execution intervals of each variable as a feature\n",
    "        1. get average execution interval for each variable, and take interval as 0 for variables that are not present in sub-trace -> number of features = total numbe of variables\n",
    "        2 take average of execution intervals of all variables in the sub-trace -> number of features = 1\n",
    "        3. run mutiple SVMs, one for each variable. Take the list of execution interval as features -> detect outliers in the execution interval and track it back to the \n",
    "        '''\n",
    "    \n",
    "    assert(len(features_2d)==len(inst)-1)\n",
    "    features_all.append(features_2d)\n",
    "    features_all_1d.append(features_1d)\n",
    "\n",
    "    # break\n",
    "assert(len(features_all)==len(data))\n",
    "\n",
    "### save features as csv file for training\n",
    "# df = pd.DataFrame(np.array(features_all))\n",
    "df_1d = pd.DataFrame(np.array(features_all_1d))\n",
    "\n",
    "# df.to_csv(f'../data-novelty/train_{CODE}_ver{VER}_W100_features_2d.csv', index=False)\n",
    "# df_1d.to_csv(f'../data-novelty/train_{CODE}_ver{VER}_W100_features_1d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load test data and labels\n",
    "test_data = {}\n",
    "test_labels = {}\n",
    "for i, test_path in enumerate(test_data_path):\n",
    "    print(test_path)\n",
    "    test_data[i] = read_traces(test_path)\n",
    "    test_labels[i] = read_json(test_label_path[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### cut instances based on labels\n",
    "\n",
    "all_mid = list()\n",
    "all_fh = list()\n",
    "all_lh = list()\n",
    "all_labels = list()\n",
    "for i in test_data.keys():\n",
    "    print(i)\n",
    "    # print(test_data[i])\n",
    "    # print(test_labels[i])\n",
    "    data = test_data[i]\n",
    "    label_raw = test_labels[i]\n",
    "    labels = label_raw['labels']\n",
    "    # print(list(labels.keys())[0])\n",
    "    label_key = list(labels.keys())[0]\n",
    "    labels = labels[label_key]\n",
    "\n",
    "    # print(data)\n",
    "    # print(labels)\n",
    "    anomaly_traces_mid = []\n",
    "    anomaly_traces_fh = []\n",
    "    anomaly_traces_lh = []\n",
    "    anomaly_traces_str = []\n",
    "    anomaly_traces_end = []\n",
    "    fh_plot_ind = []\n",
    "    lh_plot_ind = []\n",
    "    for label in labels:\n",
    "        # print(label)\n",
    "        ind1, ind2, ts1, ts2, clas = label\n",
    "        # print(ind1, ind2, ts1, ts2, clas)\n",
    "\n",
    "        #### traces with anomaly in the center\n",
    "        middle_sp = np.clip(ind1-50, 0, len(data))   #### keep the anomaly in the middle and cut window of 100 evetns. Makes sure the start point is not negative\n",
    "        middle_ep = np.clip(middle_sp+100, 0, len(data))   #### keep the anomaly in the middle and cut window of 100 evetns. Makes sure the end point is not more than the length of the data\n",
    "        # print(middle_sp, middle_ep)\n",
    "        anomaly_trace_mid = data[middle_sp:middle_ep]\n",
    "        anomaly_traces_mid.append(anomaly_trace_mid)\n",
    "\n",
    "        ##### trace wih anomaly in the first half\n",
    "        fh_ind = np.random.randint(10, 40)    ### Decide the first half randomly\n",
    "        fh_sp = np.clip(ind1-fh_ind, 0, len(data))\n",
    "        fh_ep = np.clip(fh_sp+100, 0, len(data))\n",
    "        fh_plot_ind.append(fh_ind)\n",
    "        # print(fh_sp, fh_ep)\n",
    "        anomaly_trace_fh = data[fh_sp:fh_ep]\n",
    "        anomaly_traces_fh.append(anomaly_trace_fh)\n",
    "\n",
    "        ##### trace wih anomaly in the last half\n",
    "        lh_ind = np.random.randint(60, 85)    ### Decide the last half randomly\n",
    "        lh_sp = np.clip(ind1-lh_ind, 0, len(data))\n",
    "        lh_ep = np.clip(lh_sp+100, 0, len(data))\n",
    "        lh_plot_ind.append(lh_ind)\n",
    "        # print(fh_sp, fh_ep)\n",
    "        anomaly_trace_lh = data[lh_sp:lh_ep]\n",
    "        anomaly_traces_lh.append(anomaly_trace_lh)\n",
    "\n",
    "        all_labels += [(label, 50, fh_ind, lh_ind,label_key)]   ### store the labels to plot anomalies in the subtraces\n",
    "        assert(len(anomaly_trace_mid)==100)\n",
    "\n",
    "    assert(len(anomaly_traces_mid)==len(labels))\n",
    "    print(len(anomaly_traces_mid), len(labels))\n",
    "    assert(len(anomaly_traces_fh)==len(labels))\n",
    "    print(len(anomaly_traces_fh), len(labels))\n",
    "    assert(len(anomaly_traces_lh)==len(labels))\n",
    "    print(len(anomaly_traces_lh), len(labels))\n",
    "\n",
    "    #### store all the anomaly traces for futher processing\n",
    "    all_mid.extend(anomaly_traces_mid)\n",
    "    all_fh.extend(anomaly_traces_fh)\n",
    "    all_lh.extend(anomaly_traces_lh)\n",
    "    \n",
    "\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features of Anomaly Instances (generate test data in csv format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Extract features and Save Test data as csv file\n",
    "\n",
    "subtrace_type = ('mid', 'fh', 'lh')\n",
    "for type in subtrace_type:\n",
    "    if type == 'mid':\n",
    "        data = all_mid\n",
    "    elif type == 'fh':\n",
    "        data = all_fh\n",
    "    elif type == 'lh':\n",
    "        data = all_lh\n",
    "\n",
    "    print(type)\n",
    "    features_all = []\n",
    "    features_all_1d = []\n",
    "    for label, inst in zip(all_labels, data):\n",
    "        assert(len(inst)==100)\n",
    "        \n",
    "        features_2d = []\n",
    "        features_1d = []\n",
    "        for e1, e2 in zip(inst[:-1], inst[1:]):\n",
    "            # print(e1, e2)\n",
    "\n",
    "            ########## first feature extraction method ##########\n",
    "            '''\n",
    "            it is a two dimension feature vector\n",
    "            for len of 100 events it will be 100x2\n",
    "            for each event we have two features, the first feature is the difference between the two events, \n",
    "            and the second feature is the difference between the two events divided by 1000\n",
    "            '''\n",
    "            feat1 = e2[0] - e1[0]\n",
    "            feat2 = (e2[1] - e1[1]) / 1000\n",
    "            # print(feat1, feat2)\n",
    "            features_2d.append((feat1, feat2))\n",
    "            features_1d.append(feat1)\n",
    "            features_1d.append(feat2)\n",
    "            ############ end of first feature extraction method ##########\n",
    "        \n",
    "        assert(len(features_2d)==len(inst)-1)\n",
    "        features_all.append(features_2d)\n",
    "        features_all_1d.append(features_1d)\n",
    "\n",
    "        # break\n",
    "    assert(len(features_all)==len(data))\n",
    "\n",
    "    ### save features as csv file for training\n",
    "    # df = pd.DataFrame(features_all)\n",
    "    df_1d = pd.DataFrame(np.array(features_all_1d))\n",
    "\n",
    "    # df.to_csv(f'../data-novelty/test_{CODE}_ver{VER}_W100_features_2d_{type}.csv', index=False)\n",
    "    # df_1d.to_csv(f'../data-novelty/test_{CODE}_ver{VER}_W100_features_1d_{type}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Anomaly Trace with Anomalies (verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the anomaly traces with the label (mid)\n",
    "with_time = False\n",
    "is_xticks = True\n",
    "plot_single = 0 ### give index number of the trace\n",
    "\n",
    "\n",
    "for i in range(plot_single, len(anomaly_traces_mid)):\n",
    "    anomaly_trace = anomaly_traces_mid[i]\n",
    "    anomaly_label = labels[i]\n",
    "    #### plot the trace using plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if with_time:\n",
    "        fig.add_trace(go.Scatter(x=[x[1] for x in anomaly_trace], y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "\n",
    "    ### adapt labels for the subtrace which starts from index 0\n",
    "    start_ind = 50   ### we start each trace 50 events before the anomaly starts\n",
    "    end_ind = start_ind + anomaly_label[1]-anomaly_label[0]  ### we end each trace 50 events after the anomaly ends\n",
    "    (start_ts, end_ts) = (anomaly_label[2], anomaly_label[3])\n",
    "    ground_truths_class = anomaly_label[4]\n",
    "\n",
    "    \n",
    "    ### check if time on x-axis\n",
    "    if with_time:\n",
    "        start = start_ts\n",
    "        end = end_ts\n",
    "    else:\n",
    "        start = start_ind\n",
    "        end = end_ind\n",
    "\n",
    "    \n",
    "    # Add dotted lines on the sides of the rectangle\n",
    "    for x in [start, end]:\n",
    "        fig.add_shape(type=\"line\",\n",
    "                xref=\"x\",\n",
    "                yref=\"paper\",\n",
    "                x0=x,\n",
    "                y0=0,\n",
    "                x1=x,\n",
    "                y1=1,\n",
    "                line=dict(\n",
    "                    color='red',\n",
    "                    width=4,\n",
    "                    dash=\"dot\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "    ### generate x ticks with timestamp and index num  \n",
    "    x_data = [x[1] for x in anomaly_trace]\n",
    "    if is_xticks == True and with_time == False:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [k for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == True and with_time == True:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [x_data[k] for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == False:\n",
    "        x_ticks = None\n",
    "        x_tickvals = None\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Event Trace without Time\",\n",
    "        xaxis=dict(\n",
    "            title=\"Number of events\",\n",
    "            rangeslider=dict(visible=True),\n",
    "            type='linear',\n",
    "            tickvals=x_tickvals,\n",
    "            ticktext=x_ticks,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Variables\",\n",
    "            tickvals=[k for k in range(0,len(var_list))],\n",
    "            # ticktext= var_list,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        autosize=True,\n",
    "        width=PLOTWIDTH,\n",
    "        height=PLOTHEIGHT,\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "    )\n",
    "    \n",
    "\n",
    "    fig.update_xaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "\n",
    "    # style all the traces\n",
    "    fig.update_traces(\n",
    "        #hoverinfo=\"name+x+text\",\n",
    "        line={\"width\": 0.5},\n",
    "        marker={\"size\": 8},\n",
    "        mode=\"lines+markers\",\n",
    "        showlegend=True,   \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the anomaly traces with the label\n",
    "with_time = False\n",
    "is_xticks = True\n",
    "plot_single = 4 ### give index number of the trace\n",
    "\n",
    "\n",
    "for i in range(plot_single, len(anomaly_traces_fh)):\n",
    "    anomaly_trace = anomaly_traces_fh[i]\n",
    "    anomaly_label = labels[i]\n",
    "    #### plot the trace using plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if with_time:\n",
    "        fig.add_trace(go.Scatter(x=[x[1] for x in anomaly_trace], y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "\n",
    "    ### adapt labels for the subtrace which starts from index 0\n",
    "    start_ind = fh_plot_ind[i]   ### we start each trace 50 events before the anomaly starts\n",
    "    end_ind = start_ind + anomaly_label[1]-anomaly_label[0]  ### we end each trace 50 events after the anomaly ends\n",
    "    (start_ts, end_ts) = (anomaly_label[2], anomaly_label[3])\n",
    "    ground_truths_class = anomaly_label[4]\n",
    "\n",
    "    \n",
    "    ### check if time on x-axis\n",
    "    if with_time:\n",
    "        start = start_ts\n",
    "        end = end_ts\n",
    "    else:\n",
    "        start = start_ind\n",
    "        end = end_ind\n",
    "\n",
    "    \n",
    "    # Add dotted lines on the sides of the rectangle\n",
    "    for x in [start, end]:\n",
    "        fig.add_shape(type=\"line\",\n",
    "                xref=\"x\",\n",
    "                yref=\"paper\",\n",
    "                x0=x,\n",
    "                y0=0,\n",
    "                x1=x,\n",
    "                y1=1,\n",
    "                line=dict(\n",
    "                    color='red',\n",
    "                    width=4,\n",
    "                    dash=\"dot\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "    ### generate x ticks with timestamp and index num  \n",
    "    x_data = [x[1] for x in anomaly_trace]\n",
    "    if is_xticks == True and with_time == False:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [k for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == True and with_time == True:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [x_data[k] for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == False:\n",
    "        x_ticks = None\n",
    "        x_tickvals = None\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Event Trace without Time\",\n",
    "        xaxis=dict(\n",
    "            title=\"Number of events\",\n",
    "            rangeslider=dict(visible=True),\n",
    "            type='linear',\n",
    "            tickvals=x_tickvals,\n",
    "            ticktext=x_ticks,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Variables\",\n",
    "            tickvals=[k for k in range(0,len(var_list))],\n",
    "            # ticktext= var_list,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        autosize=True,\n",
    "        width=PLOTWIDTH,\n",
    "        height=PLOTHEIGHT,\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "    )\n",
    "    \n",
    "\n",
    "    fig.update_xaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "\n",
    "    # style all the traces\n",
    "    fig.update_traces(\n",
    "        #hoverinfo=\"name+x+text\",\n",
    "        line={\"width\": 0.5},\n",
    "        marker={\"size\": 8},\n",
    "        mode=\"lines+markers\",\n",
    "        showlegend=True,   \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the anomaly traces with the label\n",
    "with_time = False\n",
    "is_xticks = True\n",
    "plot_single = 0 ### give index number of the trace\n",
    "\n",
    "\n",
    "for i in range(plot_single, len(anomaly_traces_lh)):\n",
    "    anomaly_trace = anomaly_traces_lh[i]\n",
    "    anomaly_label = labels[i]\n",
    "    #### plot the trace using plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if with_time:\n",
    "        fig.add_trace(go.Scatter(x=[x[1] for x in anomaly_trace], y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(y=[x[0] for x in anomaly_trace], mode='lines+markers', name='event-trace', marker=dict(size=10, color='midnightblue')))\n",
    "\n",
    "    ### adapt labels for the subtrace which starts from index 0\n",
    "    start_ind = lh_plot_ind[i]   ### we start each trace 50 events before the anomaly starts\n",
    "    end_ind = start_ind + anomaly_label[1]-anomaly_label[0]  ### we end each trace 50 events after the anomaly ends\n",
    "    (start_ts, end_ts) = (anomaly_label[2], anomaly_label[3])\n",
    "    ground_truths_class = anomaly_label[4]\n",
    "\n",
    "    \n",
    "    ### check if time on x-axis\n",
    "    if with_time:\n",
    "        start = start_ts\n",
    "        end = end_ts\n",
    "    else:\n",
    "        start = start_ind\n",
    "        end = end_ind\n",
    "\n",
    "    \n",
    "    # Add dotted lines on the sides of the rectangle\n",
    "    for x in [start, end]:\n",
    "        fig.add_shape(type=\"line\",\n",
    "                xref=\"x\",\n",
    "                yref=\"paper\",\n",
    "                x0=x,\n",
    "                y0=0,\n",
    "                x1=x,\n",
    "                y1=1,\n",
    "                line=dict(\n",
    "                    color='red',\n",
    "                    width=4,\n",
    "                    dash=\"dot\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "    ### generate x ticks with timestamp and index num  \n",
    "    x_data = [x[1] for x in anomaly_trace]\n",
    "    if is_xticks == True and with_time == False:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [k for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == True and with_time == True:\n",
    "        x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "        x_tickvals = [x_data[k] for k in range(0,len(x_data),10)]\n",
    "    elif is_xticks == False:\n",
    "        x_ticks = None\n",
    "        x_tickvals = None\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Event Trace without Time\",\n",
    "        xaxis=dict(\n",
    "            title=\"Number of events\",\n",
    "            rangeslider=dict(visible=True),\n",
    "            type='linear',\n",
    "            tickvals=x_tickvals,\n",
    "            ticktext=x_ticks,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Variables\",\n",
    "            tickvals=[k for k in range(0,len(var_list))],\n",
    "            # ticktext= var_list,\n",
    "            tickfont = dict(size = FONTSIZE),\n",
    "            titlefont = dict(size = FONTSIZE),\n",
    "            color='black',\n",
    "        ),\n",
    "        autosize=True,\n",
    "        width=PLOTWIDTH,\n",
    "        height=PLOTHEIGHT,\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "    )\n",
    "    \n",
    "\n",
    "    fig.update_xaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "\n",
    "    # style all the traces\n",
    "    fig.update_traces(\n",
    "        #hoverinfo=\"name+x+text\",\n",
    "        line={\"width\": 0.5},\n",
    "        marker={\"size\": 8},\n",
    "        mode=\"lines+markers\",\n",
    "        showlegend=True,   \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "\n",
    "- generate anomalous instances using label data - Done\n",
    "- take instances of len 100 with anomaly at different positions in the window. Five positions: partial at start, first half, middle, second half, partial at end\n",
    "\n",
    "1. load the data\n",
    "2. train LOF for novelty detection using 2d features\n",
    "3. test on the anomalous data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "######### Train data ##########\n",
    "train_subtrace_data = pd.read_csv(f'../data-novelty/train_{CODE}_ver{VER}_W100_features_1d.csv') \n",
    "train_standardized_data = StandardScaler().fit_transform(train_subtrace_data) \n",
    "# train_standardized_data = train_subtrace_data\n",
    "np.random.shuffle(train_standardized_data,)     ### shuffle the data\n",
    "\n",
    "x_train = train_standardized_data[:round(len(train_standardized_data)*0.8)]\n",
    "x_val = train_standardized_data[round(len(train_standardized_data)*0.8):]\n",
    "\n",
    "######### Test data ##########\n",
    "test_subtrace_data = pd.read_csv(f'../data-novelty/test_{CODE}_ver{VER}_W100_features_1d_mid.csv')\n",
    "test_standardized_data = StandardScaler().fit_transform(test_subtrace_data)\n",
    "\n",
    "x_test = test_standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train models ######\n",
    "'''\n",
    "(when novelty is set to True). Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.\n",
    "'''\n",
    "lof = LocalOutlierFactor(novelty=True, n_neighbors=35)\n",
    "lof.fit(x_train)\n",
    "\n",
    "clf_svm = OneClassSVM(kernel=\"poly\", degree=5, gamma=0.3, nu=0.1)\n",
    "clf_svm.fit(x_train)\n",
    "\n",
    "iso = IsolationForest(random_state=42)\n",
    "iso.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Test Model ######\n",
    "y_pred = lof.predict(x_test)\n",
    "print(y_pred)\n",
    "y_pred = lof.predict(x_val)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_svm.predict(x_test)\n",
    "print(y_pred)\n",
    "y_pred = clf_svm.predict(x_val)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = iso.predict(x_test)\n",
    "print(y_pred)\n",
    "y_pred = iso.predict(x_val)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
