{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRU Model for ID alone\n",
        "# To predict the next ID in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CODE = 'lora_ducy'               ### application (code) theft_protection, mamba2, lora_ducy\n",
        "BEHAVIOUR_FAULTY = 'faulty_data'        ### normal, faulty_data\n",
        "BEHAVIOUR_NORMAL = 'normal'             ### normal, faulty_data\n",
        "THREAD = 'single'                       ### single, multi\n",
        "VER = 4                                 ### format of data collection\n",
        "\n",
        "base_dir = './trace_data'               ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
        "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
        "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
        "\n",
        "print(\"Normal base path:\", normalbase_path)\n",
        "print(\"Faulty base path:\", faultybase_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
        "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
        "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
        "\n",
        "######### get paths #######################\n",
        "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
        "\n",
        "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
        "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
        "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
        "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
        "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
        "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
        "\n",
        "paths_log.sort()\n",
        "paths_traces.sort()\n",
        "varlist_path.sort()\n",
        "paths_label.sort()\n",
        "\n",
        "test_data_path = paths_traces\n",
        "test_label_path = paths_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check consistency\n",
        "if VER == 3 or VER == 4:\n",
        "    check_con, _ = is_consistent([train_varlist_path[0]] + varlist_path)\n",
        "    if check_con:\n",
        "        to_number = read_json(varlist_path[0])\n",
        "        from_number = mapint2var(to_number)\n",
        "    else:\n",
        "        to_number = read_json(train_varlist_path[0])\n",
        "        from_number = mapint2var(to_number)\n",
        "\n",
        "sorted_keys = list(from_number.keys())\n",
        "sorted_keys.sort()\n",
        "var_list = [from_number[key] for key in sorted_keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "def load_data(file_paths):\n",
        "    data = []\n",
        "    for file in file_paths:\n",
        "        traces = read_traces(file)\n",
        "        if isinstance(traces, list):\n",
        "            id_sequence = [int(trace[0]) for trace in traces]\n",
        "            data.append(id_sequence)\n",
        "    return data\n",
        "\n",
        "train_data = load_data(train_data_path)\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, td in enumerate(train_data):\n",
        "    td_array = np.array(td)  \n",
        "    print(f\"Dataset {idx + 1}: shape = {td_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare LSTM training data\n",
        "sequence_length = 10       # Already tuned parameter value\n",
        "X_train, y_train = [], []\n",
        "for single_file_data in train_data:\n",
        "    for i in range(len(single_file_data) - sequence_length):\n",
        "        X_train.append(single_file_data[i:i + sequence_length])\n",
        "        y_train.append(single_file_data[i + sequence_length])\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "X_train_new = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_val_new = X_val.reshape(-1, X_val.shape[-1])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_new)\n",
        "X_val_scaled = scaler.transform(X_val_new)\n",
        "\n",
        "X_train = X_train_scaled.reshape(X_train.shape)\n",
        "X_val = X_val_scaled.reshape(X_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import psutil\n",
        "\n",
        "# Define LSTM model\n",
        "# Layers 128, 64 and 32 are chosen by parameter tuning\n",
        "model = Sequential([\n",
        "    GRU(128, activation='relu', return_sequences=True, input_shape=(sequence_length, 1), kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.1),\n",
        "    GRU(64, activation='relu', return_sequences=True, kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.1),\n",
        "    GRU(32, activation='relu', return_sequences=False, kernel_regularizer=l2(0.001)),\n",
        "    Dense(1, activation='linear')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(),\n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "\n",
        "# Finding RAM usage\n",
        "ram_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
        "print(f\"Total RAM usage: {ram_usage:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating the MAE and Accuracy\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "pred = model.predict(X_val)\n",
        "mae = mean_absolute_error(y_val, pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "correct = []\n",
        "incorrect = []\n",
        "pred = np.round(pred).astype(int)\n",
        "for i in range(len(y_val)):\n",
        "    yt_event = y_val[i]\n",
        "    pred_event = pred[i]\n",
        "\n",
        "    yt_event = yt_event.reshape(1,)\n",
        "    pred_event = pred_event.reshape(1,)\n",
        "\n",
        "    if np.abs(yt_event - pred_event) < 1:\n",
        "        correct.append(y_val[i])\n",
        "    else:\n",
        "        incorrect.append(y_val[i])\n",
        "\n",
        "\n",
        "accuracy = len(correct) / len(y_val)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # Saving the Model for future use\n",
        "# model_path = './trained_models'\n",
        "# if not os.path.exists(model_path):\n",
        "#     os.makedirs(model_path)\n",
        "\n",
        "# model_path = f'{model_path}/gru_v4_{CODE}.keras'\n",
        "# if not os.path.exists(model_path): \n",
        "#     model.save(model_path)\n",
        "#     print(\"Model saved successfully\")\n",
        "# else:\n",
        "#     print(f\"Model {model_path} exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the scaler\n",
        "import joblib\n",
        "\n",
        "scaler_path = './scalers'\n",
        "if not os.path.exists(scaler_path):\n",
        "    os.makedirs(scaler_path)\n",
        "joblib.dump(scaler, f'{scaler_path}/scaler_gru_id_lora_ducy.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
