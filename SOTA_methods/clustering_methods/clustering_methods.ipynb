{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering methods\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CODE = 'mamba2'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 3                                 ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "print(\"Train base path:\", train_base_path)\n",
    "\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the data and transform into separate files with 50 as sequence length and window size of 1\n",
    "import os\n",
    "def read_process_train_data(file_path, sequence_length, window_size):\n",
    "    count = 0\n",
    "    output_directory = './train_data_processed/'\n",
    "    filename_dictionary = {}\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in file_path:\n",
    "        result = []\n",
    "        print(\"Reading file:\", filename)\n",
    "        count = count + 1\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        event_ids = [item[0] for item in data]\n",
    "        timestamps = [item[1] for item in data]\n",
    "        timestamp_difference = np.diff(timestamps).tolist()\n",
    "        trimmed_event_ids = event_ids[1:]\n",
    "\n",
    "        filename = f'trace_trial{count}' \n",
    "\n",
    "        for i in range(0, len(trimmed_event_ids) - sequence_length + 1, window_size):\n",
    "            event_id_seq_len = trimmed_event_ids[i:i+sequence_length]\n",
    "            timestamp_diff_seq_len = timestamp_difference[i:i+sequence_length]\n",
    "            # result = [event_id_seq_len, timestamp_diff_seq_len]\n",
    "            result = [event_id_seq_len]\n",
    "\n",
    "            index_start = i\n",
    "            index_end = i + sequence_length\n",
    "            new_filename = f'{filename}_{index_start}-{index_end}.json'\n",
    "            output_path = os.path.join(output_directory, new_filename)\n",
    "\n",
    "            #Saving the file\n",
    "            with open(output_path, 'w') as out_file:\n",
    "                json.dump(result, out_file)\n",
    "            \n",
    "            #Saving to dictionary for easy backtracking\n",
    "            filename_dictionary[new_filename] = {\n",
    "                \"source_file\": filename,\n",
    "                \"data_start_index\": index_start,\n",
    "                \"data_end_index\": index_end,\n",
    "                \"sequence_length\": sequence_length,\n",
    "                \"window_size\": window_size\n",
    "            }\n",
    "    \n",
    "    dictionary_path = f'{output_directory}/file_dict/'\n",
    "    if not os.path.exists(dictionary_path):\n",
    "        os.makedirs(dictionary_path)\n",
    "    \n",
    "    dict_path = os.path.join(dictionary_path,'filename_dict.json')\n",
    "\n",
    "    with open(dict_path, 'w') as track_dict:\n",
    "        json.dump(filename_dictionary, track_dict, indent=2)\n",
    "    \n",
    "    print(\"Filename Dictionary saved to :\", dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the data\n",
    "sequence_length = 50\n",
    "window_size = 1\n",
    "train_data_processed = read_process_train_data(train_data_path,sequence_length,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def load_data(file_path):\n",
    "    traces = []\n",
    "    filenames = []\n",
    "    for file in os.listdir(file_path):\n",
    "        if file.endswith('.json') and not file.startswith('filename_dict'):\n",
    "            with open(os.path.join(file_path, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                traces.append(data)\n",
    "                filenames.append(file)\n",
    "    return traces, filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data_path = './train_data_processed/'\n",
    "traces, files = load_data(processed_train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(traces))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries.anomaly_detection import extract_features_seglearn\n",
    "features_df = extract_features_seglearn(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_features(df, n_clusters):\n",
    "    kmeans = KMeans(n_clusters,init=\"k-means++\", max_iter=300, n_init=30, random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    labels = kmeans.labels_\n",
    "    print(\"kmeans:\", labels)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def find_n_clusters_value(data, min_k, max_k):\n",
    "    best_k = min_k\n",
    "    best_score = -1\n",
    "\n",
    "    for k in range(min_k, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    \n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_df_clean = features_df.dropna()\n",
    "valid_index = features_df_clean.index\n",
    "filtered_files = [files[i] for i in valid_index]\n",
    "\n",
    "X_scaled = scaler.fit_transform(features_df_clean)\n",
    "\n",
    "N_Clusters = find_n_clusters_value(X_scaled, min_k=6, max_k=10)\n",
    "print(\"\\n optimal number of clusters: \", N_Clusters)\n",
    "\n",
    "kmeans_model = cluster_features(pd.DataFrame(X_scaled), N_Clusters)\n",
    "features_df_clean['cluster'] = kmeans_model.labels_\n",
    "features_df_clean['file'] = filtered_files\n",
    "\n",
    "train_features = X_scaled\n",
    "train_labels = kmeans_model.labels_\n",
    "\n",
    "\n",
    "\n",
    "model_save_path = './trained_model/'\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "joblib.dump(kmeans_model, f'{model_save_path}kmeans_model.pkl')\n",
    "\n",
    "joblib.dump(train_features, f'{model_save_path}train_features.pkl')\n",
    "joblib.dump(train_labels, f'{model_save_path}train_clusters.pkl')\n",
    "\n",
    "# Saving the scaler\n",
    "scaler_save_path = './scalers/'\n",
    "if not os.path.exists(scaler_save_path):\n",
    "    os.makedirs(scaler_save_path)\n",
    "joblib.dump(scaler, f'{scaler_save_path}scaler.pkl')\n",
    "\n",
    "\n",
    "print(\"\\n Cluster assignments\")\n",
    "print(features_df_clean[['file', 'cluster']])\n",
    "\n",
    "# features_df.to_csv(\"clustered_features_seglearn.csv\", index=False)\n",
    "# print(\"\\n file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the clusters\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=features_df_clean['cluster'], cmap='tab10', s=50)\n",
    "plt.title(\"K Means(PCA)\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test data\n",
    "from libraries.anomaly_detection import test_single_for_clustering, merge_detections, get_correct_detections\n",
    "\n",
    "## checking the detections against the ground truth\n",
    "DIFF_VAL = 5 \n",
    "all_detections = []         # To store detections for each file\n",
    "y_pred_all = []             # To store the predicted labels\n",
    "y_true_all = []             # To store the ground truth labels\n",
    "all_tp = []                 # To store all true positives\n",
    "all_fp = []                 # To store all false positives\n",
    "all_fn = []                 # To store all false negatives\n",
    "all_gt = []                 # To store the ground truth\n",
    "sequence_length = 50                                                # Sequence length for the model\n",
    "\n",
    "# Loading the model and scaler\n",
    "model_path = f'{model_save_path}kmeans_model.pkl'\n",
    "loaded_kmeans_model = joblib.load(model_path)\n",
    "\n",
    "scaler_path = f'{scaler_save_path}scaler.pkl'\n",
    "loaded_scaler = joblib.load(scaler_path) \n",
    "\n",
    "\n",
    "trained_features_path = f'{model_save_path}train_features.pkl'\n",
    "trained_clusters_path = f'{model_save_path}train_clusters.pkl'\n",
    "trained_features = joblib.load(trained_features_path)\n",
    "trained_cluster_labels = joblib.load(trained_clusters_path)\n",
    "print(trained_features)\n",
    "print(trained_cluster_labels)\n",
    "\n",
    "# Iterating through each test data file and label file\n",
    "for test_data, test_label in zip(test_data_path, test_label_path):\n",
    "    detection, inference_time = test_single_for_clustering(test_data,sequence_length,trained_features, trained_cluster_labels,loaded_scaler)            # Detecting anomalies in the test data\n",
    "    print(\"Detection : \", detection)\n",
    "\n",
    "    print(\"len(detection) : \", len(detection))\n",
    "\n",
    "    merge_detection, agg_ts = merge_detections(detection, diff_val=DIFF_VAL)\n",
    "\n",
    "    print(\"Merge detection : \", merge_detection)\n",
    "    \n",
    "    ground_truth_raw = read_traces(test_label)                                               # read ground truth labels from the label file\n",
    "    ground_truth = ground_truth_raw['labels']                                                # extract labels from dictionary from ground truth data\n",
    "\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "\n",
    "    correct_pred, rest_pred, y_pred, y_true, false_neg = get_correct_detections(merge_detection, ground_truth)  # Comparing detected anomaly with ground truth\n",
    "\n",
    "    y_pred_all.extend(y_pred)          # predicted labels\n",
    "    y_true_all.extend(y_true)          # actual ground truth labels\n",
    "    all_detections.append((test_data, merge_detection, test_label))\n",
    "    all_tp.append((test_data, correct_pred, test_label))\n",
    "    all_fp.append((test_data, rest_pred, test_label))\n",
    "    all_fn.append((test_data, false_neg, test_label))\n",
    "    all_gt.append((test_data, ground_truth, test_label))\n",
    "\n",
    "    print(\"Inference time : \", inference_time)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "y_true_all = np.array(y_true_all)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Normal', 'Anomaly'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
