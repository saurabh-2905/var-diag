{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_sample(file_path):\n",
    "        data = np.load(file_path, allow_pickle=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_traces(log_path):\n",
    "    '''\n",
    "    read the trace files and extract variable names\n",
    "    data = [ [event, timestamp], [], [],......,[] ]\n",
    "    '''\n",
    "    with open(log_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    '''\n",
    "    calculate the confidence interval of the data\n",
    "    data: a list of execution intervals -> [1,2,3,4,5,6,7,8,9,10]\n",
    "    '''\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(n)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    return start, end\n",
    "\n",
    "def get_uniquevar(raw_trace):\n",
    "    ''' \n",
    "    convert the v2.2 trace into list of unique variables\n",
    "    raw_trace = data from read_traces, list( (var, ts),(var, ts),(var, ts),.... )\n",
    "    return:\n",
    "        unique_var = list(var1,var2,...) ## list of strings\n",
    "    '''\n",
    "    unique_var = []\n",
    "    for rt in raw_trace:\n",
    "        [var, timestamp] = rt\n",
    "        # print([var, timestamp])\n",
    "        if var not in unique_var:\n",
    "            unique_var += [var]\n",
    "            # print(rt)\n",
    "    return unique_var\n",
    "\n",
    "\n",
    "def generate_map(unique_events):\n",
    "    '''\n",
    "    unique_events -> list of all the variables in the code (unique, and in order of logging)\n",
    "    return:\n",
    "        event_map -> takes the variable name and gives corresponding event number\n",
    "        event_remap -> takes event number and gives associated variable name\n",
    "    '''\n",
    "    event_map = dict()\n",
    "    event_remap = dict()\n",
    "    for i in range(len(unique_events)):\n",
    "        event_remap[i+1] = unique_events[i]\n",
    "        event_map[unique_events[i]] = i+1\n",
    "\n",
    "    return(event_map, event_remap)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "code = 'theft_protection'       ### application (code)\n",
    "behaviour = 'faulty_data'            ### normal, faulty_data\n",
    "thread_typ = 'single'           ### single, multi\n",
    "version = 2.2                     ### format of data collection\n",
    "sub_len = 50\n",
    "\n",
    "# base_dir = '../data-subtraces' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "log_path = base_dir+f'/{code}/{thread_typ}_thread/version_{version}'\n",
    "print(log_path)\n",
    "normal_path = log_path+f'/normal'\n",
    "anomalies_path = log_path+f'/faulty_data'\n",
    "print(normal_path, anomalies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confidence Interval:__\n",
    "\n",
    "A confidence interval is a range around the mean that is likely to contain the true population mean. The formula for a confidence interval is mean ± margin of error mean±margin of error, where the margin of error depends on the desired confidence level and the standard error.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "1. Choose a confidence level (e.g., 95%).\n",
    "2. Calculate the standard error: standard deviation/ sqr_root(number of observations)\n",
    "3. Calculate the margin of error: critical value × standard error\n",
    "4. Determine the confidence interval: mean ± margin of error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### get file paths #######\n",
    "\n",
    "### normal files\n",
    "normal_files = os.listdir(normal_path)\n",
    "normal_files.sort()\n",
    "logs = []\n",
    "traces = []\n",
    "unknown = []\n",
    "for i in normal_files:\n",
    "    if i.find('log') == 0:\n",
    "        logs += [i]\n",
    "    elif i.find('trace') == 0 and i.find('.txt') == -1:\n",
    "        traces += [i]\n",
    "    else:\n",
    "        unknown += [i]\n",
    "\n",
    "######### path to files\n",
    "normal_logpaths = [os.path.join(normal_path, x) for x in logs]\n",
    "normal_tracespaths = [os.path.join(normal_path, x) for x in traces]\n",
    "normal_logpaths.sort()\n",
    "normal_tracespaths.sort()\n",
    "print(normal_tracespaths)\n",
    "\n",
    "### anomalies files\n",
    "anomalies_files = os.listdir(anomalies_path)\n",
    "anomalies_files.sort()\n",
    "logs = []\n",
    "traces = []\n",
    "unknown = []\n",
    "for i in anomalies_files:\n",
    "    if i.find('log') == 0:\n",
    "        logs += [i]\n",
    "    elif i.find('trace') == 0 and i.find('.txt') == -1:\n",
    "        traces += [i]\n",
    "    else:\n",
    "        unknown += [i]\n",
    "        \n",
    "######### path to files\n",
    "anomalies_logpaths = [os.path.join(anomalies_path, x) for x in logs]\n",
    "anomalies_tracespaths = [os.path.join(anomalies_path, x) for x in traces]\n",
    "anomalies_logpaths.sort()\n",
    "anomalies_tracespaths.sort()\n",
    "print(anomalies_tracespaths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get execution intervals for all variables\n",
    "exe_list = {}   ### {var1: [1,2,3,4,5,6,7,8,9,10], var2: [1,2,3,4,5,6,7,8,9,10], ....}\n",
    "filewise_exe_list = {}   ### {file1: {var1: [1,2,3,4,5,6,7,8,9,10], var2: [1,2,3,4,5,6,7,8,9,10], ....}, file2: {var1: [1,2,3,4,5,6,7,8,9,10], var2: [1,2,3,4,5,6,7,8,9,10], ....}, ....}\n",
    "for sample_path in normal_tracespaths:\n",
    "    sample_data = read_traces(sample_path)\n",
    "    filename = sample_path.split('/')[-1]\n",
    "    # print(sample_data)\n",
    "    ### collect timestamps for all variables\n",
    "    timestamps = {}\n",
    "    for i, event in enumerate(sample_data):\n",
    "        var, ts = event\n",
    "        ts = int(ts)\n",
    "        # print(var, ts)\n",
    "        if var not in timestamps.keys():\n",
    "            timestamps[var] = [ts]\n",
    "        else:\n",
    "            timestamps[var].append(ts)\n",
    "\n",
    "    ### calculate execution intervals for all variables\n",
    "    intervals = {}\n",
    "    for key in timestamps.keys():\n",
    "        ts_list = timestamps[key]\n",
    "        for ts1, ts2 in zip(ts_list[:-1], ts_list[1:]):\n",
    "            exe_time = ts2 - ts1\n",
    "            ### convert timestampt from miliseconds to seconds, and only consdider 1 decimal point. \n",
    "            exe_time = round(exe_time/1000, 1)\n",
    "            if key not in exe_list.keys():\n",
    "                exe_list[key] = [exe_time]\n",
    "                intervals[key] = [exe_time]\n",
    "            else:\n",
    "                exe_list[key].append(exe_time)\n",
    "                intervals[key].append(exe_time)\n",
    "\n",
    "    filewise_exe_list[filename] = intervals\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the confidence intervals for all variables\n",
    "\n",
    "confidence_intervals = {}\n",
    "for key in exe_list.keys():\n",
    "    data = exe_list[key]\n",
    "    start, end = calculate_confidence_interval(data)\n",
    "    confidence_intervals[key] = [start, end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ calculate upper and lower thrsholds for all variables ############\n",
    "\n",
    "### get uniques values from exe_list\n",
    "unique_values = {}\n",
    "outliers = {}\n",
    "for key in exe_list.keys():\n",
    "    data = exe_list[key]\n",
    "    unique_values[key] = list(set(data))\n",
    "    ### calculate probability for each unique value\n",
    "    prob = {}\n",
    "    for val in unique_values[key]:\n",
    "        prob[val] = data.count(val)/len(data)\n",
    "    unique_values[key] = prob\n",
    "\n",
    "### consider values with probability > 0.05\n",
    "outliers[key] = dict()\n",
    "for key in unique_values.keys():\n",
    "    print(key)\n",
    "    prob = unique_values[key]\n",
    "    print(prob.keys())\n",
    "    filtered_values = defaultdict(list)\n",
    "    out = dict()\n",
    "    for val in prob.keys():\n",
    "        print(prob[val])\n",
    "        if prob[val] > 0.05:    \n",
    "            filtered_values[val] = prob[val]\n",
    "        else:\n",
    "            out[val] = prob[val]\n",
    "\n",
    "\n",
    "    unique_values[key] = filtered_values\n",
    "    outliers[key] = out\n",
    "\n",
    "\n",
    "### get upper and lower bound by taking min and max from unique_values (can try some other approach)\n",
    "thresholds = {}\n",
    "for key in unique_values.keys():\n",
    "    values = list(unique_values[key].keys())\n",
    "    thresholds[key] = [round(min(values)-0.2, 1), round(max(values)+0.2, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds['1_0_main_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot exe_list to vsiualize the distribution of execution intervals\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set(style=\"whitegrid\")\n",
    "\n",
    "# for key in exe_list.keys():\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.title(key)\n",
    "#     sns.distplot(exe_list[key], bins=100, kde=False, rug=True)\n",
    "#     plt.axvline(confidence_intervals[key][0], color='r', linestyle='--')\n",
    "#     plt.axvline(confidence_intervals[key][1], color='r', linestyle='--')\n",
    "#     plt.axvline(min(thresholds[key]), color='g', linestyle='--')\n",
    "#     plt.axvline(max(thresholds[key]), color='g', linestyle='--')\n",
    "#     plt.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "for key in exe_list.keys():\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Histogram\n",
    "    fig.add_trace(go.Histogram(x=exe_list[key], nbinsx=100, name='execution intervals', histnorm='probability', marker=dict(color='midnightblue')))\n",
    "\n",
    "    # Vertical lines\n",
    "    fig.add_shape(type=\"line\", x0=confidence_intervals[key][0], x1=confidence_intervals[key][0], y0=0, y1=1, yref='paper', line=dict(color=\"Red\", dash=\"dash\"))\n",
    "    fig.add_shape(type=\"line\", x0=confidence_intervals[key][1], x1=confidence_intervals[key][1], y0=0, y1=1, yref='paper', line=dict(color=\"Red\", dash=\"dash\"))\n",
    "    fig.add_shape(type=\"line\", x0=min(thresholds[key]), x1=min(thresholds[key]), y0=0, y1=1, yref='paper', line=dict(color=\"Green\", dash=\"dash\"))\n",
    "    fig.add_shape(type=\"line\", x0=max(thresholds[key]), x1=max(thresholds[key]), y0=0, y1=1, yref='paper', line=dict(color=\"Green\", dash=\"dash\"))\n",
    "\n",
    "    # Add traces for the lines to include them in the legend\n",
    "    fig.add_trace(go.Scatter(x=[confidence_intervals[key][0]], y=[0], mode='lines', name='Confidence Interval', line=dict(color=\"Red\", dash=\"dash\"), showlegend=True))\n",
    "    fig.add_trace(go.Scatter(x=[min(thresholds[key])], y=[0], mode='lines', name='Dynamic Threshold', line=dict(color=\"Green\", dash=\"dash\"), showlegend=True))\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(title=key, xaxis_title=\"Value\", yaxis_title=\"Count\", bargap=0.2, bargroupgap=0.1, title_font_size=20,\n",
    "                        xaxis=dict(\n",
    "                            tickfont = dict(size = 20),\n",
    "                            titlefont = dict(size = 20),\n",
    "                            color='black',\n",
    "                        ),\n",
    "                        yaxis=dict(\n",
    "                            tickfont = dict(size = 20),\n",
    "                            titlefont = dict(size = 20),\n",
    "                            color='black'\n",
    "                        ),\n",
    "                        plot_bgcolor='rgba(0,0,0,0)',)\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect anomalies in faulty traces\n",
    "detected_anomalies = []\n",
    "for sample_path in anomalies_tracespaths:\n",
    "    sample_data = read_traces(sample_path)\n",
    "    filename = sample_path.split('/')[-1]\n",
    "\n",
    "    ### iterate trace and make decision for each exe interval\n",
    "    var_tracking = {}\n",
    "    for i in range(len(sample_data)):\n",
    "        event = sample_data[i]\n",
    "        var, ts = event\n",
    "        ts = int(ts)\n",
    "        if var not in var_tracking.keys():\n",
    "            var_tracking[var] = [ts]\n",
    "        else:\n",
    "            var_tracking[var].append(ts)\n",
    "\n",
    "        ### calculate exe interval\n",
    "        if len(var_tracking[var]) > 1:\n",
    "            exe_time = var_tracking[var][-1] - var_tracking[var][-2]\n",
    "            ### convert timestampt from miliseconds to seconds, and only consdider 1 decimal point. \n",
    "            exe_time = round(exe_time/1000, 1)\n",
    "\n",
    "            ### check if exe_time is an outlier\n",
    "            if exe_time < thresholds[var][0] or exe_time > thresholds[var][1]:\n",
    "                print(f'Anomaly detected for {var} in {filename} at {i}th event')\n",
    "                print(f'Execution interval: {exe_time}')\n",
    "                detected_anomalies += [[(var, var_tracking[var][-2]), (var, var_tracking[var][-1]), os.path.basename(sample_path)]]\n",
    "\n",
    "                # break\n",
    "\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Visualize the anomalies ############\n",
    "###############################################\n",
    "\n",
    "### prepare the traces for plotting\n",
    "\n",
    "### get variable list that is same for all log files ####\n",
    "raw_trace = read_traces(normal_tracespaths[0])\n",
    "_var_list = get_uniquevar(raw_trace)\n",
    "#np.save('var_list.npy', _var_list, allow_pickle=False)\n",
    "to_number, from_number = generate_map(_var_list)\n",
    "\n",
    "\n",
    "########## process the traces ###########\n",
    "col_data = []\n",
    "for p in anomalies_tracespaths:\n",
    "    trace = read_traces(p)\n",
    "    w = os.path.basename(p).removesuffix('.txt')\n",
    "    num_trace = []\n",
    "    time_stamp = []\n",
    "    for (t, ts) in trace:\n",
    "        nt = to_number[t]\n",
    "        num_trace.extend([nt])\n",
    "        time_stamp.extend([ts])\n",
    "        # ### take limited samples\n",
    "        # if ts > 250000:\n",
    "        #     break\n",
    "    col_data += [(w, time_stamp, num_trace, _var_list, p)]   ### in the format (trace_name, x_data, y_data, y_labels, trace_path) \n",
    "\n",
    "\n",
    "all_df = []\n",
    "for col in col_data:\n",
    "    # print(col)\n",
    "    plot_data = dict()\n",
    "    plot_data['time'] = col[1]   ### x_data\n",
    "    plot_data[col[0]] = col[2]   ### y_data (traces)\n",
    "\n",
    "    ### convert the list to data frame and store it for plotting\n",
    "    df = pd.DataFrame(plot_data)\n",
    "    all_df += [df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### prepare anomalies for plotting ########\n",
    "\n",
    "### seperate the anomalies accoring to the trace files and store the timestamps in seperate lists\n",
    "anomalies = defaultdict(list)\n",
    "for anomaly in detected_anomalies:\n",
    "    # print(anomaly)\n",
    "    file_name = anomaly[2]\n",
    "    anomalies[file_name] += [anomaly[0][1], anomaly[1][1]]\n",
    "\n",
    "### sort the anomalies according to the timestamp and remove duplicates\n",
    "for k in anomalies.keys():\n",
    "    anomalies[k] = sorted(list(set(anomalies[k])))\n",
    "\n",
    "### get the index number for timestamps in the traces to plot\n",
    "anomalies_df = []\n",
    "for i in range(len(all_df)):\n",
    "    df = all_df[i]\n",
    "    k = df.columns[1]   ### get name of the trace\n",
    "    #print(k)\n",
    "    anomalies_plot = defaultdict(list)\n",
    "    for ts in anomalies[k]:\n",
    "        #print(ts)\n",
    "        index = df.index[df['time'] == ts].tolist()\n",
    "        ### store the respective values from the trace columns\n",
    "        y_val = df[k].iloc[index[0]]\n",
    "        #print(index, y_val)\n",
    "        anomalies_plot['index'] += index\n",
    "        anomalies_plot[k+'-anomalies'] += [y_val]\n",
    "\n",
    "    ### convert the dict to data frame and store it for plotting\n",
    "    df = pd.DataFrame(anomalies_plot)\n",
    "    anomalies_df += [df]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot the traces with anomalies #####\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### add all the traces to the graph\n",
    "for df, dfa in zip(all_df, anomalies_df):\n",
    "\n",
    "    # ### plot only one graph\n",
    "    # trace_toplot = 1\n",
    "    # df = all_df[trace_toplot]\n",
    "    # dfa = anomalies_df[trace_toplot]\n",
    "    # ############################\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    df_col = df.columns\n",
    "    fig.add_trace(\n",
    "                go.Scatter(y=list(df[df_col[1]]), name=df_col[1], marker=dict(size=10, color='midnightblue')),   ### equivalent to: y=list(df['trace1'])\n",
    "                )\n",
    "    ### add anomalies to the graph without lines\n",
    "    for (ax, ay) in zip(dfa['index'], dfa[df_col[1]+'-anomalies']):\n",
    "        fig.add_trace(\n",
    "                go.Scatter(x=[ax], y=[ay], name=df_col[1]+'-anomalies', mode='markers', marker=dict(size=50, color='red', symbol='square'), showlegend=False),\n",
    "                )\n",
    "    \n",
    "    \n",
    "    # break\n",
    "    ### generate x ticks with timestamp and index num  \n",
    "    x_data = df[df_col[0]]\n",
    "    x_ticks = [(i,x_data[i]) for i in range(0,len(x_data),10) ]\n",
    "\n",
    "    # Add range slider, title, yticks, axes labels\n",
    "    fig.update_layout(\n",
    "        title_text=\"Event Trace without Time\",\n",
    "        xaxis=dict(\n",
    "            title=\"Number of events\",\n",
    "            rangeslider=dict(visible=True),\n",
    "            type='linear',\n",
    "            # tickvals=[k for k in range(0,len(x_data),10)],\n",
    "            # ticktext=x_ticks,\n",
    "            tickfont = dict(size = 20),\n",
    "            titlefont = dict(size = 20),\n",
    "            color='black',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Variables\",\n",
    "            tickvals=[k for k in range(1,len(_var_list)+1)],\n",
    "            ticktext=_var_list,\n",
    "            tickfont = dict(size = 20),\n",
    "            titlefont = dict(size = 20),\n",
    "            color='black',\n",
    "        ),\n",
    "        autosize=True,\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        mirror=True,\n",
    "        ticks='outside',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgrey'\n",
    "    )\n",
    "    \n",
    "    # style all the traces\n",
    "    fig.update_traces(\n",
    "        #hoverinfo=\"name+x+text\",\n",
    "        line={\"width\": 0.5},\n",
    "        marker={\"size\": 8},\n",
    "        mode=\"lines+markers\",\n",
    "        showlegend=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Evaluation ############\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ plot the confusion matrix ############\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create the labels for the Confusion Matrix\n",
    "labels = ['Normal', 'Anomaly']\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame for the Confusion Matrix\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Create the Confusion Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "#sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\n",
    "sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g', annot_kws={\"size\": 24})  # Change font size\n",
    "\n",
    "# increase label font size\n",
    "plt.yticks(fontsize=22)\n",
    "plt.xticks(fontsize=22)\n",
    "\n",
    "# increase title font size\n",
    "\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=22)\n",
    "plt.ylabel('True Label', fontsize=22)\n",
    "plt.xlabel('Predicted Label', fontsize=22)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('confusion_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
