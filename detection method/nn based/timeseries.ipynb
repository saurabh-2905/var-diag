{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_sample(file_path):\n",
    "        data = np.load(file_path, allow_pickle=False)\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_traces(log_path):\n",
    "    '''\n",
    "    read the trace files and extract variable names\n",
    "    data = [ [event, timestamp], [], [],......,[] ]\n",
    "    '''\n",
    "    with open(log_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    '''\n",
    "    calculate the confidence interval of the data\n",
    "    data: a list of execution intervals -> [1,2,3,4,5,6,7,8,9,10]\n",
    "    '''\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(n)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    return start, end\n",
    "\n",
    "def get_uniquevar(raw_trace):\n",
    "    ''' \n",
    "    convert the v2.2 trace into list of unique variables\n",
    "    raw_trace = data from read_traces, list( (var, ts),(var, ts),(var, ts),.... )\n",
    "    return:\n",
    "        unique_var = list(var1,var2,...) ## list of strings\n",
    "    '''\n",
    "    unique_var = []\n",
    "    for rt in raw_trace:\n",
    "        [var, timestamp] = rt\n",
    "        # print([var, timestamp])\n",
    "        if var not in unique_var:\n",
    "            unique_var += [var]\n",
    "            # print(rt)\n",
    "    return unique_var\n",
    "\n",
    "\n",
    "def generate_map(unique_events):\n",
    "    '''\n",
    "    unique_events -> list of all the variables in the code (unique, and in order of logging)\n",
    "    return:\n",
    "        event_map -> takes the variable name and gives corresponding event number\n",
    "        event_remap -> takes event number and gives associated variable name\n",
    "    '''\n",
    "    event_map = dict()\n",
    "    event_remap = dict()\n",
    "    for i in range(len(unique_events)):\n",
    "        event_remap[i+1] = unique_events[i]\n",
    "        event_map[unique_events[i]] = i+1\n",
    "\n",
    "    return(event_map, event_remap)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "code = 'theft_protection'       ### application (code)\n",
    "behaviour = 'faulty_data'            ### normal, faulty_data\n",
    "thread_typ = 'single'           ### single, multi\n",
    "version = 2.2                     ### format of data collection\n",
    "sub_len = 'dynamic'             ### length of subtraces\n",
    "\n",
    "base_dir = '../data-subtraces' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normal_path = base_dir+f'/version_{version}/{behaviour}/subtraces/50/normal'    ### train on normal samples of length 50\n",
    "# anomalies_path = base_dir+f'/version_{version}/{behaviour}/subtraces/{sub_len}/anomalies'\n",
    "# print(normal_path, anomalies_path)\n",
    "\n",
    "#### subtraces\n",
    "subtrace_path = f\"../data-subtraces/version_{version}/{behaviour}/subtraces/{sub_len}/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get the Data ############\n",
    "'''\n",
    "unlike thresholding and state machine, we work with subtraces, where only sequence with correct behaviour is considered. \n",
    "This is due to the fact that, we dont need to calculate and features which needs larger traces. Also, dividing the traces in subtraces allows us to remove the anomalies in the normal trace. \n",
    "'''\n",
    "\n",
    "normal_files = os.listdir(normal_path)\n",
    "if '.DS_Store' in normal_files:\n",
    "    normal_files.remove('.DS_Store')\n",
    "\n",
    "anomalies_files = os.listdir(subtrace_path)\n",
    "for af in anomalies_files:\n",
    "    if af.find('trace') == -1:\n",
    "        anomalies_files.remove(af)\n",
    "\n",
    "normal_files = [os.path.join(normal_path, x) for x in normal_files]\n",
    "anomalies_files = [os.path.join(subtrace_path, x) for x in anomalies_files]\n",
    "\n",
    "print(normal_files)\n",
    "print(anomalies_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert subtraces into sequences\n",
    "def create_sequences(subtraces, sequence_length, labels=None):\n",
    "    if labels is not None:\n",
    "        assert len(subtraces) == len(labels)\n",
    "\n",
    "    if labels == None:\n",
    "        sequences = []\n",
    "        for subtrace in subtraces:\n",
    "            for i in range(len(subtrace) - sequence_length + 1):\n",
    "            # for i in range(0,len(subtrace) - sequence_length + 1, sequence_length-1):\n",
    "                sequence = subtrace[i:i+sequence_length]\n",
    "                sequence = [ x[0] for x in sequence]\n",
    "                sequences.append(sequence)\n",
    "        return np.array(sequences)\n",
    "    else:\n",
    "        sequences = []\n",
    "        sequence_labels = []\n",
    "        for subtrace, label in zip(subtraces, labels):\n",
    "            for i in range(len(subtrace) - sequence_length + 1):\n",
    "                sequence = subtrace[i:i+sequence_length]\n",
    "                sequence = [ x[0] for x in sequence]\n",
    "                sequences.append(sequence)\n",
    "                lab_seq = label[i:i+sequence_length]\n",
    "                # print(lab_seq)\n",
    "                if sum(lab_seq) > 0:\n",
    "                    # print(lab_seq)\n",
    "                    sequence_labels.append(1)\n",
    "                else:\n",
    "                    sequence_labels.append(0)\n",
    "        return np.array(sequences), np.array(sequence_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read subtraces\n",
    "normal_subtraces = []    ### fromat -> [ [[var1, ts1], [var2, ts2] ], [[var1, ts1], [var2, ts2] ], [[var1, ts1], [var2, ts2] ],.... ]\n",
    "for nf in normal_files:\n",
    "    data = load_sample(nf)\n",
    "    normal_subtraces += [data]\n",
    "\n",
    "### read labels\n",
    "label_path = os.path.join(subtrace_path, 'labels')\n",
    "label_files = os.listdir(label_path)\n",
    "label_files = [ os.path.join(label_path,i) for i in label_files if i.find('labels') != -1]   ### select only label files '.json'\n",
    "# print(label_files)\n",
    "\n",
    "### load data and labels\n",
    "anomalies_subtraces = []\n",
    "anomalies_labels = []\n",
    "for af in anomalies_files:\n",
    "    print(af)\n",
    "    label_file = af.split('/')[-1].split('.')[0] + '_labels.json'\n",
    "    label_file = os.path.join(label_path, label_file)\n",
    "    # print(label_file)\n",
    "    \n",
    "    data = read_traces(af)\n",
    "\n",
    "    with open(label_file) as f:\n",
    "        labels_index = json.load(f)\n",
    "\n",
    "    # print(labels_index)\n",
    "    anomalies_subtraces += [data]\n",
    "    anomalies_labels += [labels_index]   ### the index of the anomalies in the subtrace from excel sheet\n",
    "\n",
    "### generate raw ground truth\n",
    "raw_ground_truth = []\n",
    "for i in range(len(anomalies_subtraces)):\n",
    "    data = anomalies_subtraces[i]\n",
    "    labels = anomalies_labels[i]\n",
    "    # print(labels)\n",
    "    print(len(data))\n",
    "    print(len(labels))\n",
    "    rgt = [0]*len(data)\n",
    "    ### groundtruth = 1 if anomaly\n",
    "    for ei in labels:\n",
    "        # print(ei)\n",
    "        # print(ei)\n",
    "        rgt[ei] = 1\n",
    "\n",
    "    # print(rgt)\n",
    "    raw_ground_truth += [rgt]\n",
    "\n",
    "### replace the variable names with event numbers based on var_list.npy\n",
    "event_map, event_remap = generate_map(np.load('../../analysis scripts/var_list.npy', allow_pickle=True))\n",
    "# print(event_map)\n",
    "\n",
    "for i in range(len(normal_subtraces)):\n",
    "    for j in range(len(normal_subtraces[i])):\n",
    "        # print(i,j)\n",
    "        # print(normal_subtraces[i][j])\n",
    "        normal_subtraces[i][j][0] = event_map[normal_subtraces[i][j][0]]\n",
    "\n",
    "for i in range(len(anomalies_subtraces)):\n",
    "    for j in range(len(anomalies_subtraces[i])):\n",
    "        anomalies_subtraces[i][j][0] = event_map[anomalies_subtraces[i][j][0]]\n",
    "\n",
    "# # Find the length of the longest sequence\n",
    "# max_len = max(len(seq) for seq in normal_subtraces)\n",
    "\n",
    "# # remove the sequences with length less than max_len\n",
    "# normal_subtraces = [seq for seq in normal_subtraces if len(seq) == max_len]\n",
    "# anomalies_subtraces = [seq for seq in anomalies_subtraces if len(seq) == max_len]\n",
    "\n",
    "# Normalize the data separately for 'normal' and 'anomalies' subtraces\n",
    "scaler_normal = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_anomalies = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "normal_subtraces_normalized = [None]*len(normal_subtraces)\n",
    "for i in range(len(normal_subtraces)):\n",
    "    normal_subtraces_normalized[i] = scaler_normal.fit_transform(normal_subtraces[i])\n",
    "\n",
    "anomalies_subtraces_normalized = [None]*len(anomalies_subtraces)\n",
    "for i in range(len(anomalies_subtraces)):\n",
    "    anomalies_subtraces_normalized[i] = scaler_anomalies.fit_transform(anomalies_subtraces[i])\n",
    "\n",
    "\n",
    "# Create sequences with a specified sequence length for 'normal' subtraces\n",
    "sequence_length = 10  # Adjust based on your data\n",
    "\n",
    "# Create sequences for 'normal' subtraces\n",
    "sequences_normal = create_sequences(normal_subtraces_normalized, sequence_length)\n",
    "\n",
    "# # Split 'normal' sequences into training and testing sets\n",
    "# train_size_normal = int(len(sequences_normal) * 0.8)\n",
    "# train_normal, test_normal = sequences_normal[0:train_size_normal], sequences_normal[train_size_normal:]\n",
    "\n",
    "train_normal = sequences_normal\n",
    "\n",
    "# Create sequences for 'anomalies' subtraces\n",
    "sequences_anomalies, sequences_anomalies_labels = create_sequences(anomalies_subtraces_normalized, sequence_length, raw_ground_truth)\n",
    "\n",
    "# Split 'anomalies' sequences into testing set (no training on anomalies)\n",
    "test_anomalies = sequences_anomalies\n",
    "\n",
    "# generate ground truth for test_anomalies\n",
    "test_anomalies_labels = sequences_anomalies_labels\n",
    "\n",
    "# Prepare input and output data for the LSTM model\n",
    "X_train, y_train = train_normal[:, :-1], train_normal[:, -1]\n",
    "\n",
    "# # Concatenate 'normal' and 'anomalies' testing sets\n",
    "# X_test = np.concatenate((test_normal[:, :-1], test_anomalies[:, :-1]), axis=0)\n",
    "# y_test = np.concatenate((test_normal[:, -1], test_anomalies[:, -1]), axis=0)\n",
    "\n",
    "# # Concatenate 'normal' and 'anomalies' ground truth\n",
    "# y_test_labels = np.concatenate((test_normal_labels, test_anomalies_labels), axis=0)\n",
    "\n",
    "\n",
    "X_test = test_anomalies[:, :-1]\n",
    "y_test = test_anomalies[:, -1]\n",
    "y_test_labels = test_anomalies_labels\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the training set\n",
    "shuffle_indices_train = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffle_indices_train], y_train[shuffle_indices_train]\n",
    "\n",
    "# # Shuffle the testing set\n",
    "# shuffle_indices_test = np.random.permutation(len(X_test))\n",
    "# X_test, y_test = X_test[shuffle_indices_test], y_test[shuffle_indices_test]\n",
    "# y_test_labels = y_test_labels[shuffle_indices_test]\n",
    "\n",
    "# Reshape input data to be 3D [samples, timesteps, features] | Redundnat in our case\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_anomalies.shape)\n",
    "print(test_anomalies[:, :-1].shape)\n",
    "print(test_anomalies[:, -1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_labels)\n",
    "print(y_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Train model ############\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))  # Adjust the number of units based on your data\n",
    "model.add(Dense(X_train.shape[2]))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model on 'normal' subtraces\n",
    "model.fit(X_train, y_train, epochs=60, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### make prediction on X_test files individually\n",
    "# predictions = []\n",
    "# for i in range(len(X_test)):\n",
    "#     predictions += [model.predict(X_test[i].reshape(1, X_test[i].shape[0], X_test[i].shape[1]))]\n",
    "\n",
    "# Make predictions on 'anomalies' subtraces\n",
    "anomalies_predictions = model.predict(X_test)\n",
    "# print(\"Anomalies Predictions:\", anomalies_predictions)\n",
    "print(\"Anomalies Predictions Shape:\", anomalies_predictions.shape)\n",
    "\n",
    "# Calculate reconstruction error for each prediction\n",
    "mse_anomalies = np.mean(np.square(y_test - anomalies_predictions), axis=1)\n",
    "print(f\"Mean Squared Errors on Anomalies: {mse_anomalies}\")\n",
    "\n",
    "# Set a threshold for anomaly detection based on the reconstruction error\n",
    "threshold = 0.001  # Adjust based on your data\n",
    "# anomalies_detected = np.where(mse_anomalies > threshold)[0]\n",
    "anomalies_detected = np.where(mse_anomalies > threshold, 1, 0)\n",
    "print(\"Anomalies Detected Indices:\", anomalies_detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## selecting Threshold ##################\n",
    "#########################################################\n",
    "\n",
    "#### visualize the groundtruth and mse_anomalies for selecting threshold\n",
    "\n",
    "g_0 = np.where(y_test_labels == 0)[0]\n",
    "g_1 = np.where(y_test_labels == 1)[0]\n",
    "mse_0 = mse_anomalies[g_0]\n",
    "mse_1 = mse_anomalies[g_1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### plot the mse for normal and anomalies\n",
    "plt.figure(figsize=(10,5))\n",
    "### log scale\n",
    "plt.yscale('log')\n",
    "plt.plot(mse_0, 'b.', label='normal')\n",
    "plt.plot(mse_1, 'r.', label='anomalies')\n",
    "# plt.plot(y_test_labels, 'b.', label='ground truth')\n",
    "# plt.plot(anomalies_detected, 'r.', label='anomalies detected')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "_Classify Instances:_\n",
    "\n",
    "- The model's predictions are converted to binary labels (0 for normal, 1 for anomaly) based on the specified threshold.\n",
    "\n",
    "_Classification Report:_\n",
    "\n",
    "- The classification_report function from scikit-learn is used to generate precision, recall, and F1-score for both classes (normal and anomaly).\n",
    "\n",
    "_ROC AUC Score:_\n",
    "\n",
    "- The ROC AUC score is calculated using the roc_auc_score function.\n",
    "\n",
    "_Plot ROC Curve:_\n",
    "\n",
    "- The ROC curve is plotted using the roc_curve function and visualized using Matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = anomalies_detected\n",
    "# predictions = np.where(mse_anomalies > threshold, 1, 0)\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, predictions))\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_labels, predictions)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize on Faulty data Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
