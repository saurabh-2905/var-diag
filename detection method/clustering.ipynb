{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Means Clustering:\n",
    "\n",
    "    - Calculate features for each time series (discussed in the next point) and use K-Means clustering to group similar patterns.\n",
    "\n",
    "- Hierarchical Clustering:\n",
    "\n",
    "    - Cluster time series hierarchically based on similarity.\n",
    "\n",
    "__Things to Consider__\n",
    "\n",
    "- Here we take the subtraces (50 events) \n",
    "- Every subtrace that contains any anomaly is labelled as anomalous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_sample(file_path):\n",
    "        data = np.load(file_path, allow_pickle=False)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "code = 'theft_protection'       ### application (code)\n",
    "behaviour = 'faulty_data'            ### normal, faulty_data\n",
    "thread_typ = 'single'           ### single, multi\n",
    "version = 2.2                     ### format of data collection\n",
    "sub_len = 50\n",
    "\n",
    "base_dir = '../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "log_path = base_dir+f'/{code}/{thread_typ}_thread/version_{version}/{behaviour}'\n",
    "\n",
    "#### subtraces\n",
    "subtrace_path = f\"../data-subtraces/version_{version}/{behaviour}/subtraces/{sub_len}\"\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get files from subtraces\n",
    "# all_subtraces = os.listdir(subtrace_path)\n",
    "# all_subtraces.remove('.DS_Store')\n",
    "\n",
    "anomalies_files = os.listdir(subtrace_path+'/anomalies')\n",
    "if '.DS_Store' in anomalies_files:\n",
    "    anomalies_files.remove('.DS_Store')\n",
    "\n",
    "normal_files = os.listdir(subtrace_path+'/normal')\n",
    "if '.DS_Store' in normal_files:\n",
    "    normal_files.remove('.DS_Store')\n",
    "\n",
    "anomalies_path = [subtrace_path+'/anomalies/'+file for file in anomalies_files]\n",
    "normal_path = [subtrace_path+'/normal/'+file for file in normal_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Extraction for K-Means Clustering:_\n",
    "\n",
    "__Execution Intervals:__\n",
    "\n",
    "- Calculate the mean, standard deviation, and other statistical measures of the time differences between consecutive executions for each variable within a subtrace.\n",
    "\n",
    "__Event Frequency:__\n",
    "\n",
    "- Count the frequency of each variable within the subtrace.\n",
    "\n",
    "__Sequence Patterns:__\n",
    "\n",
    "- Convert the subtrace into a sequence of events and use techniques like sequence embedding to represent these sequences numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def extract_features(subtrace):\n",
    "    timestamps = [int(timestamp) for _, timestamp in subtrace]\n",
    "    # print(timestamps)\n",
    "    execution_intervals = np.diff(timestamps)\n",
    "    \n",
    "    mean_execution_interval = np.mean(execution_intervals)\n",
    "    std_execution_interval = np.std(execution_intervals)\n",
    "\n",
    "    # Count occurrences of unique variables in the subtrace\n",
    "    unique_variables, variable_counts = np.unique(subtrace[:, 0], return_counts=True)\n",
    "    event_frequency = dict(zip(unique_variables, variable_counts))\n",
    "\n",
    "    # Additional features can be added based on your specific requirements\n",
    "\n",
    "    return [mean_execution_interval, std_execution_interval] + list(event_frequency.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_labels = [0]*len(normal_files)\n",
    "anomalies_labels = [1]*len(anomalies_files)\n",
    "\n",
    "# #### split the normal data in 80:20 ratio\n",
    "# X_train, X_test, y_train, y_test = train_test_split(normal_files, normal_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# #### combine the train and test data\n",
    "# X_test += anomalies_files\n",
    "# y_test += anomalies_labels\n",
    "\n",
    "# #### shuffle test files\n",
    "# X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
    "\n",
    "all_files = normal_path + anomalies_path\n",
    "all_labels = normal_labels + anomalies_labels\n",
    "\n",
    "#### shuffle all files\n",
    "all_files, all_labels = shuffle(all_files, all_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract features for each subtrace\n",
    "all_features = []\n",
    "for sub_path in all_files:\n",
    "    subtrace = np.load(sub_path, allow_pickle=False)\n",
    "    # print(subtrace)\n",
    "\n",
    "    features = extract_features(subtrace)\n",
    "    all_features.append(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature matrix to a numpy array\n",
    "X = np.array(features)\n",
    "\n",
    "# Normalize features (optional but can be beneficial for K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Fit K-Means model\n",
    "num_clusters = 2  # Adjust based on your understanding of the data\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X_normalized)\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = kmeans.labels_\n",
    "\n",
    "# Assume anomalous cluster is the one with fewer instances\n",
    "anomalous_cluster = np.argmin(np.bincount(cluster_assignments))\n",
    "\n",
    "# Label subtraces based on cluster assignments\n",
    "# If a subtrace is assigned to the anomalous cluster, label it as anomalous\n",
    "labels = ['Anomalous' if label == anomalous_cluster else 'Normal' for label in cluster_assignments]\n",
    "\n",
    "# Print or use the labels as needed\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Subtrace {i+1}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Extraction for Hierarchical Clustering:_\n",
    "\n",
    "__Temporal Patterns:__\n",
    "\n",
    "- Use the timestamps of events within the subtrace to capture temporal patterns. Features could include mean, standard deviation, and other statistical measures of timestamps.\n",
    "\n",
    "__Event Co-occurrence:__\n",
    "\n",
    "- Create a matrix indicating the co-occurrence of events within the subtrace.\n",
    "\n",
    "__Time Series Characteristics:__\n",
    "\n",
    "- Extract basic statistical features such as mean, variance, skewness, and kurtosis for the entire subtrace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_for_hierarchical(subtrace):\n",
    "    timestamps = [timestamp for _, timestamp in subtrace]\n",
    "    execution_intervals = np.diff(timestamps)\n",
    "\n",
    "    mean_execution_interval = np.mean(execution_intervals)\n",
    "    std_execution_interval = np.std(execution_intervals)\n",
    "\n",
    "    event_frequency = {var: subtrace.count([var, _]) for var, _ in subtrace}\n",
    "\n",
    "    # Additional features can be added based on your specific requirements\n",
    "\n",
    "    return [mean_execution_interval, std_execution_interval] + list(event_frequency.values())\n",
    "\n",
    "def calculate_event_co_occurrence_matrix(subtrace):\n",
    "    unique_vars = list(set(var for var, _ in subtrace))\n",
    "    co_occurrence_matrix = np.zeros((len(unique_vars), len(unique_vars)))\n",
    "\n",
    "    for i, var1 in enumerate(unique_vars):\n",
    "        for j, var2 in enumerate(unique_vars):\n",
    "            co_occurrence_matrix[i, j] = sum(1 for x, _ in subtrace if x == var1 and [var2, _] in subtrace)\n",
    "\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "def hierarchical_clustering_features(subtrace):\n",
    "    features = extract_features_for_hierarchical(subtrace)\n",
    "    co_occurrence_matrix = calculate_event_co_occurrence_matrix(subtrace)\n",
    "\n",
    "    # Flatten the upper triangular part of the co-occurrence matrix (excluding the diagonal)\n",
    "    flattened_co_occurrence = squareform(pdist(co_occurrence_matrix, 'euclidean'))\n",
    "\n",
    "    # Concatenate features with flattened co-occurrence matrix\n",
    "    hierarchical_features = features + list(flattened_co_occurrence)\n",
    "\n",
    "    return hierarchical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example subtraces (replace with your own data)\n",
    "subtraces = [\n",
    "    [['var1', 100], ['var2', 110], ['var1', 120]],\n",
    "    [['var2', 110], ['var1', 120], ['var2', 130]],\n",
    "    [['var1', 140], ['var2', 150], ['var1', 160]],\n",
    "    # ... more subtraces\n",
    "]\n",
    "\n",
    "# Extract features for Hierarchical clustering\n",
    "# Extract features for each subtrace\n",
    "features_hierarchical = [extract_features_for_hierarchical(subtrace) for subtrace in subtraces]\n",
    "print(\"Hierarchical Features:\", features_hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each subtrace\n",
    "features_hierarchical = [extract_features_for_hierarchical(subtrace) for subtrace in subtraces]\n",
    "\n",
    "# Calculate event co-occurrence matrix\n",
    "co_occurrence_matrices = [calculate_event_co_occurrence_matrix(subtrace) for subtrace in subtraces]\n",
    "\n",
    "# Flatten the upper triangular part of the co-occurrence matrices (excluding the diagonal)\n",
    "flattened_co_occurrence = [squareform(pdist(matrix, 'euclidean')) for matrix in co_occurrence_matrices]\n",
    "\n",
    "# Concatenate features with flattened co-occurrence matrices\n",
    "hierarchical_features = np.concatenate([features_hierarchical, flattened_co_occurrence], axis=1)\n",
    "\n",
    "# Normalize features (optional but can be beneficial for Hierarchical Clustering)\n",
    "scaler = StandardScaler()\n",
    "hierarchical_features_normalized = scaler.fit_transform(hierarchical_features)\n",
    "\n",
    "# Fit Hierarchical Clustering model\n",
    "num_clusters = 2  # Adjust based on your understanding of the data\n",
    "hierarchical = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "hierarchical_assignments = hierarchical.fit_predict(hierarchical_features_normalized)\n",
    "\n",
    "# Assume anomalous cluster is the one with fewer instances\n",
    "anomalous_cluster_hierarchical = np.argmin(np.bincount(hierarchical_assignments))\n",
    "\n",
    "# Label subtraces based on cluster assignments\n",
    "# If a subtrace is assigned to the anomalous cluster, label it as anomalous\n",
    "labels_hierarchical = ['Anomalous' if label == anomalous_cluster_hierarchical else 'Normal' for label in hierarchical_assignments]\n",
    "\n",
    "# Print or use the labels as needed\n",
    "for i, label in enumerate(labels_hierarchical):\n",
    "    print(f\"Subtrace {i+1}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_clustering(labels_true, labels_pred):\n",
    "    # Convert labels to binary (1 for anomalous, 0 for normal)\n",
    "    labels_true_binary = np.array([1 if label == 'Anomalous' else 0 for label in labels_true])\n",
    "    labels_pred_binary = np.array([1 if label == 'Anomalous' else 0 for label in labels_pred])\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels_true_binary, labels_pred_binary)\n",
    "    recall = recall_score(labels_true_binary, labels_pred_binary)\n",
    "    f1 = f1_score(labels_true_binary, labels_pred_binary)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ground truth labels and predicted labels\n",
    "labels_true = ['Normal', 'Anomalous', 'Normal', 'Normal', 'Anomalous']\n",
    "labels_pred_kmeans = ['Normal', 'Anomalous', 'Normal', 'Normal', 'Anomalous']  # Replace with your predicted labels for K-Means\n",
    "labels_pred_hierarchical = ['Normal', 'Anomalous', 'Normal', 'Normal', 'Anomalous']  # Replace with your predicted labels for Hierarchical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means model\n",
    "precision_kmeans, recall_kmeans, f1_kmeans = evaluate_clustering(labels_true, labels_pred_kmeans)\n",
    "print(\"K-Means Model:\")\n",
    "print(f\"Precision: {precision_kmeans:.2f}\")\n",
    "print(f\"Recall: {recall_kmeans:.2f}\")\n",
    "print(f\"F1-score: {f1_kmeans:.2f}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Hierarchical Clustering model\n",
    "precision_hierarchical, recall_hierarchical, f1_hierarchical = evaluate_clustering(labels_true, labels_pred_hierarchical)\n",
    "print(\"Hierarchical Clustering Model:\")\n",
    "print(f\"Precision: {precision_hierarchical:.2f}\")\n",
    "print(f\"Recall: {recall_hierarchical:.2f}\")\n",
    "print(f\"F1-score: {f1_hierarchical:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
