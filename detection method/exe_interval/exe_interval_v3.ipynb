{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Interval Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from libraries.utils import *\n",
    "from libraries.exeint import exeInt\n",
    "\n",
    "\n",
    "def get_uniquevar(raw_trace):\n",
    "    ''' \n",
    "    convert the v2.2 trace into list of unique variables\n",
    "    raw_trace = data from read_traces, list( (var, ts),(var, ts),(var, ts),.... )\n",
    "    return:\n",
    "        unique_var = list(var1,var2,...) ## list of strings\n",
    "    '''\n",
    "    unique_var = []\n",
    "    for rt in raw_trace:\n",
    "        [var, timestamp] = rt\n",
    "        # print([var, timestamp])\n",
    "        if var not in unique_var:\n",
    "            unique_var += [var]\n",
    "            # print(rt)\n",
    "    return unique_var\n",
    "\n",
    "\n",
    "def generate_map(unique_events):\n",
    "    '''\n",
    "    unique_events -> list of all the variables in the code (unique, and in order of logging)\n",
    "    return:\n",
    "        event_map -> takes the variable name and gives corresponding event number\n",
    "        event_remap -> takes event number and gives associated variable name\n",
    "    '''\n",
    "    event_map = dict()\n",
    "    event_remap = dict()\n",
    "    for i in range(len(unique_events)):\n",
    "        event_remap[i+1] = unique_events[i]\n",
    "        event_map[unique_events[i]] = i+1\n",
    "\n",
    "    return(event_map, event_remap)\n",
    "\n",
    "def get_correct_detections(detection, ground_truth):\n",
    "    '''\n",
    "    detection -> list: detections from the  -> [[(var1, 0), (ts1, ts2), file_name], [], [], ...., []]\n",
    "    ground_truth -> list: ground truth labels -> [[(ind1, ind2), (ts1, ts2), class], [], [], ...., []]\n",
    "\n",
    "    return:\n",
    "    y_pred -> list: [1, 1, 0, 1, 0, 0, ...., 1]\n",
    "    y_true -> list: [1, 1, 1, 1, 0, 0, ...., 0]\n",
    "    '''\n",
    "    \n",
    "    # gt_pred = defaultdict(list)      ### list of detections for each gt instance. The index of list denote its respective pred at that index, and list contains gt for that pred.\n",
    "    rest_pred = [] ### list of detections that are not associated with any gt instance\n",
    "    correct_pred = [] ### list of correct predictions\n",
    "    y_true_ind = []\n",
    "    y_pred_ind = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    # print('gt_pred:', gt_pred)\n",
    "    # print(y_pred, y_true)\n",
    "\n",
    "    if len(ground_truth) != 0:\n",
    "        for gt_ind ,gt in enumerate(ground_truth):\n",
    "            ind1 = gt[0]\n",
    "            ind2 = gt[1]\n",
    "            gt_ts1 = gt[2]\n",
    "            gt_ts2 = gt[3]\n",
    "            class_label = gt[4]\n",
    "\n",
    "            if len(detection) != 0:\n",
    "                tmp_pred = []\n",
    "                for im, pred in enumerate(detection):\n",
    "                    state1, state2 = pred[0]\n",
    "                    pd_ts1, pd_ts2 = pred[1]\n",
    "                    filename = pred[2]\n",
    "                    # print('(gt_ts1, gt_ts2), (pd_ts1, pd_ts2)', (gt_ts1, gt_ts2), (pd_ts1, pd_ts2))\n",
    "\n",
    "                    cond_1 = pd_ts1 > gt_ts1 and pd_ts2 < gt_ts2  ### check if the detection timestamp is within the ground truth timestamp (case 1)\n",
    "                    cond_2 = pd_ts1 < gt_ts1 and pd_ts2 > gt_ts2  ### check if the gorund truth timestamp is within the detection timestamp (case 2)\n",
    "                    cond_3 = pd_ts1 > gt_ts1 and pd_ts1 < gt_ts2 and pd_ts2 > gt_ts2    ### partial detection on right of the ground truths, check 5 second difference after this (case 3)\n",
    "                    cond_4 = pd_ts2 < gt_ts2 and pd_ts2 > gt_ts1 and pd_ts1 < gt_ts1   ### partial detection on left of the ground truths, check 5 second difference after this (case 4)\n",
    "\n",
    "                    \n",
    "                    if cond_1 or cond_2 or cond_3 or cond_4:\n",
    "                        print(gt_ind, im, cond_1, cond_2, cond_3, cond_4)\n",
    "                        tmp_pred += [(im, pred)]    ### store all correct predictions that match with current gt                        \n",
    "\n",
    "                if tmp_pred != []:\n",
    "                    # print('tmp_pred', tmp_pred)\n",
    "                    y_true_ind += [gt_ind]\n",
    "                    ### if there are multiple correct predictions for a single gt, then choose the one that is closest to gt\n",
    "                    if len(tmp_pred) > 1:\n",
    "                        # print('if:', tmp_pred)\n",
    "                        iou_pred = []\n",
    "                        for ip, pred in tmp_pred:\n",
    "                            print('ip, pred', ip, pred)\n",
    "                            state1, state2 = pred[0]\n",
    "                            pd_ts1, pd_ts2 = pred[1]\n",
    "                            filename = pred[2]\n",
    "                            ### get IOU for all detections with gt\n",
    "                            iou = bbox_iou(gt_ts1, gt_ts2, pd_ts1, pd_ts2)    ### calculate the IOU between the ground truth and the detection to evaluate which is the best detection for the given gt\n",
    "                            iou_pred += [iou]\n",
    "                        best_pred_ind = iou_pred.index(max(iou_pred))\n",
    "                        best_pred = tmp_pred[best_pred_ind]\n",
    "                        print('ground_truth', gt)\n",
    "                        print('best_pred:', best_pred)\n",
    "                        print('y_pred_ind:', y_pred_ind)\n",
    "                        # gt_pred[gt_ind] += [pred]  ### store the best detection for the given gt\n",
    "                        if best_pred[0] not in y_pred_ind:\n",
    "                            y_pred_ind += [best_pred[0]]\n",
    "                            correct_pred += [best_pred[1]]\n",
    "                            y_true += [1]\n",
    "                            y_pred += [1]\n",
    "                    else:\n",
    "                        # print('else:', tmp_pred)\n",
    "                        print('y_pred_ind:', y_pred_ind)\n",
    "                        if tmp_pred[0][0] not in y_pred_ind:\n",
    "                            y_pred_ind += [tmp_pred[0][0]]\n",
    "                            correct_pred += [tmp_pred[0][1]]  \n",
    "                            y_true += [1]\n",
    "                            y_pred += [1]     \n",
    "                else:\n",
    "                    ### this means no detection for this gt, denots FN\n",
    "                    y_true += [1]\n",
    "                    y_pred += [0]\n",
    "        \n",
    "    ### calculate FP\n",
    "    for im, pred in enumerate(detection):\n",
    "        if im not in y_pred_ind:\n",
    "            rest_pred += [pred]\n",
    "            y_true += [0]\n",
    "            y_pred += [1]\n",
    "\n",
    "    return correct_pred, rest_pred, y_pred, y_true\n",
    "   \n",
    "\n",
    "\n",
    "def bbox_iou(b1_x1, b1_x2, b2_x1, b2_x2,):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    b1_y1 = b2_y1 = 0\n",
    "    b1_y2 = b2_y2 = 1\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = np.clip(inter_rect_x2 - inter_rect_x1 , a_min=0, a_max=None) * np.clip(\n",
    "        inter_rect_y2 - inter_rect_y1 , a_min=0, a_max=None)\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 ) * (b1_y2 - b1_y1 )\n",
    "    b2_area = (b2_x2 - b2_x1 ) * (b2_y2 - b2_y1 )\n",
    "    print('inter:', inter_area)\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('D1:', bbox_iou(1,100,11,20))\n",
    "print('D2:', bbox_iou(10,20,11,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE = 'theft_protection'       ### application (code)\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'            ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'            ### normal, faulty_data\n",
    "THREAD = 'single'           ### single, multi\n",
    "VER = 3                     ### format of data collection\n",
    "\n",
    "base_dir = '../../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(normalbase_path)\n",
    "print(faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "\n",
    "\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "# print(paths_log)\n",
    "# print(paths_traces)\n",
    "# print(varlist_path)\n",
    "# print(paths_label)\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label\n",
    "\n",
    "print(train_data_path)\n",
    "print(test_data_path)\n",
    "print(test_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3:\n",
    "    to_number = is_consistent(varlist_path)\n",
    "\n",
    "    if to_number != False:\n",
    "        from_number = mapint2var(to_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confidence Interval:__\n",
    "\n",
    "A confidence interval is a range around the mean that is likely to contain the true population mean. The formula for a confidence interval is mean ± margin of error mean±margin of error, where the margin of error depends on the desired confidence level and the standard error.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "1. Choose a confidence level (e.g., 95%).\n",
    "2. Calculate the standard error: standard deviation/ sqr_root(number of observations)\n",
    "3. Calculate the margin of error: critical value × standard error\n",
    "4. Determine the confidence interval: mean ± margin of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize exeinz\n",
    "ei = exeInt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get execution intervals for all variables\n",
    "\n",
    "exe_list, filewise_exe_list = ei.get_exeint(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the confidence intervals for all variables\n",
    "confidence_intervals = ei.get_confinterval(exe_list)\n",
    "\n",
    "############ calculate dynamic thresholds ############\n",
    "thresholds = ei.get_dynamicthresh(exe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot exe_list to vsiualize the distribution of execution intervals\n",
    "ei.viz_thresholds(exe_list, confidence_intervals, thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detect anomalies in faulty traces\n",
    "DIFF_VAL = 1\n",
    "all_tp = []\n",
    "all_fp = []\n",
    "all_detections = [] ### format [file1_detection, file2_detection] -> file1_detection: [(state1, 0), (ts1, ts2), filename]  \n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "for test_data, test_label in zip(test_data_path, test_label_path):\n",
    "    print(test_data, test_label)\n",
    "    detection = ei.test_single(test_data, thresholds)   ### detection in format: [var, (ts1,ts2), file_name]\n",
    "    dedup_detection, grouped_det = ei.remove_duplicates(detection, DIFF_VAL)  ### remove multiple detections for single ground truth\n",
    "    detection = dedup_detection\n",
    "    all_detections += [(test_data, detection, test_label)]  ### used to plot detections\n",
    "\n",
    "    ### load ground truths\n",
    "    ground_truth_raw = read_traces(test_label)\n",
    "    ground_truth = ground_truth_raw['labels']\n",
    "    label_trace_name = list(ground_truth.keys())[0]\n",
    "    ground_truth = ground_truth[label_trace_name]\n",
    "    print('ground truths:', ground_truth)\n",
    "    print(len(ground_truth))\n",
    "\n",
    "    # correct_pred, rest_pred, y_pred, y_true = get_ypred_ytrue(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "    correct_pred, rest_pred, y_pred, y_true = get_correct_detections(detection, ground_truth)  ### case1_pred, case2_pred, case34_pred, rest_pred\n",
    "\n",
    "    assert( len(detection) == len(correct_pred) + len(rest_pred) )\n",
    "\n",
    "    all_tp += [(test_data, correct_pred, test_label)]\n",
    "    all_fp += [(test_data, rest_pred, test_label)]\n",
    "\n",
    "    y_pred_all.extend(y_pred)\n",
    "    y_true_all.extend(y_true)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, average_precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true_all, y_pred_all)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true_all, y_pred_all)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# # Calculate average precision\n",
    "# average_precision = average_precision_score(y_true_all, y_pred_all)\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true_all, y_pred_all)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['normal', 'anomaly'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot gt and detections\n",
    "# for test_data, detections, test_label in all_detections:\n",
    "for test_data, detections, test_label in all_tp:\n",
    "    # print('test_data:', test_data)\n",
    "    # print('detections:', detections)\n",
    "    # print(test_label)\n",
    "\n",
    "    ### prepare trace to plot\n",
    "    col_data = preprocess_traces([test_data])\n",
    "    all_df = get_dataframe(col_data) \n",
    "    # print(all_df[0])\n",
    "\n",
    "    ### prepare detections to plot\n",
    "    timestamps = col_data[0][1]\n",
    "    print('timestamps:', timestamps)\n",
    "    plot_val = []\n",
    "    plot_x_ticks = []\n",
    "    plot_class = []\n",
    "    for det in detections:\n",
    "        # print(det)\n",
    "        det_ts1, det_ts2 = det[1]\n",
    "        # print(det_ts1, det_ts2)\n",
    "\n",
    "        det_ind1_pre = [ abs(t-det_ts1) for t in timestamps]\n",
    "        det_ind1 = det_ind1_pre.index(min(det_ind1_pre))\n",
    "\n",
    "        det_ind2_pre = [ abs(t-det_ts2) for t in timestamps]\n",
    "        det_ind2 = det_ind2_pre.index(min(det_ind2_pre))\n",
    "        # print(det_ind1, det_ind2)\n",
    "        # print(timestamps[det_ind1], timestamps[det_ind2])\n",
    "\n",
    "        plot_val += [(det_ind1, det_ind2)]\n",
    "        plot_x_ticks += [(timestamps[det_ind1], timestamps[det_ind2])]\n",
    "        plot_class += [0]\n",
    "\n",
    "    plot_detections = [plot_val, plot_x_ticks, plot_class]\n",
    "\n",
    "    ### get ground truths\n",
    "    gt_plot = prepare_gt(test_label)\n",
    "\n",
    "    ### plot\n",
    "    for df in all_df:\n",
    "        # print(df.columns)\n",
    "        plot_single_trace(df, \n",
    "                          var_list, \n",
    "                          with_time=False, \n",
    "                          is_xticks=True, \n",
    "                          detections=plot_detections, \n",
    "                          dt_classlist=['detection'],\n",
    "                          ground_truths=gt_plot,\n",
    "                          gt_classlist=['gt_communication', 'gt_sensor', 'gt_bitflip'],\n",
    "                          )\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "---\n",
    "- since multiple variables are affected due to single anomaly, multiple detections are generated for each anomaly.\n",
    "- This leads to multiple FP.\n",
    "- To avoid this, we implement deduplication which groups the detections that are close to each other bsed on timestamp\n",
    "- However, in this process along with decrease in FP, we have more False Negatives i.e. some anomalies are not detected. \n",
    "\n",
    "TODO:\n",
    "- change deduplication stratergy, if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
