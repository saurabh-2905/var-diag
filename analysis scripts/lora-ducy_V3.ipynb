{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from libv3.utils import *\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ configuration ################\n",
    "############################################\n",
    "\n",
    "CODE, BEHAVIOUR, THREAD, VER = get_config('lora_ducy_config')   ### config stored in libv3/exp_config.txt\n",
    "\n",
    "base_dir = '../trace_data' ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "log_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR}'\n",
    "\n",
    "print(log_path)\n",
    "\n",
    "#### file to display\n",
    "trace_file = 0\n",
    "\n",
    "print('file number:', trace_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(log_path)\n",
    "\n",
    "### remove.Ds_store from all lists\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "\n",
    "print(paths_log)\n",
    "print(paths_traces)\n",
    "print(varlist_path)\n",
    "print(paths_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_traces[0]    ## 14, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if all keys are constant\n",
    "# for i in range(len(varlist_path)):\n",
    "varlist1 = read_json(varlist_path[0])\n",
    "varlist2 = read_json(varlist_path[1])\n",
    "# varlist3 = read_json(varlist_path[2])\n",
    "# varlist4 = read_json(varlist_path[3])\n",
    "# print(varlist.keys())\n",
    "keys = list(varlist1.keys())\n",
    "keys.sort()\n",
    "print(keys)\n",
    "# print(len(varlist1.keys()), len(varlist2.keys()), len(varlist3.keys()), len(varlist4.keys()))\n",
    "print(len(varlist1.keys()), len(varlist2.keys()))\n",
    "\n",
    "# for k in keys:\n",
    "#     print(varlist1[k], varlist2[k], varlist3[k], varlist4[k])\n",
    "#     # print(k)\n",
    "#     if not (varlist1[k] == varlist2[k] == varlist3[k] == varlist4[k]): \n",
    "#         print(k)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "for vl in varlist_path:\n",
    "    varlist = read_json(vl)\n",
    "    print(len(varlist.keys()))\n",
    "\n",
    "    # for k in varlist.keys():\n",
    "    #     print(k, varlist[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# check varlist is consistent ############\n",
    "############# only for version 3 ######################\n",
    "\n",
    "if VER == 3:\n",
    "    check_con, _ = is_consistent(varlist_path)\n",
    "\n",
    "    if check_con != False:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        print('varlist is not consistent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Get variable list ######################\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]   ### get the variable list\n",
    "# print(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## process the traces ###########\n",
    "col_data = preprocess_traces(paths_traces, var_list)   ### in the format (trace_name, x_data, y_data, y_labels, trace_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate plot trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### preprocessing data to plot using plotly ##############\n",
    "'''\n",
    "Restructure the data in dictionary with (keys,value) pair :-  (time, timestamps) , (trace_name, trace)\n",
    "'''\n",
    "all_df = get_dataframe(col_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### plot single trace\n",
    "for i, df in enumerate(all_df):\n",
    "    \n",
    "    if i == trace_file:\n",
    "        plot_single_trace(df, sorted_keys, with_time=False, is_xticks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get timestamp\n",
    "\n",
    "timestamp = index2timestamp(all_df[0], 227)\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Interval Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### preprocessing data ########\n",
    "\n",
    "#### extract timestamps for each variable and store them in a dictionary along with index values for each variable in event trace\n",
    "\n",
    "var_timestamps = get_var_timestamps(paths_traces)    #### in format (filename, dict of timestamps and index values)\n",
    "\n",
    "to_plot = preprocess_variable_plotting(var_timestamps, var_list, from_number, trace_number=trace_file)   ### restructure the data for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_plot[6][-1])\n",
    "to_plot[6][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate execution interval plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the data\n",
    "plot_execution_interval_single(to_plot, is_xticks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### count and prepare labels to plot\n",
    "'''\n",
    "labels are of format [index1, index2, timestamp1, timestamp2, class]\n",
    "'''\n",
    "class_count = defaultdict(int)\n",
    "for i, path in enumerate(paths_label):\n",
    "    label_content = prepare_gt(path)\n",
    "    ind, ts, cls = label_content\n",
    "    # print(ind, ts, cls)\n",
    "    for c in cls:\n",
    "        class_count[c]+=1\n",
    "        \n",
    "    if i == trace_file:\n",
    "        print(path)\n",
    "        toplot_gt = label_content\n",
    "\n",
    "    print(os.path.split(path)[-1], class_count)\n",
    "\n",
    "    # break\n",
    "for key, val in class_count.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot\n",
    "for i, df in enumerate(all_df):\n",
    "    if i == trace_file:\n",
    "        plot_obj = plot_single_trace(df, var_list, with_time=False, is_xticks=True, ground_truths=toplot_gt)\n",
    "        plot_obj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Add the indices for the interval in 'normal_seq_inter' that show correct behvaiour in the traces. \n",
    "# The format is as follows:\n",
    "# path_traces = [path1, path2, ...]\n",
    "# normal_seq_inter = [ ( intervals for trace1, ...), \n",
    "#                     ( intervals for trace2 ...),\n",
    "#                       ...] \n",
    "# '''\n",
    "\n",
    "# normal_seq_inter = (((0,6200), (6500,9910)),\n",
    "#                     ((0,8260), (8260,11000)))  ### v3, mamba2 trial1, trial2\n",
    "            \n",
    "\n",
    "# for p, n_inter in zip(paths_traces, normal_seq_inter):\n",
    "#     trace = read_traces(p)\n",
    "#     train_data_path = os.path.join(os.path.dirname(p), 'train_data')\n",
    "\n",
    "#     if not os.path.exists(train_data_path):\n",
    "#         os.makedirs(train_data_path)\n",
    "\n",
    "#     for i, inter in enumerate(n_inter):\n",
    "#         start, end = inter\n",
    "#         # trace[start:end].to_csv(os.path.join(train_data_path, f'interval_{start}_{end}.csv'), index=False)\n",
    "#         json.dump(trace[start:end], open(os.path.join(train_data_path, f'interval_{start}_{end}.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "days = 7\n",
    "hours_per_day = 24\n",
    "total_hours = days * hours_per_day  # Total number of data points for 7 days\n",
    "sensors = 4\n",
    "\n",
    "# Generate base temperature data for each sensor (normal values between 20°C and 30°C)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "base_temperature = np.random.uniform(20, 30, size=(total_hours, sensors))\n",
    "\n",
    "# Introduce random spikes (anomalies) in the data with a value of 100\n",
    "num_anomalies = 10  # Number of anomalies to introduce\n",
    "for _ in range(num_anomalies):\n",
    "    sensor = random.randint(0, sensors - 1)  # Randomly choose a sensor\n",
    "    time_point = random.randint(0, total_hours - 1)  # Randomly choose a time point\n",
    "    base_temperature[time_point, sensor] = 100  # Set the anomaly value to 100\n",
    "\n",
    "# Generate time points (hours) for 7 days as datetime objects for proper formatting\n",
    "start_time = datetime.datetime(2022, 7, 22, 4, 48)  # Set a starting datetime\n",
    "time_points = [start_time + datetime.timedelta(hours=i) for i in range(total_hours)]\n",
    "\n",
    "# Save the data to an Excel file\n",
    "df = pd.DataFrame(base_temperature, columns=[f'Sensor {i+1}' for i in range(sensors)])\n",
    "df['Time'] = time_points\n",
    "df = df[['Time'] + [f'Sensor {i+1}' for i in range(sensors)]]  # Reorder to put 'Time' first\n",
    "df.to_excel('sensor_temperature_data_7days.xlsx', index=False)  # Save the DataFrame to Excel\n",
    "\n",
    "# Set up the figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot temperature data for each sensor\n",
    "for sensor in range(sensors):\n",
    "    ax.plot(time_points, base_temperature[:, sensor], label=f'Sensor {sensor + 1}', linewidth=2)\n",
    "\n",
    "# Customize time format on x-axis\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))  # Show a tick per day\n",
    "ax.xaxis.set_minor_locator(mdates.HourLocator(interval=12))  # Minor ticks every 12 hours\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m.%y\\n%H:%M'))  # Format as date and time\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks()\n",
    "\n",
    "# Labeling the axes\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "\n",
    "# Set Y-axis limit\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "# Adding gridlines and a legend\n",
    "ax.grid(True, which='major', axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=sensors)\n",
    "\n",
    "# Set title and show the plot\n",
    "# plt.title('Temperature Readings from 4 Sensors Over 7 Days with Anomalies', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "days = 7\n",
    "hours_per_day = 24\n",
    "total_hours = days * hours_per_day  # Total number of data points for 7 days\n",
    "sensors = 4\n",
    "\n",
    "# Generate base temperature data for each sensor (normal values between 20°C and 30°C)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "base_temperature = np.random.uniform(20, 30, size=(total_hours, sensors))\n",
    "\n",
    "# Introduce random spikes (anomalies) in the data with a value of 100\n",
    "num_anomalies = 10  # Number of anomalies to introduce\n",
    "for _ in range(num_anomalies):\n",
    "    sensor = random.randint(0, sensors - 1)  # Randomly choose a sensor\n",
    "    time_point = random.randint(0, total_hours - 1)  # Randomly choose a time point\n",
    "    base_temperature[time_point, sensor] = 100  # Set the anomaly value to 100\n",
    "\n",
    "# Generate time points (hours) for 7 days\n",
    "time_points = np.arange(total_hours)\n",
    "\n",
    "# Set up the figure and axis for plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot temperature data for each sensor\n",
    "for sensor in range(sensors):\n",
    "    plt.plot(time_points, base_temperature[:, sensor], label=f'Sensor {sensor + 1}')\n",
    "\n",
    "# Labeling the axes\n",
    "plt.xlabel('Time (hours)', fontsize=12)\n",
    "plt.ylabel('Temperature (°C)', fontsize=12)\n",
    "\n",
    "# Adding gridlines and a legend\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Temperature Readings from 4 Sensors Over 7 Days with Anomalies at 100°C', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
